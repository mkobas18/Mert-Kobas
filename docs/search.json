[
  {
    "objectID": "posts/Thomas et al. 2013/index.html",
    "href": "posts/Thomas et al. 2013/index.html",
    "title": "Thomas et al. (2014) Memory Constraints on Hypothesis Generation and Decision Making",
    "section": "",
    "text": "The focus of this article is hypothesis generation and memory relation. In everyday life, we use hypothesis to produce explanations after we test it. The general idea here is how memory as a cognitive structure constrains hypothesis generation and related decision-making processes.\nThree primary processes can explain this idea:\n\nRetrieval (Long term memory, retrieving memory traces)\nMaintenance (Working memory, sustaining these traces in the active thinking process)\nJudgement (Decision after test these hypotheses)\n\n\n\n\nThomas et al. (2013), page 265, Figure 1\n\n\nOther important term here is hypothesis-guided search. I interpreted this as a top-down evidence looking stage between hypothesis generation and decision making. Medical diagnoses are examples for this.\nComment 1: However, the first hypothesis must be bottom-up or based on the testimony here. So, a medical doctor may see their patient with holding their arm, so the first hypothesis must be that the arm was broken. After this, X-ray, etc. can be used to prove this hypothesis and make a decision.\n\n\nThey mention how experts and other people generate only a small number of these hypotheses (probably due to WM restrictions, they mentioned in their other article: People with high WM produce 4 hypothesis in a time window, but low WM produce around 2.)\nMost likely hypotheses will be produced first (broken arm). New hypotheses are produced, if they better matches to the data (after X-ray screening, it may be only muscle ache).\nQuestion 1: They gave an example here about a mental illness: “Entertaining the hypothesis that the kitchen faucet is communicating with you when the dripping is perfectly consistent with its just being leaky”. Is this a failure of data-hypothesis match or interpreting data based on prior beliefs? (if this would be another context like believing spirituality) If you believe that non-living things can have spirit/can talk/etc., then you may find this more likely (more match with data hypothesis). So beyond data and hypothesis, do our beliefs play a role to distort our probability judgement process?\nAnother interesting point here is related to experience. Experts often recall specific hypothesis (broken bones) because those are activated more often. The article said that these hypotheses have more a priori likelihood. I interpreted this as these hypotheses were seen as more likely in the beginning. So if there are two hypotheses that come to my mind (broken arm vs. shoulder problem in the example of painful arm), I would prioritize broken arm because of my previous experiences.\nComment 2: Does it mean that experts produce some heuristics for explanations (holding arms mean broken bones without any alternatives -alternatives come after testing-)?\nOr expertise still provides more memory space (less cognitive load), so does it mean for more accurate hypothesis generation and testing process? (holding arms may mean broken bone because I see this many times, but alternative hypotheses can be possible but less likely -alternatives bring with the main hypothesis-)\nSo, do experts produce explanation both with inherent and extrinsic features (because they can use their cognitive resources flexibly to produce hypotheses with inherent and extrinsic features) just like the nonspeed condition group in Hussak & Cimpian (2018)? What if under time limitation and LTM restriction?\n\n\n\nThey introduce us here another term: Subadditivity. It means that the sum of the probabilities of sub-arguments in a hypothesis is more than the probability of hypothesis itself. The probabilities here reflect the evaluation of an individual. The probability of broken bones (25%), muscle problems (20%) and carpal tunnel (15%) > the probability of arm problem (50%)\n\n\n\nWith only one hypothesis, it becomes including confirmation search strategy. For example, If a person thought that somebody is married already, they search engagement ring of the other person.\nWith two or more hypothesis, it becomes diagnostic search. For example, if my computer cannot open any Youtube videos, I may think that it is because of my web browser or internet connection or Youtube servers, so I check my connection, I probably try another browser and search internet about servers, so I try to search diagnostically.\nIn the social cases, diagnostic search may be impossible sometimes? like if my partner is angry, I think that it maybe due to work or something that I say in the morning or an irrelevant thing, but checking all of these diagnostically is only possible to ask my partner, but this may make them more angry, so it is impossible?\n\n\n\nTime pressure led to generation of fewer hypotheses.\nQuestion 2a: So time pressure both leads to produce inherent biases in explanation and also less alternative explanations. Are those related somehow (people start with inherent explanations then they add extrinsic ones with alternative hypotheses), or separate processes (less alternative explanations + including mostly inherent features)?\nQuestion 2b: In the education system, we use time pressure to measure expertise (at least the assumption is this) like college entrance exams or language exams. People relate the mistakes in those exams usually with attentional issues (or didn’t have the knowledge) rather than memory problems. It reminds me Horne et al. (2019)’s article here. Time pressure may be a distractor due to related stress, so both hypothesis generation and generated explanations -including mostly inherent biases- were affected by attentional proccesses (focusing effects may become more effective).\nComment 3:Related to question 2b, they also mention here about primacy bias and recency bias. For the explanations, it may mean that people have difficulties the focus on middle part of the data (or an event). As an example, a person may be near the seaside and sees another person going to the sea. When the person after saw that person B as drowning, they explained it as person B doesn’t know swimming (inherent bias) in the first look and try to save them. The recent event is drowning here, however, it may be also waves of sea (extrinsic one). Since there is a time pressure (person b is dying), person A may not care the process of the event.\nAnother important point here is with slow presentation of data, people have recency bias but with fast rate of presentation, primacy bias emerged. Complexity of task or increased working memory capacity leads to increase in primacy bias during hypothesis generation.\nThe last point: People produce good hypotheses to explain the data, but they have also systematic biases (?? can inherent bias be an example?) in beliefs and information search due to memory limitations.\nQuestion 3 (bonus question, theoretical): Does it mean that cognitive biases in explanation generation process are the production of memory constrains? Or do they place in cognitive architecture -due to evoluationary processes, etc.-? Or both of them?"
  },
  {
    "objectID": "posts/data/index.html",
    "href": "posts/data/index.html",
    "title": "Expertise Data Analysis Trials",
    "section": "",
    "text": "This post includes the trial analyses of an example data related to expertise.\nImport necessary packages and expertise data\n\nlibrary(tidyverse,quietly = T)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.0      ✔ stringr 1.4.0 \n✔ readr   2.1.2      ✔ forcats 0.5.1 \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(heatmaply,quietly = T)\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\n\n\n======================\nWelcome to heatmaply version 1.4.0\n\nType citation('heatmaply') for how to cite the package.\nType ?heatmaply for the main documentation.\n\nThe github page is: https://github.com/talgalili/heatmaply/\nPlease submit your suggestions and bug-reports at: https://github.com/talgalili/heatmaply/issues\nYou may ask questions at stackoverflow, use the r and heatmaply tags: \n     https://stackoverflow.com/questions/tagged/heatmaply\n======================\n\nexpertise<-read_csv(\"~/Desktop/Expertise/NEW/Combined STUDY for R.csv\", show_col_types = FALSE)\n\nDescriptive statistics of inherence scores from the 1st and 2nd study\n\nexpertise%>%\n  filter(study==1)%>%\n  summary()\n\n    subject        item                 ih            high        \n Min.   :  1   Length:1592        Min.   :1.00   Min.   :-0.6448  \n 1st Qu.: 50   Class :character   1st Qu.:3.50   1st Qu.:-0.3706  \n Median :100   Mode  :character   Median :5.00   Median :-0.2488  \n Mean   :100                      Mean   :4.87   Mean   : 0.0000  \n 3rd Qu.:150                      3rd Qu.:6.00   3rd Qu.:-0.1632  \n Max.   :199                      Max.   :9.00   Max.   :12.2091  \n                                                                  \n      coll              grad             books              mag          \n Min.   :-0.5406   Min.   :-0.1541   Min.   :-0.4928   Min.   :-0.41020  \n 1st Qu.:-0.2714   1st Qu.:-0.1234   1st Qu.:-0.3380   1st Qu.:-0.24982  \n Median :-0.2290   Median :-0.1038   Median :-0.2529   Median :-0.11861  \n Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.00000  \n 3rd Qu.:-0.1865   3rd Qu.:-0.0953   3rd Qu.:-0.2354   3rd Qu.:-0.08064  \n Max.   : 9.9358   Max.   :14.0358   Max.   :11.2170   Max.   :13.91452  \n                   NA's   :199                                           \n      know            needforcog       needclo         study  \n Min.   :-2.65921   Min.   :2.111   Min.   : NA    Min.   :1  \n 1st Qu.:-0.76721   1st Qu.:5.056   1st Qu.: NA    1st Qu.:1  \n Median : 0.01276   Median :5.889   Median : NA    Median :1  \n Mean   : 0.00000   Mean   :5.958   Mean   :NaN    Mean   :1  \n 3rd Qu.: 0.75596   3rd Qu.:6.889   3rd Qu.: NA    3rd Qu.:1  \n Max.   : 2.95800   Max.   :8.778   Max.   : NA    Max.   :1  \n                                    NA's   :1592              \n   high_w025          coll_w025          grad_w025          books_w025      \n Min.   :-0.64478   Min.   :-0.54058   Min.   :-0.15413   Min.   :-0.49281  \n 1st Qu.:-0.37065   1st Qu.:-0.27138   1st Qu.:-0.12341   1st Qu.:-0.33797  \n Median :-0.24880   Median :-0.22903   Median :-0.10384   Median :-0.25290  \n Mean   :-0.04743   Mean   :-0.05107   Mean   :-0.10788   Mean   :-0.05969  \n 3rd Qu.:-0.16320   3rd Qu.:-0.18651   3rd Qu.:-0.09530   3rd Qu.:-0.23538  \n Max.   : 2.91073   Max.   : 3.13722   Max.   :-0.07089   Max.   : 2.62381  \n                                       NA's   :199                          \n    mag_w025         readingcomp        schoolcomp       readcomp025      \n Min.   :-0.41020   Min.   :-0.3961   Min.   :-0.4465   Min.   :-0.39605  \n 1st Qu.:-0.24982   1st Qu.:-0.2757   1st Qu.:-0.2506   1st Qu.:-0.27567  \n Median :-0.11861   Median :-0.1825   Median :-0.2212   Median :-0.18254  \n Mean   :-0.08237   Mean   : 0.0000   Mean   : 0.0000   Mean   :-0.05538  \n 3rd Qu.:-0.08064   3rd Qu.:-0.1051   3rd Qu.:-0.1687   3rd Qu.:-0.10513  \n Max.   : 1.21479   Max.   :11.9385   Max.   : 7.4510   Max.   : 1.96513  \n                                      NA's   :199                         \n schoolcomp025      objectivecomp1     objectivecomp01    objectivecomp025  \n Min.   :-0.44650   Min.   :-0.41880   Min.   :-0.41880   Min.   :-0.41880  \n 1st Qu.:-0.25065   1st Qu.:-0.25965   1st Qu.:-0.25965   1st Qu.:-0.25965  \n Median :-0.22124   Median :-0.21247   Median :-0.21247   Median :-0.21247  \n Mean   :-0.04046   Mean   : 0.00000   Mean   :-0.01449   Mean   :-0.02807  \n 3rd Qu.:-0.16872   3rd Qu.:-0.02451   3rd Qu.:-0.02451   3rd Qu.:-0.02451  \n Max.   : 2.03964   Max.   : 6.72741   Max.   : 2.70624   Max.   : 1.87096  \n NA's   :199        NA's   :199        NA's   :199        NA's   :199       \n objectivecomp05       know_w05           know_w025         \n Min.   :-0.38152   Min.   :-1.581299   Min.   :-1.8858149  \n 1st Qu.:-0.25965   1st Qu.:-0.767212   1st Qu.:-0.7672123  \n Median :-0.21247   Median : 0.012760   Median : 0.0127596  \n Mean   :-0.05507   Mean   : 0.003705   Mean   :-0.0004338  \n 3rd Qu.:-0.02451   3rd Qu.: 0.755964   3rd Qu.: 0.7559637  \n Max.   : 1.02637   Max.   : 1.619293   Max.   : 1.8324862  \n NA's   :199                                                \n    know_w01         \n Min.   :-2.0975756  \n 1st Qu.:-0.7672123  \n Median : 0.0127596  \n Mean   : 0.0000465  \n 3rd Qu.: 0.7559637  \n Max.   : 2.0733853  \n                     \n\n\n\nexpertise%>%\n  filter(study==2)%>%\n  summary()\n\n    subject          item                 ih             high        \n Min.   :200.0   Length:1536        Min.   :1.000   Min.   :-0.3519  \n 1st Qu.:247.8   Class :character   1st Qu.:3.500   1st Qu.:-0.2716  \n Median :295.5   Mode  :character   Median :5.000   Median :-0.2143  \n Mean   :295.5                      Mean   :5.112   Mean   : 0.0000  \n 3rd Qu.:343.2                      3rd Qu.:7.000   3rd Qu.:-0.1104  \n Max.   :391.0                      Max.   :9.000   Max.   :13.7543  \n                                                                     \n      coll               grad             books               mag          \n Min.   :-0.50140   Min.   :-0.1224   Min.   :-0.42153   Min.   :-0.44152  \n 1st Qu.:-0.26320   1st Qu.:-0.1208   1st Qu.:-0.29282   1st Qu.:-0.30989  \n Median :-0.25482   Median :-0.1023   Median :-0.25439   Median :-0.23676  \n Mean   : 0.00000   Mean   : 0.0000   Mean   : 0.00000   Mean   : 0.00000  \n 3rd Qu.:-0.08065   3rd Qu.:-0.0744   3rd Qu.:-0.08109   3rd Qu.:-0.08575  \n Max.   :13.78290   Max.   :13.7842   Max.   :13.65792   Max.   :13.72199  \n                    NA's   :384                                            \n      know           needforcog      needclo          study  \n Min.   :-2.1972   Min.   : NA    Min.   :3.929   Min.   :2  \n 1st Qu.:-0.8439   1st Qu.: NA    1st Qu.:5.143   1st Qu.:2  \n Median :-0.1677   Median : NA    Median :5.702   Median :2  \n Mean   : 0.0000   Mean   :NaN    Mean   :5.650   Mean   :2  \n 3rd Qu.: 0.6845   3rd Qu.: NA    3rd Qu.:6.095   3rd Qu.:2  \n Max.   : 3.0503   Max.   : NA    Max.   :7.548   Max.   :2  \n                   NA's   :1536                              \n   high_w025          coll_w025          grad_w025         books_w025      \n Min.   :-0.35188   Min.   :-0.50140   Min.   :-0.1224   Min.   :-0.42153  \n 1st Qu.:-0.27160   1st Qu.:-0.26320   1st Qu.:-0.1208   1st Qu.:-0.29282  \n Median :-0.21427   Median :-0.25482   Median :-0.1023   Median :-0.25439  \n Mean   :-0.05455   Mean   :-0.07605   Mean   :-0.0958   Mean   :-0.07792  \n 3rd Qu.:-0.11043   3rd Qu.:-0.08065   3rd Qu.:-0.0744   3rd Qu.:-0.08109  \n Max.   : 2.79051   Max.   : 1.75819   Max.   : 0.0130   Max.   : 2.15922  \n                                       NA's   :384                         \n    mag_w025         readingcomp         schoolcomp       readcomp025      \n Min.   :-0.44152   Min.   :-0.39298   Min.   :-0.2817   Min.   :-0.39297  \n 1st Qu.:-0.30989   1st Qu.:-0.25168   1st Qu.:-0.1933   1st Qu.:-0.25168  \n Median :-0.23676   Median :-0.20129   Median :-0.1175   Median :-0.20129  \n Mean   :-0.07705   Mean   : 0.00000   Mean   : 0.0000   Mean   :-0.05458  \n 3rd Qu.:-0.08575   3rd Qu.:-0.07418   3rd Qu.:-0.0682   3rd Qu.:-0.07418  \n Max.   : 2.11494   Max.   : 9.32467   Max.   :13.7736   Max.   : 2.01190  \n                                       NA's   :384                         \n schoolcomp025     objectivecomp1 objectivecomp01   objectivecomp025 \n Min.   :-0.2817   Min.   : NA    Min.   :-0.2812   Min.   :-0.2812  \n 1st Qu.:-0.1933   1st Qu.: NA    1st Qu.:-0.2164   1st Qu.:-0.2164  \n Median :-0.1175   Median : NA    Median :-0.1537   Median :-0.1537  \n Mean   :-0.0547   Mean   :NaN    Mean   :-0.0215   Mean   :-0.0378  \n 3rd Qu.:-0.0682   3rd Qu.: NA    3rd Qu.:-0.0368   3rd Qu.:-0.0368  \n Max.   : 1.3437   Max.   : NA    Max.   : 2.5783   Max.   : 1.5410  \n NA's   :384       NA's   :1536   NA's   :384       NA's   :384      \n objectivecomp05      know_w05           know_w025         \n Min.   :-0.2812   Min.   :-1.304722   Min.   :-1.5674281  \n 1st Qu.:-0.2164   1st Qu.:-0.843905   1st Qu.:-0.8439054  \n Median :-0.1537   Median :-0.167666   Median :-0.1676661  \n Mean   :-0.0611   Mean   :-0.003341   Mean   :-0.0008275  \n 3rd Qu.:-0.0368   3rd Qu.: 0.684484   3rd Qu.: 0.6844843  \n Max.   : 0.8561   Max.   : 1.824793   Max.   : 2.1532753  \n NA's   :384                                               \n    know_w01         \n Min.   :-1.7913281  \n 1st Qu.:-0.8439054  \n Median :-0.1676661  \n Mean   :-0.0008895  \n 3rd Qu.: 0.6844843  \n Max.   : 2.4240202  \n                     \n\n\n\n\n\nThe boxplot of inherence scores by expertise areas in study 1\n\nplot1<-expertise%>%\n  filter(study==1)%>%\n  ggplot(aes(x=item, y=ih)) + \n  geom_boxplot(colour = \"black\"\n  )+\n  ggtitle(\"Inherence scores by expertise areas in study 1\") +\n  ylab(\"Inherence Scores\")+\n  xlab(\"Expertise areas\")+\n  theme(panel.grid.major = element_blank(),\n        panel.background = element_blank(),\n        strip.placement = \"outside\",              \n        strip.background = element_blank(),\n        legend.position=\"none\",\n        panel.border = element_blank(), \n        panel.spacing.x = unit(0,\"line\"))\nplot1+geom_jitter(shape=16, position=position_jitter(0.2))\n\n\n\n\nThe boxplot of inherence scores by expertise areas in study 2\n\nplot2<-expertise%>%\n  filter(study==2)%>%\n  ggplot(aes(x=item, y=ih)) + \n  geom_boxplot(colour = \"black\"\n  )+\n  ggtitle(\"Inherence scores by expertise areas in study 2\") +\n  ylab(\"Inherence Scores\")+\n  xlab(\"Expertise areas\")+\n  theme(panel.grid.major = element_blank(),\n        panel.background = element_blank(),\n        strip.placement = \"outside\",              \n        strip.background = element_blank(),\n        legend.position=\"none\",\n        panel.border = element_blank(), \n        panel.spacing.x = unit(0,\"line\"))\nplot2+geom_jitter(shape=16, position=position_jitter(0.2))\n\n\n\n\nCorrelation matrix of language expertise in study 1\n\nexpert_sublev_st1<-expertise%>%\n  filter(study==1 & item=='LANGUAGE')%>%\n  select(subject,ih, high, coll, grad, books, mag, know, needforcog)\n\ndf <- expert_sublev_st1\n\nheatmaply_cor(x = cor(df),\n              k_col = 2,\n              k_row = 2)\n\nWarning in doTryCatch(return(expr), name, parentenv, handler): unable to load shared object '/Library/Frameworks/R.framework/Resources/modules//R_X11.so':\n  dlopen(/Library/Frameworks/R.framework/Resources/modules//R_X11.so, 6): Library not loaded: /opt/X11/lib/libSM.6.dylib\n  Referenced from: /Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/modules/R_X11.so\n  Reason: image not found\n\n\n\n\n\n\n\nexpert_sublev_scho<-expertise%>%\n  filter(study==1 & item=='SCHOOL')%>%\n  select(subject,ih, high, coll, grad, books, mag, know, needforcog)\n\ndf <- expert_sublev_scho\n\nheatmaply_cor(x = cor(df),\n              k_col = 2,\n              k_row = 2)\n\n\n\n\n\nA regression model for inherence scores\n\nexpertise.lm<-lm(ih ~ factor(item)+high+coll+grad+books+mag+know+needforcog, data = expertise)\n\nsummary(expertise.lm)\n\n\nCall:\nlm(formula = ih ~ factor(item) + high + coll + grad + books + \n    mag + know + needforcog, data = expertise)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.9768 -1.1633 -0.0215  1.1037  5.5806 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(>|t|)    \n(Intercept)           5.63224    0.25312  22.251  < 2e-16 ***\nfactor(item)CARDS     0.18090    0.17646   1.025  0.30545    \nfactor(item)LANGUAGE  1.22864    0.17646   6.963 5.15e-12 ***\nfactor(item)SCHOOL    0.56784    0.17646   3.218  0.00132 ** \nfactor(item)TEETH     0.76633    0.17646   4.343 1.51e-05 ***\nfactor(item)TRAFFIC   1.66332    0.17646   9.426  < 2e-16 ***\nfactor(item)WEDDINGS -0.07035    0.17646  -0.399  0.69019    \nhigh                  0.02024    0.05289   0.383  0.70195    \ncoll                  0.08939    0.05542   1.613  0.10700    \ngrad                  0.05006    0.05094   0.983  0.32589    \nbooks                -0.13683    0.05642  -2.425  0.01543 *  \nmag                  -0.05032    0.05208  -0.966  0.33408    \nknow                  0.10723    0.04940   2.171  0.03013 *  \nneedforcog           -0.22673    0.03696  -6.134 1.12e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.76 on 1379 degrees of freedom\n  (1735 observations deleted due to missingness)\nMultiple R-squared:  0.1362,    Adjusted R-squared:  0.1281 \nF-statistic: 16.73 on 13 and 1379 DF,  p-value: < 2.2e-16\n\n\nThis is neither a true model nor an appropriate analysis to check whether inherence scores related to expertise areas, perceived expertise (know), experience with formal education (high, coll, grad), experience with books or magazines, and tendency to think or intellectual activities (needforcog -need for cognition scale, Cacioppo & Petty, 1982-). However, this superfacial analysis showed that perceived knowledge, books, and need for cognition are related to inherence significantly adjusting other variables. Interestingly, perceived knowledge is positively related although others are negatively related to inherence scores. Speculatively, it reminds me Dunning–Kruger effect (Kruger & Dunning, 1999). People may overestimate their abilities, but judge the explanations with inherence bias. Not suprisingly, people who are seeking for cognitive activities (need for cognition) have lower inherence scores. Those people probably have more motivation to produce and look for detailed explanations. People who read books about the topic have lower inherence scores also. This may tap to the role of recalling in explanation. Although formal education and/or articles (probably including superficial or popular science knowledge) does not guarantee to teach expertise, books may offer more permanent information for long term memory with self-learning."
  },
  {
    "objectID": "posts/Thomas et al. 2008/index.html",
    "href": "posts/Thomas et al. 2008/index.html",
    "title": "Thomas et al. (2008) Diagnostic Hypothesis Generation and Human Judgment",
    "section": "",
    "text": "The focus of this article is providing a model of human judgment (Hygene) that describes\n\nHow hypotheses are generated on the basis of data extracted from the environment\nHow the hypotheses generated from memory are used to make probability judgments\nHow the generated hypotheses frame subsequent information search in hypothesis-testing situations"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Recent Posts",
    "section": "",
    "text": "Thomas et al. (2014) Memory Constraints on Hypothesis Generation and Decision Making\n\n\n\n\n\n\n\narticles\n\n\nnotes\n\n\nnew ideas\n\n\n\n\n\n\n\n\n\n\n\nDec 21, 2022\n\n\nMK\n\n\n\n\n\n\n  \n\n\n\n\nExpertise Data Analysis Trials\n\n\n\n\n\n\n\nexample data\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nDec 21, 2022\n\n\nMK\n\n\n\n\n\n\n  \n\n\n\n\nThomas et al. (2008) Diagnostic Hypothesis Generation and Human Judgment\n\n\n\n\n\n\n\narticles\n\n\nnotes\n\n\nnew ideas\n\n\n\n\n\n\n\n\n\n\n\nDec 21, 2022\n\n\nMK\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This blog includes my notes about articles, example data analysis and other things like a journal, so I call this as learning journal."
  }
]