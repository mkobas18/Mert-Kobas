[
  {
    "objectID": "posts/Thomas et al. 2013/index.html",
    "href": "posts/Thomas et al. 2013/index.html",
    "title": "Thomas et al. (2014) Memory Constraints on Hypothesis Generation and Decision Making",
    "section": "",
    "text": "The focus of this article is hypothesis generation and memory relation. In everyday life, we use hypothesis to produce explanations after we test it. The general idea here is how memory as a cognitive structure constrains hypothesis generation and related decision-making processes.\nThree primary processes can explain this idea:\n\nRetrieval (Long term memory, retrieving memory traces)\nMaintenance (Working memory, sustaining these traces in the active thinking process)\nJudgement (Decision after test these hypotheses)\n\n\n\n\nThomas et al. (2013), page 265, Figure 1\n\n\nOther important term here is hypothesis-guided search. I interpreted this as a top-down evidence looking stage between hypothesis generation and decision making. Medical diagnoses are examples for this.\nComment 1: However, the first hypothesis must be bottom-up or based on the testimony here. So, a medical doctor may see their patient with holding their arm, so the first hypothesis must be that the arm was broken. After this, X-ray, etc. can be used to prove this hypothesis and make a decision.\n\n\nThey mention how experts and other people generate only a small number of these hypotheses (probably due to WM restrictions, they mentioned in their other article: People with high WM produce 4 hypothesis in a time window, but low WM produce around 2.)\nMost likely hypotheses will be produced first (broken arm). New hypotheses are produced, if they better matches to the data (after X-ray screening, it may be only muscle ache).\nQuestion 1: They gave an example here about a mental illness: “Entertaining the hypothesis that the kitchen faucet is communicating with you when the dripping is perfectly consistent with its just being leaky”. Is this a failure of data-hypothesis match or interpreting data based on prior beliefs? (if this would be another context like believing spirituality) If you believe that non-living things can have spirit/can talk/etc., then you may find this more likely (more match with data hypothesis). So beyond data and hypothesis, do our beliefs play a role to distort our probability judgement process?\nAnother interesting point here is related to experience. Experts often recall specific hypothesis (broken bones) because those are activated more often. The article said that these hypotheses have more a priori likelihood. I interpreted this as these hypotheses were seen as more likely in the beginning. So if there are two hypotheses that come to my mind (broken arm vs. shoulder problem in the example of painful arm), I would prioritize broken arm because of my previous experiences.\nComment 2: Does it mean that experts produce some heuristics for explanations (holding arms mean broken bones without any alternatives -alternatives come after testing-)?\nOr expertise still provides more memory space (less cognitive load), so does it mean for more accurate hypothesis generation and testing process? (holding arms may mean broken bone because I see this many times, but alternative hypotheses can be possible but less likely -alternatives bring with the main hypothesis-)\nSo, do experts produce explanation both with inherent and extrinsic features (because they can use their cognitive resources flexibly to produce hypotheses with inherent and extrinsic features) just like the nonspeed condition group in Hussak & Cimpian (2018)? What if under time limitation and LTM restriction?\n\n\n\nThey introduce us here another term: Subadditivity. It means that the sum of the probabilities of sub-arguments in a hypothesis is more than the probability of hypothesis itself. The probabilities here reflect the evaluation of an individual. The probability of broken bones (25%), muscle problems (20%) and carpal tunnel (15%) > the probability of arm problem (50%)\n\n\n\nWith only one hypothesis, it becomes including confirmation search strategy. For example, If a person thought that somebody is married already, they search engagement ring of the other person.\nWith two or more hypothesis, it becomes diagnostic search. For example, if my computer cannot open any Youtube videos, I may think that it is because of my web browser or internet connection or Youtube servers, so I check my connection, I probably try another browser and search internet about servers, so I try to search diagnostically.\nIn the social cases, diagnostic search may be impossible sometimes? like if my partner is angry, I think that it maybe due to work or something that I say in the morning or an irrelevant thing, but checking all of these diagnostically is only possible to ask my partner, but this may make them more angry, so it is impossible?\n\n\n\nTime pressure led to generation of fewer hypotheses.\nQuestion 2a: So time pressure both leads to produce inherent biases in explanation and also less alternative explanations. Are those related somehow (people start with inherent explanations then they add extrinsic ones with alternative hypotheses), or separate processes (less alternative explanations + including mostly inherent features)?\nQuestion 2b: In the education system, we use time pressure to measure expertise (at least the assumption is this) like college entrance exams or language exams. People relate the mistakes in those exams usually with attentional issues (or didn’t have the knowledge) rather than memory problems. It reminds me Horne et al. (2019)’s article here. Time pressure may be a distractor due to related stress, so both hypothesis generation and generated explanations -including mostly inherent biases- were affected by attentional proccesses (focusing effects may become more effective).\nComment 3:Related to question 2b, they also mention here about primacy bias and recency bias. For the explanations, it may mean that people have difficulties the focus on middle part of the data (or an event). As an example, a person may be near the seaside and sees another person going to the sea. When the person after saw that person B as drowning, they explained it as person B doesn’t know swimming (inherent bias) in the first look and try to save them. The recent event is drowning here, however, it may be also waves of sea (extrinsic one). Since there is a time pressure (person b is dying), person A may not care the process of the event.\nAnother important point here is with slow presentation of data, people have recency bias but with fast rate of presentation, primacy bias emerged. Complexity of task or increased working memory capacity leads to increase in primacy bias during hypothesis generation.\nThe last point: People produce good hypotheses to explain the data, but they have also systematic biases (?? can inherent bias be an example?) in beliefs and information search due to memory limitations.\nQuestion 3 (bonus question, theoretical): Does it mean that cognitive biases in explanation generation process are the production of memory constrains? Or do they place in cognitive architecture -due to evoluationary processes, etc.-? Or both of them?"
  },
  {
    "objectID": "posts/Expertise_Data_From_Zero/index.html",
    "href": "posts/Expertise_Data_From_Zero/index.html",
    "title": "Expertise3 Data Analysis from the very beginning",
    "section": "",
    "text": "This post includes the trial analyses of an example data related to expertise.\nImport necessary packages and expertise data\n\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(purrr)\n\nExpertise3 <- read_csv(\"~/Desktop/Expertise3_December 23, 2022_10.53.csv\")\n\nCreates a new dataframe called expertise3_clean, which is a copy of the original dataframe called Expertise3 and removes the second row of the dataframe\n\nexpertise3_clean<-Expertise3%>% \n  slice(-2) \n\nexpertise3_clean <- expertise3_clean %>%\n  select(-StartDate, -EndDate, -Status, -IPAddress, -Progress,-ResponseId,-RecordedDate,\n         -RecipientLastName, -RecipientFirstName,-RecipientEmail, -ExternalReference, \n         -LocationLatitude,-LocationLongitude, -DistributionChannel, -UserLanguage)\n\nCreate a variable called column_names and assign it the names of the columns in the dataframe and change the column names\n\ncolumn_names <- names(expertise3_clean)\ncolnames(expertise3_clean) <- c('duration', 'finished', 'stih_lang','r_ih_lang','stih_school','r_ih_school','stih_cards','r_ih_cards','stih_breakfast','r_ih_breakfast','stih_weddings','r_ih_weddings','stih_teeth','r_ih_teeth','stih_traffic','r_ih_traffic','stih_tv','r_ih_tv', 'know_lang','know_school','know_cards','know_breakfast','know_weddings','know_teeth','know_traffic','know_tv', 'course_lang','high_lang', 'colle_lang', 'grad_lang',\n'book_lang','numbook_lang','article_lang','numarticle_lang','course_school','high_school', 'colle_school', 'grad_school','book_school','numbook_school','article_school','numarticle_school','course_cards','high_cards', 'colle_cards', 'grad_cards',\n 'book_cards','numbook_cards','article_cards','numarticle_cards','course_breakfast','high_breakfast', 'colle_breakfast', 'grad_breakfast','book_breakfast','numbook_breakfast','article_breakfast','numarticle_breakfast','course_weddings','high_weddings', 'colle_weddings', 'grad_weddings','book_weddings','numbook_weddings','article_weddings','numarticle_weddings','course_teeth','high_teeth', 'colle_teeth', 'grad_teeth', 'book_teeth','numbook_teeth','article_teeth','numarticle_teeth','course_traffic','high_traffic', 'colle_traffic', 'grad_traffic','book_traffic','numbook_traffic','article_traffic','numarticle_traffic','course_tv','high_tv', 'colle_tv', 'grad_tv','book_tv','numbook_tv','article_tv','numarticle_tv','needforcog1','needforcog2','needforcog3','needforcog4','needforcog5','needforcog6','needforcog7','needforcog8','needforcog9','needforcog10','needforcog11','needforcog12','needforcog13','needforcog14','needforcog15','needforcog16','needforcog17','needforcog18','otherways','sex','birthdate','education','income','religion','identity','age','political_atti','english_level','proceure_confu','whatwestudied','moretothisstudy','additional_thoughts','attention')\n\nThe code below shows the survey items:\n\n#selects all columns except the ones specified\nrow_values <- expertise3_clean %>%\n  select(-duration,-finished)%>%\n  #selects only the first row\n  filter(row_number() == 1)\n\n#unlist the row_values\nrow_values <- unlist(row_values)\n\nmy_list <- map(row_values, ~paste0(.))\n\nlibrary(stringr)\nmy_list <- str_replace(my_list, \"(?<! )\\\\n(?! )\", \" \")\nmy_list <- str_replace(my_list, \"[^\\\\s]*\\\\\\\\n[^\\\\s]*\", \" \")\nlist_string <- paste0(\"* \", paste(my_list, collapse = \"\\n* \"))\n#Show the survey items\ncat(list_string)\n\n* Human languages are structured to best convey our thoughts and feelings. Words and their meanings likely form ideal matches.\n* There are absolutely no good reasons why we use specific words to represent our thoughts. Any combination of sounds\ncould in principle refer to any idea.\n* The fact that elementary school stops at 5th grade is probably ideal for children's learning. This is likely the best way to organize K-12 schooling.\n* Middle school (grades 6-8) is separate from elementary school (grades K-5) largely because of decisions made by educators a long time ago. This may not be the most optimal way of organizing early education.\n* It’s not a coincidence that we send people cards on holidays. This tradition seems\nparticularly fitting.\n* The fact that we send people cards on holidays is only a convention. A different way of sending warm wishes could've been implemented just as easily.\n* There are good reasons why orange juice is typically consumed for breakfast. There are features about it that make it\nparticularly suited for this meal (for example, its refreshing taste).\n* The current popularity of orange juice for breakfast reflects in part  marketing campaigns that promoted\ndrinking orange juice in the morning. However, had history taken a different\nturn, orange juice could just as easily have been more popular for lunch or\ndinner.\n* It seems right to use white for wedding dresses. Other colors, such as red and blue, have features that make them less suited for\nwedding dresses.\n* Even though white is the traditional color for wedding dresses, this could have easily been different.\nWhen you really think about it, there is no reason why other, brighter, colors couldn’t\nbe used for wedding dresses.\n* It seems ideal that toothpaste is typically flavored with mint. Mint is inherently more refreshing than any other\nflavor that currently exists.\n* When you think about it, toothpaste could have easily been flavored with something other than mint, such as cinnamon. Many pleasing flavors would work just as\nwell.\n* Traffic lights, with three different colored lights signaling three speeds, seem like the most efficient\nand effective way to direct traffic. Another process likely would not work as\nwell.\n* The current design of traffic lights, with three different colors reflecting three different speeds, is entirely due\nto historical factors. This is probably not the most efficient or effective way\nto manage traffic.\n* Black seems like a good choice for the color of televisions. Other colors just would not work as well.\n* The only reason why most TVs are black is\nhistorical happenstance. TVs could practically be a\nvariety of colors.\n* How much do you know about language and linguistics? - Please use the slider to select your answer choice.\n* How much do you know about school and education systems? - Please use the slider to select your answer choice.\n* How much do you know about holiday customs and traditions? - Please use the slider to select your answer choice.\n* How much do you know about breakfast foods? - Please use the slider to select your answer choice.\n* How much do you know about weddings and wedding traditions? - Please use the slider to select your answer choice.\n* How much do you know about teeth and oral hygiene? - Please use the slider to select your answer choice.\n* How much do you know about transportation science and traffic signal systems? - Please use the slider to select your answer choice.\n* How much do you know about the manufacturing of consumer electronics (TVs, MP3 players, camcorders, etc.)? - Please use the slider to select your answer choice.\n* Have you ever taken a class that discussed language and linguistics?\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - High School\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - College\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - Graduate School\n* Have you ever read any books on language and linguistics?\n* If you answered \"yes\" above, please approximate the number of books you have read on this topic:\n* Have you ever read any magazine, newspaper, or academic articles on language and linguistics?\n* If you answered \"yes\" above, please approximate the number of articles you have read on this topic:\n* Have you ever taken a class that discussed school and education systems?\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - High School\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - College\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - Graduate School\n* Have you ever read any books on school and education systems?\n* If you answered \"yes\" above, please approximate the number of books you have read on this topic:\n* Have you ever read any magazine, newspaper, or academic articles on school and education systems?\n* If you answered \"yes\" above, please approximate the number of articles you have read on this topic:\n* Have you ever taken a class that discussed holiday customs and traditions?\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - High School\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - College\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - Graduate School\n* Have you ever read any books on holiday customs and traditions?\n* If you answered \"yes\" above, please approximate the number of books you have read on this topic:\n* Have you ever read any magazine, newspaper, or academic articles on holiday customs and traditions?\n* If you answered \"yes\" above, please approximate the number of articles you have read on this topic:\n* Have you ever taken a class that discussed breakfast foods?\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - High School\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - College\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - Graduate School\n* Have you ever read any books on breakfast foods?\n* If you answered \"yes\" above, please approximate the number of books you have read on this topic:\n* Have you ever read any magazine, newspaper, or academic articles on breakfast foods?\n* If you answered \"yes\" above, please approximate the number of articles you have read on this topic:\n* Have you ever taken a class that discussed weddings and wedding traditions?\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - High School\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - College\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - Graduate School\n* Have you ever read any books on weddings and wedding traditions?\n* If you answered \"yes\" above, please approximate the number of books you have read on this topic:\n* Have you ever read any magazine, newspaper, or academic articles on weddings and wedding traditions?\n* If you answered \"yes\" above, please approximate the number of articles you have read on this topic:\n* Have you ever taken a class that discussed teeth and oral hygiene?\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - High School\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - College\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - Graduate School\n* Have you ever read any books on teeth and oral hygiene?\n* If you answered \"yes\" above, please approximate the number of books you have read on this topic:\n* Have you ever read any magazine, newspaper, or academic articles on teeth and oral hygiene?\n* If you answered \"yes\" above, please approximate the number of articles you have read on this topic:\n* Have you ever taken a class that discussed transportation science and traffic signal systems?\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - High School\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - College\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - Graduate School\n* Have you ever read any books on transportation science and traffic signal systems?\n* If you answered \"yes\" above, please approximate the number of books you have read on this topic:\n* Have you ever read any magazine, newspaper, or academic articles on transportation science and traffic signal systems?\n* If you answered \"yes\" above, please approximate the number of articles you have read on this topic:\n* Have you ever taken a class that discussed the manufacturing of consumer electronics (TVs, MP3 players, camcorders, etc.)?\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - High School\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - College\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - Graduate School\n* Have you ever read any books on the manufacturing of consumer electronics (TVs, MP3 players, camcorders, etc.)?\n* If you answered \"yes\" above, please approximate the number of books you have read on this topic:\n* Have you ever read any magazine, newspaper, or academic articles on the manufacturing of consumer electronics (TVs, MP3 players, camcorders, etc.)?\n* If you answered \"yes\" above, please approximate the number of articles you have read on this topic:\n* I would prefer complex to simple problems.\n* I like to have the responsibility of handling a situation that requires a lot of thinking.\n* Thinking is not my idea of fun.\n* I would rather do something that requires little thought than something that is sure to challenge my thinking abilities.\n* I try to anticipate and avoid situations where there is a likely chance I will have to think in depth about something.\n* I find satisfaction in deliberating hard and long for hours.\n* I only think as hard as I have to.\n* I prefer to think about small, daily projects rather than long-term ones.\n* I like tasks that require little thought once I’ve learned them.\n* The idea of relying on thought to make my way to the top appeals to me.\n* I really enjoy a task that involves coming up with new solutions to problems.\n* Learning new ways to think doesn’t excite me very much.\n* I prefer my life to be filled with puzzles that I must solve.\n* The notion of thinking abstractly appeals to me.\n* I would prefer a task that is intellectual, difficult, and important to one that is somewhat important but does not require much thought.\n* I feel relief rather than satisfaction after completing a task that required a lot of mental effort.\n* It’s enough for me that something gets the job done; I don’t care about why or how it works.\n* I usually end up deliberating about issues even when they do not affect me personally.\n* In this survey you were asked about your knowledge in a variety of subject areas. \nAside from the ways you may have learned about these topics that were mentioned in the survey (i.e., reading or  a college class), how else may you have learned about these topics? Please name other ways that you have learned about the topics mentioned in the survey you just took.\n* Are you male or female?\n* Q60 - What is your date of birth? (mm/dd/yyyy)\n* What is the highest level of education you have completed?\n* Q64 - What is your yearly household income?\n* Q64 - What is your religious affiliation?\n* Q64 - What is your racial or ethnic identity?\n* Q64 - What is your age in years?\n* How would you describe your political attitudes? Please select one of the points on the scale below.\n* Please rate your overall ability in the English language:\n* 1. Did you find any aspect of the procedure odd or confusing?\n* 2. What did you think we were studying?\n* 3. Do you think that there may have been more to this study than meets the eye? If so, what do you think this might have been?\n* 4. Do you have any additional thoughts or comments about the study?\n* Thank you for completing this survey! We just have one last question for you. You will not be penalized for your answer to this question. Since you completed the whole survey, you will receive payment no matter what answer you give here. \n     \n\n    It's very important to the quality and scientific aims of our study that participants pay attention (i.e., read the survey carefully, consider the response options, and avoid distractions).\n\n    \n\n    Were you paying attention while completing this survey?\n\n\nData preparation for further analyses\n\n#Attention check and deletion of cases that didn't attend or finish the study\nexpertise3_new<-expertise3_clean%>%\n  filter(attention==1&finished==1)\n\n#Exclude the variables that will not be used in the later analyses\nexpertise3_new<-expertise3_new%>%\n  select(-finished,-otherways,-birthdate,-proceure_confu,-whatwestudied,-moretothisstudy,-additional_thoughts,-attention)\n\n#adds a column to the dataframe, with the name \"id\"\nexpertise3_new<-cbind(ID = 1:nrow(expertise3_new), expertise3_new)\n\n# change the data type of the variables to numeric\nexpertise3_new <- expertise3_new %>%\n  mutate_at(vars(stih_lang, r_ih_lang, stih_school, r_ih_school, stih_cards, r_ih_cards, stih_breakfast, r_ih_breakfast, \n                 stih_weddings, r_ih_weddings, stih_teeth, r_ih_teeth, stih_traffic, r_ih_traffic, stih_tv, r_ih_tv), as.numeric)\n\n# Create a list of variable names\nvariables <- c(\"stih_lang\", \"r_ih_lang\", \"stih_school\", \"r_ih_school\", \"stih_cards\", \"r_ih_cards\", \"stih_breakfast\", \"r_ih_breakfast\", \"stih_weddings\", \"r_ih_weddings\", \"stih_teeth\", \"r_ih_teeth\", \"stih_traffic\", \"r_ih_traffic\", \"stih_tv\", \"r_ih_tv\")\n\nCheck the correlations between inherence (the variables starting with st) and reverse inherence (the variables starting with r) scores to check whether it’s appropriate for averaging\n\n# Initialize an empty data frame to store the correlation coefficients\ncorrelations <- data.frame(variable1 = character(), variable2 = character(), correlation = numeric(), p.value = numeric(), conf.int = character())\n\n# Iterate over the pairs of variables\nfor (i in seq(1, length(variables), 2)) {\n  j <- i + 1\n  \n  # Calculate the Pearson correlation coefficient and test the statistical significance\n  correlation_test <- cor.test(expertise3_new[, variables[i]], expertise3_new[, variables[j]], method = \"pearson\")\n  \n  # Add the correlation coefficient, p-value, and confidence interval to the data frame\n  correlations <- rbind(correlations, data.frame(variable1 = variables[i], variable2 = variables[j], correlation = correlation_test$estimate, p.value = correlation_test$p.value, conf.int = paste(correlation_test$conf.int[1], correlation_test$conf.int[2], sep = \" - \")))\n}\n\n# View the correlation coefficients and statistical measures\ncorrelations\n\n          variable1      variable2 correlation      p.value\ncor       stih_lang      r_ih_lang  -0.6068527 6.542313e-22\ncor1    stih_school    r_ih_school  -0.5223201 1.125194e-15\ncor2     stih_cards     r_ih_cards  -0.5433698 4.572854e-17\ncor3 stih_breakfast r_ih_breakfast  -0.3592235 1.317523e-07\ncor4  stih_weddings  r_ih_weddings  -0.5284362 4.539085e-16\ncor5     stih_teeth     r_ih_teeth  -0.4440395 3.226080e-11\ncor6   stih_traffic   r_ih_traffic  -0.4869944 1.514193e-13\ncor7        stih_tv        r_ih_tv  -0.3471023 3.653644e-07\n                                    conf.int\ncor  -0.686956408754957 - -0.512178848971074\ncor1  -0.61552619526787 - -0.414704679541722\ncor2 -0.633457573965096 - -0.438748248188404\ncor3 -0.473241603677093 - -0.233368287331007\ncor4 -0.620746208212983 - -0.421675200702786\ncor5 -0.548226097515688 - -0.326281347959786\ncor6  -0.58521523210812 - -0.374689532050954\ncor7 -0.462424268063511 - -0.220232239305416\n\n\nIt seems that each pairs have negative significant correlation, so we can take the average scores to calculate inherence scores\n\nexpertise3_new <- expertise3_new %>%\n  mutate(ih_lang = (stih_lang + (10 - r_ih_lang)) / 2,\n    ih_school = (stih_school + (10 - r_ih_school)) / 2,\n    ih_cards = (stih_cards + (10 - r_ih_cards)) / 2,\n    ih_breakfast = (stih_breakfast + (10 - r_ih_breakfast)) / 2,\n    ih_weddings = (stih_weddings + (10 - r_ih_weddings)) / 2,\n    ih_teeth = (stih_teeth + (10 - r_ih_teeth)) / 2,\n    ih_traffic = (stih_traffic + (10-r_ih_traffic)) / 2,\n    ih_tv = (stih_tv + (10-r_ih_tv)) / 2 )\n\nCalculate “Need for cognition” scale scores\n\n# change the data type of the variables to numeric\nexpertise3_new <- expertise3_new %>%\n  mutate_at(vars(needforcog1,needforcog2,needforcog3,needforcog4,needforcog5,needforcog6\n                 ,needforcog7,needforcog8,needforcog9,needforcog10,needforcog11,needforcog12,\n                   needforcog13,needforcog14,needforcog15,needforcog16,needforcog17,needforcog18), as.numeric)\n\n#add a new variable called needforcog, which is the sum of all the need for cognition items, the items are summed or extracted according to the normal or reverse items\nexpertise3_new <- expertise3_new %>%\n  mutate(needforcog=(needforcog1+needforcog2-needforcog3-needforcog4-needforcog5+needforcog6\n         -needforcog7-needforcog8-needforcog9+needforcog10+needforcog11-needforcog12+\n         needforcog13+needforcog14+needforcog15+needforcog16-needforcog17+needforcog18))\n\nPrepare the expertise scores and other scores ready for analyses\n\n# Replace all NA values in the expertise columns with 0\nexpertise3_new <- apply(expertise3_new, 2, function(x) ifelse(is.na(x), 0, x))\n# Turn the output to a dataframe\nexpertise3_new <- data.frame(expertise3_new)\n\n# Create new data frame as analyzable \nexpertise3_new<-expertise3_new%>%\n  select(-stih_lang,-r_ih_lang,-stih_school,-r_ih_school,\n         -stih_cards,-r_ih_cards,-stih_breakfast,-r_ih_breakfast,-stih_weddings,-r_ih_weddings,\n         -stih_teeth,-r_ih_teeth,-stih_traffic,-r_ih_traffic,-stih_tv,-r_ih_tv,\n         -course_lang,\n         -book_lang,-article_lang,\n         -course_school,\n         -book_school,-article_school,\n         -course_cards,\n         -book_cards,-article_cards,\n         -course_breakfast,\n         -book_breakfast,-article_breakfast,\n         -course_weddings,\n         -book_weddings,-article_weddings,\n         -course_teeth,\n         -book_teeth,-article_teeth,\n         -course_traffic,\n         -book_traffic,-article_traffic,\n         -course_tv,\n         -book_tv,-article_tv,\n         -needforcog1,-needforcog2,-needforcog3,-needforcog4,-needforcog5,-needforcog6,\n         -needforcog7,-needforcog8,-needforcog9,-needforcog10,-needforcog11,-needforcog12,\n         -needforcog13,-needforcog14,-needforcog15,-needforcog16,-needforcog17,-needforcog18)\n\n\n\n# change the data type of the variables to numeric\nexpertise3_new <- expertise3_new %>%\n  mutate_at(vars('know_lang','know_school','know_cards','know_breakfast',\n                 'know_weddings','know_teeth','know_traffic','know_tv',\n                 'high_lang', 'colle_lang', 'grad_lang',\n                 'numbook_lang','numarticle_lang',\n                 'high_school', 'colle_school', 'grad_school',\n                 'numbook_school','numarticle_school',\n                 'high_cards', 'colle_cards', 'grad_cards',\n                 'numbook_cards','numarticle_cards',\n                 'high_breakfast', 'colle_breakfast', 'grad_breakfast',\n                 'numbook_breakfast','numarticle_breakfast',\n                 'high_weddings', 'colle_weddings', 'grad_weddings',\n                 'numbook_weddings','numarticle_weddings',\n                 'high_teeth', 'colle_teeth', 'grad_teeth',\n                 'numbook_teeth','numarticle_teeth',\n                 'high_traffic', 'colle_traffic', 'grad_traffic',\n                 'numbook_traffic','numarticle_traffic',\n                 'high_tv', 'colle_tv', 'grad_tv',\n                 'numbook_tv','numarticle_tv'), as.numeric)\n\n# Select the variables to standardize\nvars_to_standardize <- c('know_lang','know_school','know_cards','know_breakfast',\n                         'know_weddings','know_teeth','know_traffic','know_tv',\n                         'high_lang', 'colle_lang', 'grad_lang',\n                         'numbook_lang','numarticle_lang',\n                         'high_school', 'colle_school', 'grad_school',\n                         'numbook_school','numarticle_school',\n                         'high_cards', 'colle_cards', 'grad_cards',\n                         'numbook_cards','numarticle_cards',\n                         'high_breakfast', 'colle_breakfast', 'grad_breakfast',\n                         'numbook_breakfast','numarticle_breakfast',\n                         'high_weddings', 'colle_weddings', 'grad_weddings',\n                         'numbook_weddings','numarticle_weddings',\n                         'high_teeth', 'colle_teeth', 'grad_teeth',\n                         'numbook_teeth','numarticle_teeth',\n                         'high_traffic', 'colle_traffic', 'grad_traffic',\n                         'numbook_traffic','numarticle_traffic',\n                         'high_tv', 'colle_tv', 'grad_tv',\n                         'numbook_tv','numarticle_tv')\n\n# Standardize the selected variables\nexpertise3_new[, vars_to_standardize] <- scale(expertise3_new[, vars_to_standardize])\n\n# Now we are ready for analysis\n\nhead(expertise3_new)\n\n   ID duration   know_lang know_school know_cards know_breakfast know_weddings\n1   1      403 -0.92933457 -0.09219094 -1.1258315      0.7943452    -0.1799059\n2   2      473  1.39400185 -0.09219094 -0.7512429     -0.6065023    -0.2230326\n3   3      442 -1.60521425 -2.05124849 -1.8750087     -2.2332930    -1.4305799\n4   4      337  0.04224248 -0.13572556 -0.1893600     -0.6065023     0.2513609\n5   5      376  0.38018232  0.82203591  0.1852286      0.7039680    -0.9130596\n6   6      618  0.04224248 -0.13572556  1.1685236      1.0202884     1.8039217\n  know_teeth know_traffic    know_tv  high_lang colle_lang  grad_lang\n1 -1.5832390   0.40296860  0.1980935 -0.6364644 -0.5394097 -0.1521967\n2 -0.2551931  -0.01538048  1.3258444 -0.6364644 -0.5394097 -0.1521967\n3 -2.2029937  -1.06125318 -0.8168823 -0.6364644 -0.5394097 -0.1521967\n4  0.6744390   0.36113369 -0.1778235 -0.6364644 -0.5394097 -0.1521967\n5  1.1613891  -0.01538048  0.8371523 -0.1776703 -0.5394097 -0.1521967\n6 -0.7864115  -0.72657392  1.9273115 -0.6364644 -0.5394097 -0.1521967\n  numbook_lang numarticle_lang high_school colle_school grad_school\n1  -0.49509859      -0.2633927  -0.2455616   -0.3847762  -0.1141508\n2  -0.49509859      -0.2633927  -0.2455616   -0.3847762  -0.1141508\n3  -0.49509859      -0.2633927  -0.2455616   -0.3847762  -0.1141508\n4  -0.49509859      -0.2633927  -0.2455616   -0.3847762  -0.1141508\n5  -0.07774275      -0.2633927  -0.2455616   -0.3847762  -0.1141508\n6  -0.49509859      -0.2633927  -0.2455616   -0.3847762  -0.1141508\n  numbook_school numarticle_school high_cards colle_cards grad_cards\n1     -0.3351031        -0.2475552 -0.2950077   -0.175091 -0.1028024\n2     -0.3351031        -0.2475552 -0.2950077   -0.175091 -0.1028024\n3     -0.3351031        -0.2475552 -0.2950077   -0.175091 -0.1028024\n4     -0.3351031        -0.2475552 -0.2950077   -0.175091 -0.1028024\n5     -0.3351031        -0.2475552 -0.2950077   -0.175091 -0.1028024\n6     -0.3351031         9.3827397 -0.2950077   -0.175091 -0.1028024\n  numbook_cards numarticle_cards high_breakfast colle_breakfast grad_breakfast\n1    -0.3549311       -0.3570679      0.4510718      -0.2243077     -0.1218696\n2    -0.3549311       -0.3570679     -0.2910141      -0.2243077     -0.1218696\n3    -0.3549311       -0.3570679     -0.2910141      -0.2243077     -0.1218696\n4     0.6479213       -0.1076095     -0.2910141      -0.2243077     -0.1218696\n5    -0.3549311       -0.3570679     -0.2910141      -0.2243077     -0.1218696\n6    -0.3549311       -0.3570679     -0.2910141      -0.2243077     -0.1218696\n  numbook_breakfast numarticle_breakfast high_weddings colle_weddings\n1         -0.235773           -0.2393171    -0.1795269     -0.2260702\n2         -0.235773           -0.2393171    -0.1795269     -0.2260702\n3         -0.235773           -0.2393171    -0.1795269     -0.2260702\n4         -0.235773           -0.2393171    -0.1795269     -0.2260702\n5         -0.235773           -0.2393171    -0.1795269     -0.2260702\n6         -0.235773           -0.2393171    -0.1795269     -0.2260702\n  grad_weddings numbook_weddings numarticle_weddings high_teeth colle_teeth\n1   -0.09411928       -0.3825866          -0.4102006 -0.3312423  -0.2490223\n2   -0.09411928       -0.3825866          -0.4102006 -0.3312423  -0.2490223\n3   -0.09411928       -0.3825866          -0.4102006 -0.3312423  -0.2490223\n4   -0.09411928       -0.3825866          -0.4102006 -0.3312423  -0.2490223\n5   -0.09411928       -0.3825866          -0.4102006 -0.3312423  -0.2490223\n6   -0.09411928       -0.3825866          -0.4102006 -0.3312423  -0.2490223\n   grad_teeth numbook_teeth numarticle_teeth high_traffic colle_traffic\n1 -0.09925954    -0.2779491      -0.16517115   -0.2045128    -0.1362734\n2 -0.09925954    -0.2779491      -0.16517115   -0.2045128    -0.1362734\n3 -0.09925954    -0.2779491      -0.16517115   -0.2045128    -0.1362734\n4 -0.09925954     1.3192794      -0.05229037   -0.2045128    -0.1362734\n5 -0.09925954    -0.2779491      -0.16517115   -0.2045128    -0.1362734\n6 -0.09925954    -0.2779491      -0.16517115   -0.2045128    -0.1362734\n  grad_traffic numbook_traffic numarticle_traffic    high_tv   colle_tv grad_tv\n1    -0.070014       -0.254263         -0.1112561 -0.1611468 -0.1414978     NaN\n2    -0.070014       -0.254263         -0.1112561 -0.1611468 -0.1414978     NaN\n3    -0.070014       -0.254263         -0.1112561 -0.1611468 -0.1414978     NaN\n4    -0.070014       -0.254263         -0.1112561 -0.1611468 -0.1414978     NaN\n5    -0.070014       -0.254263         -0.1112561 -0.1611468 -0.1414978     NaN\n6    -0.070014       -0.254263         -0.1112561 -0.1611468 -0.1414978     NaN\n  numbook_tv numarticle_tv sex education income religion identity age\n1 -0.2473571    -0.1178382   1         3 105000     none    white  26\n2 -0.2473571    -0.1178382   1         5  53000  atheist    white  38\n3 -0.2473571    -0.1178382   2         2 15.000     none    white  22\n4 -0.2473571    -0.1178382   2         4 150000   jewish    white  55\n5 -0.2473571    -0.1178382   1         3  45000     None    White  29\n6 -0.2473571    -0.1178382   1         3  60000 catholic    white  58\n  political_atti english_level ih_lang ih_school ih_cards ih_breakfast\n1              4             4     4.0       4.0      2.5          4.5\n2              2             4     5.0       5.0      5.0          5.0\n3              5             4     6.0       4.5      4.0          5.5\n4              5             4     5.0       2.5      2.0          3.0\n5              2             4     4.5       4.0      3.0          6.0\n6              4             4     5.5       3.5      5.0          3.5\n  ih_weddings ih_teeth ih_traffic ih_tv needforcog\n1         3.5      8.0        8.5   9.0         43\n2         5.0      5.0        5.0   5.0         11\n3         5.5      6.5        6.0   4.5          7\n4         1.0      1.0        5.0   1.0         17\n5         3.5      5.5        5.0   4.0         47\n6         3.5      5.5        4.5   4.0         10\n\n\nTransform the data to the long format for analyses:\n\nlibrary('reshape2')\n\nexpertise3_long <- melt(expertise3_new, id.vars = c(\"ID\",'duration',\"sex\" ,\"education\" ,\"income\",\"religion\",'identity','age','political_atti','english_level','needforcog'),\n          measure.vars = c(\"ih_lang\", \"ih_school\",\"ih_cards\", \"ih_breakfast\", \"ih_weddings\", \n\"ih_teeth\", \"ih_traffic\", \"ih_tv\",'know_lang','know_school','know_cards','know_breakfast',\n'know_weddings','know_teeth','know_traffic','know_tv','high_lang', 'colle_lang', 'grad_lang',\n'numbook_lang','numarticle_lang','high_school', 'colle_school', 'grad_school',\n'numbook_school','numarticle_school','high_cards', 'colle_cards', 'grad_cards',\n'numbook_cards','numarticle_cards', 'high_breakfast', 'colle_breakfast', 'grad_breakfast',\n'numbook_breakfast','numarticle_breakfast','high_weddings', 'colle_weddings', 'grad_weddings','numbook_weddings','numarticle_weddings','high_teeth', 'colle_teeth', 'grad_teeth',\n'numbook_teeth','numarticle_teeth','high_traffic', 'colle_traffic', 'grad_traffic',\n'numbook_traffic','numarticle_traffic','high_tv', 'colle_tv', 'grad_tv','numbook_tv','numarticle_tv'),sep = \"_\", variable.name = \"Category\", value.name = \"Score\")\n\n# Split the Category column into two columns based on the underscore separator\nexpertise3_long <- expertise3_long %>% separate(Category, into = c(\"Category\", \"Score_Type\"), sep = \"_\")\n\n#spread the data from long to wide format\nexpertise3_ready <- expertise3_long %>% spread(Category, Score)\n\n#change the score type to a factor\nexpertise3_ready$Score_Type<-as.factor(expertise3_ready$Score_Type)\n\nexpertise3_ready$ih<-as.numeric(expertise3_ready$ih)\n\n\n#view the data\nhead(expertise3_ready)\n\n   ID duration sex education income religion identity age political_atti\n1   1      403   1         3 105000     none    white  26              4\n2   2      473   1         5  53000  atheist    white  38              2\n3   3      442   2         2 15.000     none    white  22              5\n4   4      337   2         4 150000   jewish    white  55              5\n5   5      376   1         3  45000     None    White  29              2\n6   6      618   1         3  60000 catholic    white  58              4\n  english_level needforcog Score_Type              colle               grad\n1             4         43       lang -0.539409671771391 -0.152196749527831\n2             4         11       lang -0.539409671771391 -0.152196749527831\n3             4          7       lang -0.539409671771391 -0.152196749527831\n4             4         17       lang -0.539409671771391 -0.152196749527831\n5             4         47       lang -0.539409671771391 -0.152196749527831\n6             4         10       lang -0.539409671771391 -0.152196749527831\n                high  ih               know         numarticle\n1 -0.636464435362702 4.0 -0.929334565418051 -0.263392653035181\n2 -0.636464435362702 5.0   1.39400184812708 -0.263392653035181\n3 -0.636464435362702 6.0  -1.60521424935845 -0.263392653035181\n4 -0.636464435362702 5.0 0.0422424802462751 -0.263392653035181\n5 -0.177670284076514 4.5  0.380182322216475 -0.263392653035181\n6 -0.636464435362702 5.5 0.0422424802462751 -0.263392653035181\n              numbook\n1  -0.495098592648002\n2  -0.495098592648002\n3  -0.495098592648002\n4  -0.495098592648002\n5 -0.0777427542174549\n6  -0.495098592648002\n\n\nWe can start to analyze our models with using hlm:\n\n## Start to analyze\n\nlibrary(lme4) \nModel.Null<-lmer(ih ~1+(1|Score_Type)+(1|ID),  \n                 data=expertise3_ready)\nsummary(Model.Null)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: ih ~ 1 + (1 | Score_Type) + (1 | ID)\n   Data: expertise3_ready\n\nREML criterion at convergence: 6485.1\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.99061 -0.64339 -0.00129  0.60758  2.86910 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.5764   0.7592  \n Score_Type (Intercept) 0.3589   0.5991  \n Residual               2.7064   1.6451  \nNumber of obs: 1632, groups:  ID, 204; Score_Type, 8\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)   4.8768     0.2221   21.95\n\n# get pseudo R^2\nperformance::icc(Model.Null)\n\n# Intraclass Correlation Coefficient\n\n    Adjusted ICC: 0.257\n  Unadjusted ICC: 0.257\n\n\n\n# Now add infoavoid as a fixed effect\n# same slope for all sites\nModel.1<-lmer(ih ~needforcog+(1|Score_Type)+(1|ID),   \n              data=expertise3_ready)\nsummary(Model.1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: ih ~ needforcog + (1 | Score_Type) + (1 | ID)\n   Data: expertise3_ready\n\nREML criterion at convergence: 6313.2\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.4258 -0.6418 -0.0155  0.6080  2.8740 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.4809   0.6935  \n Score_Type (Intercept) 0.3589   0.5991  \n Residual               2.7064   1.6451  \nNumber of obs: 1632, groups:  ID, 204; Score_Type, 8\n\nFixed effects:\n              Estimate Std. Error t value\n(Intercept)     6.0313     0.6741   8.947\nneedforcog  1  -0.9063     1.1085  -0.818\nneedforcog  2  -1.0313     1.1085  -0.930\nneedforcog  3  -2.1250     0.9051  -2.348\nneedforcog  4  -1.0729     0.8262  -1.299\nneedforcog  5  -1.0313     1.1085  -0.930\nneedforcog  6  -0.5938     1.1085  -0.536\nneedforcog  7  -2.0313     0.9051  -2.244\nneedforcog  8  -0.7813     0.7838  -0.997\nneedforcog  9  -2.3438     1.1085  -2.114\nneedforcog -1  -0.6563     1.1085  -0.592\nneedforcog -2  -0.6563     1.1085  -0.592\nneedforcog -3  -0.4688     0.9051  -0.518\nneedforcog -4  -0.4063     1.1085  -0.366\nneedforcog -5  -0.9063     1.1085  -0.818\nneedforcog -6  -0.4688     0.9051  -0.518\nneedforcog -7  -1.9063     1.1085  -1.720\nneedforcog -9  -0.2813     1.1085  -0.254\nneedforcog 10  -0.9375     0.7155  -1.310\nneedforcog 11  -0.3259     0.7257  -0.449\nneedforcog 12  -0.9509     0.7257  -1.310\nneedforcog 13  -1.3813     0.7573  -1.824\nneedforcog 14  -1.1146     0.8262  -1.349\nneedforcog 15  -0.6875     0.9051  -0.760\nneedforcog 16  -1.3646     0.8262  -1.652\nneedforcog 17  -1.2634     0.7257  -1.741\nneedforcog 18  -1.1979     0.8262  -1.450\nneedforcog 19  -0.5000     0.7838  -0.638\nneedforcog 20  -1.5313     0.8262  -1.853\nneedforcog 21  -1.2188     0.7573  -1.609\nneedforcog 22  -1.4375     0.7838  -1.834\nneedforcog 23  -1.8438     0.9051  -2.037\nneedforcog 24  -0.9688     0.9051  -1.070\nneedforcog 25  -1.5938     1.1085  -1.438\nneedforcog 26  -1.0313     0.8262  -1.248\nneedforcog 27  -1.4688     1.1085  -1.325\nneedforcog 28  -0.7344     0.7838  -0.937\nneedforcog 29  -0.7604     0.8262  -0.920\nneedforcog 30  -1.5156     0.7838  -1.934\nneedforcog 31  -0.3125     0.9051  -0.345\nneedforcog 32  -0.2396     0.8262  -0.290\nneedforcog 33  -0.9479     0.8262  -1.147\nneedforcog 34  -1.3229     0.7390  -1.790\nneedforcog 35  -0.8938     0.7573  -1.180\nneedforcog 36  -1.1354     0.8262  -1.374\nneedforcog 37  -0.9219     0.7838  -1.176\nneedforcog 38  -2.1563     1.1085  -1.945\nneedforcog 39  -0.9271     0.8262  -1.122\nneedforcog 40  -1.5000     0.9051  -1.657\nneedforcog 41  -0.4063     0.9051  -0.449\nneedforcog 42  -1.3438     1.1085  -1.212\nneedforcog 43  -0.8229     0.8262  -0.996\nneedforcog 44   1.0937     1.1085   0.987\nneedforcog 45  -0.5063     0.7573  -0.669\nneedforcog 46  -2.2031     0.7838  -2.811\nneedforcog 47  -1.2656     0.7838  -1.615\nneedforcog 48  -1.3646     0.8262  -1.652\nneedforcog 50  -1.4688     0.9051  -1.623\nneedforcog 51  -1.6979     0.8262  -2.055\nneedforcog 52  -2.0000     0.7390  -2.706\nneedforcog 53  -1.8125     0.7838  -2.312\nneedforcog 54  -1.6563     1.1085  -1.494\nneedforcog 55  -3.9063     1.1085  -3.524\nneedforcog 56  -2.3438     0.7838  -2.990\nneedforcog 57  -0.7813     0.9051  -0.863\nneedforcog 58  -1.7813     0.9051  -1.968\nneedforcog 59  -0.9063     1.1085  -0.818\nneedforcog 60  -1.3750     0.9051  -1.519\nneedforcog 66  -1.0000     0.9051  -1.105\nneedforcog 68  -0.6563     1.1085  -0.592\nneedforcog 69  -2.4688     1.1085  -2.227\nneedforcog 70  -3.0938     1.1085  -2.791\nneedforcog 74  -3.0313     1.1085  -2.734\nneedforcog 78  -1.9063     1.1085  -1.720\nneedforcog-21   0.4687     1.1085   0.423\nneedforcog-30   0.7812     1.1085   0.705\nneedforcog-34  -1.4063     0.9051  -1.554\nneedforcog-36  -0.7813     1.1085  -0.705\n\n# get pseudo R^2\nperformance::icc(Model.1)\n\n# Intraclass Correlation Coefficient\n\n    Adjusted ICC: 0.237\n  Unadjusted ICC: 0.213\n\n\n\n# compare the two models\n\nanova(Model.Null,Model.1)\n\nrefitting model(s) with ML (instead of REML)\n\n\nData: expertise3_ready\nModels:\nModel.Null: ih ~ 1 + (1 | Score_Type) + (1 | ID)\nModel.1: ih ~ needforcog + (1 | Score_Type) + (1 | ID)\n           npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)   \nModel.Null    4 6491.9 6513.5 -3241.9   6483.9                        \nModel.1      81 6526.6 6963.8 -3182.3   6364.6 119.25 77   0.001448 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nOther hierarchical stages will be run, based on this comparison, we can say that need for cognition scores are related to inherence significantly."
  },
  {
    "objectID": "posts/Thomas et al. 2008/index.html",
    "href": "posts/Thomas et al. 2008/index.html",
    "title": "Thomas et al. (2008) Diagnostic Hypothesis Generation and Human Judgment",
    "section": "",
    "text": "The focus of this article is providing a model of human judgment/reasoning (Hygene).\nThe universe of possible states means external events related to data that provides a basis to generate hypothesis.\nHypothesis means mental representation of an external event, so decision making process requires learning some subset of the external events. Which subset is an important question here with how this learning happens (e.g., self-explanation, associations or relational understanding?).\n\n\nThree principles\n\n“Data extracted from the environment serve as memory retrieval cues that prompt the retrieval of diagnostic hypotheses from long-term memory.”\nComment1a: Data works as the probing items to retrieve hypothesis from ltm, so it’s maybe like when I saw a shark tail in the ocean, I can retrieve that there may be a shark in the ocean because that shape of tails is specific to sharks.\n“The number of diagnostic hypotheses that one can actively entertain at any point in time is constrained by both cognitive limitations and task characteristics.”\nComment1b:So heuristics and biases play a role to limit the number of diagnostic hypotheses. For the previous example, I may not think that the tail is a joke and a person is using to scare people (an extrinsic reason).\n“Hypotheses maintained in the focus of attention (i.e., WM) serve as input into a comparison process to derive probability judgments and frame information search.”\nComment1c:Hypotheses in WM initate the probability judgement and h-testing.\n\nHygene consists of three parts:\n\nH Generation: How hypotheses are generated on the basis of data extracted from the environment\n-People generate few H’s without any limitation, fewer H’s under time pressure.\nQ1: We are here again, what is the mechanism behind time pressure? Would it be same thing if we also limit the data usage(watching video only 1 time vs. several times example)?\n-People generate H’s highest in a priori probability. (Like in the shark example, few h’s but those have highest probability (effort for effective usage of cognitive resources?))\n-Number of H’s is constrained by WM limitations.\nH Evaluation: How the hypotheses generated from memory are used to make probability judgments\nH Testing: How the generated hypotheses frame subsequent information search in hypothesis-testing situations\n\n\n\n\nThomas et al. (2008), p.161\n\n\nThey had different simulations to test their models. In the simulation 1, they tested their 1st principle, which is H generation.\nHow they modeled the amount of experience seems important here. They defined this as the number of traces stored in the model’s episodic memory. So, it probably means that experience provides more alternative and plausible hypotheses, which explains their results also (Figure 4, p.165). Based on those, we can argue that the traces of real event are more distorted, experience becomes effective. Experience provides true interpretation of distorted data/observations. It makes sense I guess, but only if the task/observation is a thing that can be grasped easily.\nQ2: This is not an empirical result, but it seems plausible but maybe missing. Besides more alternative H’s, experience also makes faster the processing of H’s evaluation, so that new H’s can be generated fast?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Recent Posts",
    "section": "",
    "text": "Grad School Applications\n\n\n\n\n\n\n\n\n\n\n\n\nMert Kobaş\n\n\n\n\n\n\n  \n\n\n\n\nExpertise6 Data Analysis from the very beginning\n\n\n\n\n\n\n\nexample data\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nFeb 23, 2023\n\n\nMK\n\n\n\n\n\n\n  \n\n\n\n\nExpertise3 Data Analysis from the very beginning\n\n\n\n\n\n\n\nexample data\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nFeb 23, 2023\n\n\nMK\n\n\n\n\n\n\n  \n\n\n\n\nHow to R deal with missing values in arithmetic calculations\n\n\n\n\n\n\n\nexample data\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJan 16, 2023\n\n\nMK\n\n\n\n\n\n\n  \n\n\n\n\nCimpian (2012) Generic Statements, Causal Attributions, and Children’s Naive Theories\n\n\n\n\n\n\n\narticles\n\n\nnotes\n\n\nnew ideas\n\n\n\n\n\n\n\n\n\n\n\nJan 14, 2023\n\n\nMK\n\n\n\n\n\n\n  \n\n\n\n\nCimpian & Salomon (2014) The inherence heuristic: An intuitive means of making sense of the world, and a potential precursor to psychological essentialism\n\n\n\n\n\n\n\narticles\n\n\nnotes\n\n\nnew ideas\n\n\n\n\n\n\n\n\n\n\n\nJan 4, 2023\n\n\nMK\n\n\n\n\n\n\n  \n\n\n\n\nCimpian & Steinberg (2014) The inherence heuristic across development: Systematic differences between children’s and adults’ explanations for everyday facts\n\n\n\n\n\n\n\narticles\n\n\nnotes\n\n\nnew ideas\n\n\n\n\n\n\n\n\n\n\n\nDec 28, 2022\n\n\nMK\n\n\n\n\n\n\n  \n\n\n\n\nExpertise3 Data Analysis from the very beginning\n\n\n\n\n\n\n\nexample data\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nDec 26, 2022\n\n\nMK\n\n\n\n\n\n\n  \n\n\n\n\nThomas et al. (2014) Memory Constraints on Hypothesis Generation and Decision Making\n\n\n\n\n\n\n\narticles\n\n\nnotes\n\n\nnew ideas\n\n\n\n\n\n\n\n\n\n\n\nDec 21, 2022\n\n\nMK\n\n\n\n\n\n\n  \n\n\n\n\nThomas et al. (2008) Diagnostic Hypothesis Generation and Human Judgment\n\n\n\n\n\n\n\narticles\n\n\nnotes\n\n\nnew ideas\n\n\n\n\n\n\n\n\n\n\n\nDec 21, 2022\n\n\nMK\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This blog includes my notes about articles, example data analysis and other things like a journal, so I call this as learning journal."
  },
  {
    "objectID": "posts/Cimpian & Steinberg. 2014/index.html",
    "href": "posts/Cimpian & Steinberg. 2014/index.html",
    "title": "Cimpian & Steinberg (2014) The inherence heuristic across development: Systematic differences between children’s and adults’ explanations for everyday facts",
    "section": "",
    "text": "Definition of inherence heuristics: Explaining the data with mostly inherent features of the things. For example, the question of why people walk to the places that are close to their home can be answered with because they have legs rather than it’s more money saving or it’s the environmental friendly way. It’s prevalent in adult’s thinking, but what about children? Based on this question, they have two research questions:\n\nWould children exhibit a tendency to explain observed regularities in inherent terms?\nWould these inherent explanations in fact be more prevalent in children’s than in adults’ thinking?\n\nComment 1: I found developmental patterns part here interesting. It argued that unlike Piaget’s arguments, developmental changes are mostly quantitative rather than qualitative. It explains why inherence bias is still a thing in adulthood, but probably less prevalent than children because they have more knowledge, cognitive capacity and cognitive control.\nQuestion 1: It is still a curiosity for me that whether structure of the data has an effect on explanation generation. For example, generic language boosts inherence bias like: “When told, say, that boys are good at a game called ‘’gorp,’’ preschoolers tend to explain this fact in terms of the inherent features of boys (e.g., they are smart) rather than factors external to boys (e.g., their parents taught them; Cimpian & Markman, 2011).” But what if the the data/pattern/etc. includes possibilities like boys might be good at a game called gorp, does ‘might’ have a similar booster effect on inherence bias because it indicates possibilities?\nIn the five experiments, they asked participants to evaluate the explanations because children have limited capacity for explanations, so comparisons with adults might be difficult in sense of this study’s aims.\nBesides age and inherence, they used different patterns probably to not favor only one type of explanation (right/wrong answers with inherence). These are conventional and control patterns. Conventional patterns include social conventions, so “why the fire trucks are red” can be better explained with extrinsic factors. Control patterns can be better explained with inherent factors.\nThey found that children have tendency for inherence bias and they prefer the explanations with inherence biases more than adults."
  },
  {
    "objectID": "posts/Expertise3_Data_From_Zero_2022-01-04/index.html",
    "href": "posts/Expertise3_Data_From_Zero_2022-01-04/index.html",
    "title": "Expertise3 Data Analysis from the very beginning",
    "section": "",
    "text": "This post includes the trial analyses of an example data related to expertise."
  },
  {
    "objectID": "posts/Cimpian & Salomon 2014/index.html",
    "href": "posts/Cimpian & Salomon 2014/index.html",
    "title": "Cimpian & Salomon (2014) The inherence heuristic: An intuitive means of making sense of the world, and a potential precursor to psychological essentialism",
    "section": "",
    "text": "Inherence = in and on itself.\n\n\n\nCimpian & Salomon (2014), p. 462\n\n\nOne of the important point in the article is the explanation of a cognitive mechanism for inherence bias. Inspired by Kahneman (2011)’s terms, the article defined this as:\n\n\n\nCimpian & Salomon (2014), p. 463\n\n\nIf we go back to Horne et al. (2019), the first part (question or task) is similar to explanandum (the thing that needs to be explained), so it requires attention and WM systems.\nThe second part (mental shotgun), I think, contains both contrast class part and the difference between explanandum and contrast class parts (this difference part can also be in storytelling and quick intuition part). Therefore, it requires long term memory and WM systems again. Horne et al. (2019)’s cognitive systems approach prioritize to explain explanation generation process, so we may see it more general, I guess. Mental shotgun stage provides usually generation of assortment of facts. Like in the example of coins in Horne et al. (2019), people bring coins are made of metals as a fact to their mind rather than the answer of “why is bitcoin symbol a coin?” (probably to remind money because people probably first think coin when they think about money rather than bank credits?). So mental shotgun stage is the starting point of this heuristic (because thinking about representation of money in history with sociocultural facts and the meaning of money for people produced more cognitive load and not fast, it is related to semantic-associative info).\nStorytell part is the combination of fact stage. The most important part here, people used inherent facts when they explain patterns rather than extrinsic information because of their essentialist beliefs. Just like what happened for the generics vs. specific units, patterns are related to essentialist beliefs in the mind, which is also inherent information, but instances (why this bitcoin has coin symbol?) may be explained with more extrinsic fact (because it was sold to a coin producer?). Storytell part is also related working memory systems (updating the information). This part with mental shotgun may be similar to hypothesis generation part\nAfter storytelling part, people have a rule that governs their mind, which is a intuitive mind related rule: “The intuitive mind, however, seems to operate by the principle of”what you see is all there is”: Any information that is not activated by the mental shotgun is completely ignored for purposes of making a judgment”. This part may be related to metacognitive process and hypothesis evaluation/testing part. Since this is a fast and intuitive process, metacognitive related mental systems work poorly. The last part is the production of all these parts, which taps to difference between explanandum and contrast class totally, quick intuition.\nWe are capable of avoiding this bias like if mental shotgun activates first external information (I think experience related part is this) or like if we can take a feedback (p.467, first paragraph).\nQuestion 1: Feedback included data? Like mentioning color suggestions for different genders from the history first, after asking why boys wear blue, girls pink. Or like the visual categorization studies? Feedback means also experience, but well-experienced people must switch between inherent and extrinsic facts appropriately and fast, so they can generate satisfactory explanation quickly?"
  },
  {
    "objectID": "posts/How to R deal with missing values in arithmetic calculations (example)/index.html",
    "href": "posts/How to R deal with missing values in arithmetic calculations (example)/index.html",
    "title": "How to R deal with missing values in arithmetic calculations",
    "section": "",
    "text": "This post includes an example for R codes or to test how R deal with different calculations."
  },
  {
    "objectID": "posts/How to R deal with missing values in arithmetic calculations (example)/index.html#operations-with-missing-values",
    "href": "posts/How to R deal with missing values in arithmetic calculations (example)/index.html#operations-with-missing-values",
    "title": "How to R deal with missing values in arithmetic calculations",
    "section": "Operations with missing values",
    "text": "Operations with missing values\nParticipant 2, 4 and 5 have missing values, let’s calculate Bmi and a summing operation with mutate function:\n\n#Transform height to meter\nice<-ice%>%\n  mutate(Height1=Height/100)\n\nice<-ice%>%\n  mutate(bmi=Weight/(Height1*Height1),\n         sum=Weight+(10-Height1))\n\nhead(ice)\n\n# A tibble: 6 × 8\n     ID   Age Height `Ice Cream` Weight Height1   bmi   sum\n  <dbl> <dbl>  <dbl>       <dbl>  <dbl>   <dbl> <dbl> <dbl>\n1     1    23    176           3     78    1.76  25.2  86.2\n2     2    26     NA           4     65   NA     NA    NA  \n3     3    23    168           5     69    1.68  24.4  77.3\n4     4    34    187           2     NA    1.87  NA    NA  \n5     5    65     NA           4     NA   NA     NA    NA  \n6     6    27    154           3     56    1.54  23.6  64.5\n\n\nR handled missing values in arithmetic operations, it didn’t omit them or replaced them with 0, that’s great news!"
  },
  {
    "objectID": "posts/Cimpian 2012/index.html",
    "href": "posts/Cimpian 2012/index.html",
    "title": "Cimpian (2012) Generic Statements, Causal Attributions, and Children’s Naive Theories",
    "section": "",
    "text": "Categorization is a strong feature of our cognition because it reduces informational complexity of our environments, we can disregard individual differences and infer generalizations across individuals in a category.\nThe development of generic generalization shapes and is shaped by the children’s interaction with the environment. Generics are common in child directed speech and even 2 or 3 years old children can comprehend. Generic statements also include information sources for causal attributions beside communication and learning of generic facts.\nHow we understand the category feature (friendliness) depends on how this feature is integrated into the causal relational networks (essential and central: friendliness is in the DNA of dogs vs. superficial and peripheral: they are raised by humans like that). Children understand generic statements mostly a must feature but non-generic statements as an accidental case. However, this is flexible and context sensitive, if they know the sentence must be generic (apples have seeds vs. this apple has seeds, it doesn’t matter because we already know that every apple has seeds), this division became not important. However, this distinction is important for novel cases.\nComment 1: This may be a bold comment but generic statements at first may work as a causal sources rather than informing about category features. I wonder whether some people argue that learning category features is also starting form causal or semi-causal relations like people argue about learning is based on learning with frequentist or bayesian probabilities (statistical learning or theory theory) as well as perceptual learning -associations among things-. So this process also work with these mechanisms:\n1-Learning ‘dogs are friendly’ with different dogs physical features (common sound, shape, etc.)\n2-When the person see a dog is not friendly, the first causal attribution was damaged, which is every dog until that time was not hostile because dogs are friendly (sometimes the person misattributed even silence as being friendly)\n3-At that moment, there is an explanandum, so contrast class occurs (this hostile dog-every friendly dog) and explanation can be found: because this dog ate raw meat and raw meat makes dogs hostile (attributing the extrinsic features)\n4-If these examples get more and more, the first generic statement may be placed as ‘some dogs are friendly’, ‘dogs are wild creatures’, or it may be reduced to specific breeds (‘goldens are friendly but german sheaperds are hostile’).\n5-Sometimes this argument may change without any physical event:\n\ninteraction with experts or authority such as learning with formal education (strong argument or relying on these sources or inferring the opposite based on their arguments like someone may learn that dogs are evolutionarily close to wolfs in biology course, so they may infer that dogs must be hostile -essentialist ideas-) or\ninteraction with many different sources who claim the contrast (maybe weak but high number of contrast arguments) or\nSocial pressure (friends like conformist behaviors) / echo chambers / cultural beliefs (these may be even some kind of preassumptions ‘dogs are not clean, so they are not friendly’)\n\nQuestion 1:"
  },
  {
    "objectID": "posts/Expertise3_Data_From_Zero_2022-01-04/index.html#correlation-for-ih-scores",
    "href": "posts/Expertise3_Data_From_Zero_2022-01-04/index.html#correlation-for-ih-scores",
    "title": "Expertise3 Data Analysis from the very beginning",
    "section": "Correlation for ih scores",
    "text": "Correlation for ih scores\nCheck the correlations between inherence (the variables starting with st) and reverse inherence (the variables starting with r) scores to check whether it’s appropriate for averaging\n\n# Correlations between ih scores ----\n\n# Create a list of variable names\nvariables <- c(\"stih_lang\", \"r_ih_lang\", \"stih_school\", \"r_ih_school\", \"stih_cards\", \"r_ih_cards\", \"stih_breakfast\", \"r_ih_breakfast\", \"stih_weddings\", \"r_ih_weddings\", \"stih_teeth\", \"r_ih_teeth\", \"stih_traffic\", \"r_ih_traffic\", \"stih_tv\", \"r_ih_tv\")\n\n# Initialize an empty data frame to store the correlation coefficients\ncorrelations <- data.frame(variable1 = character(), variable2 = character(), correlation = numeric(), p.value = numeric(), conf.int = character())\n\n# Iterate over the pairs of variables\nfor (i in seq(1, length(variables), 2)) {\n  j <- i + 1\n  \n  # Calculate the Pearson correlation coefficient and test the statistical significance\n  correlation_test <- cor.test(expertise3_new[, variables[i]], expertise3_new[, variables[j]], method = \"pearson\")\n  \n  # Add the correlation coefficient, p-value, and confidence interval to the data frame\n  correlations <- rbind(correlations, data.frame(variable1 = variables[i], variable2 = variables[j], correlation = correlation_test$estimate, p.value = correlation_test$p.value, conf.int = paste(correlation_test$conf.int[1], correlation_test$conf.int[2], sep = \" - \")))}\n\n## View the correlation coefficients and statistical measures ----\ncorrelations\n\n          variable1      variable2 correlation      p.value\ncor       stih_lang      r_ih_lang  -0.6112615 8.942654e-22\ncor1    stih_school    r_ih_school  -0.5181904 4.547542e-15\ncor2     stih_cards     r_ih_cards  -0.5395115 2.001074e-16\ncor3 stih_breakfast r_ih_breakfast  -0.3813141 2.751024e-08\ncor4  stih_weddings  r_ih_weddings  -0.5222404 2.554910e-15\ncor5     stih_teeth     r_ih_teeth  -0.4231281 5.283701e-10\ncor6   stih_traffic   r_ih_traffic  -0.4832966 4.822560e-13\ncor7        stih_tv        r_ih_tv  -0.3396313 9.221208e-07\n                                    conf.int\ncor  -0.691555186062648 - -0.516045968176506\ncor1  -0.613091701381407 - -0.40854648901427\ncor2 -0.631233272015308 - -0.432907170777665\ncor3 -0.494193465446722 - -0.255790470305453\ncor4  -0.61654545073014 - -0.413161868080018\ncor5 -0.531226389630016 - -0.301474099514304\ncor6  -0.583183953827757 - -0.36901235203396\ncor7 -0.457126840023376 - -0.210484471909784"
  },
  {
    "objectID": "posts/Expertise3_Data_From_Zero_2022-01-04/index.html#ih-scores-calculation",
    "href": "posts/Expertise3_Data_From_Zero_2022-01-04/index.html#ih-scores-calculation",
    "title": "Expertise3 Data Analysis from the very beginning",
    "section": "IH scores calculation",
    "text": "IH scores calculation\nIt seems that each pairs have negative significant correlation, so we can take the average scores to calculate inherence scores\n\n## Average of ih scores  ----\n#It seems that each pairs have negative significant correlation, so we can take the average scores to measure inherence scores\nexpertise3_new <- expertise3_new %>%\n  mutate(ih_lang = (stih_lang + (10 - r_ih_lang))/2,\n         ih_school = (stih_school + (10 - r_ih_school)) / 2,\n         ih_cards = (stih_cards + (10 - r_ih_cards)) / 2,\n         ih_breakfast = (stih_breakfast + (10 - r_ih_breakfast)) / 2,\n         ih_weddings = (stih_weddings + (10 - r_ih_weddings)) / 2,\n         ih_teeth = (stih_teeth + (10 - r_ih_teeth)) / 2,\n         ih_traffic = (stih_traffic + (10-r_ih_traffic)) / 2,\n         ih_tv = (stih_tv + (10-r_ih_tv)) / 2 )"
  },
  {
    "objectID": "posts/Expertise3_Data_From_Zero_2022-01-04/index.html#need-for-cognition-scores-calculation",
    "href": "posts/Expertise3_Data_From_Zero_2022-01-04/index.html#need-for-cognition-scores-calculation",
    "title": "Expertise3 Data Analysis from the very beginning",
    "section": "Need for Cognition scores calculation",
    "text": "Need for Cognition scores calculation\nCalculate “Need for cognition” scale scores\n\n# Need for cognition scale scores  ----\n\n# change the data type of the variables to numeric\nexpertise3_new <- expertise3_new %>%\n  mutate_at(vars(needforcog1,needforcog2,needforcog3,needforcog4,needforcog5,needforcog6\n                 ,needforcog7,needforcog8,needforcog9,needforcog10,needforcog11,needforcog12,\n                 needforcog13,needforcog14,needforcog15,needforcog16,needforcog17,needforcog18), as.numeric)\n\n## Calculate needforcog scores  ----\n#add a new variable called needforcog, which is the sum of all the need for cognition items, the items are weighted according to the scoring key\nexpertise3_new <- expertise3_new %>%\n  group_by(ID)%>%\n  mutate(needforcog=(needforcog1+needforcog2+\n                       (10-needforcog3)+(10-needforcog4)+(10-needforcog5)+needforcog6+\n                       (10-needforcog7)+(10-needforcog8)+(10-needforcog9)+\n                       needforcog10+needforcog11+(10-needforcog12)+\n                       needforcog13+needforcog14+needforcog15+\n                       (10-needforcog16)+(10-needforcog17)+needforcog18)/18)"
  },
  {
    "objectID": "posts/Expertise3_Data_From_Zero_2022-01-04/index.html#data-preparation",
    "href": "posts/Expertise3_Data_From_Zero_2022-01-04/index.html#data-preparation",
    "title": "Expertise3 Data Analysis from the very beginning",
    "section": "Data preparation",
    "text": "Data preparation\nPrepare the expertise scores and other scores ready for analyses\n\n# Replace expertise variables' NA values in the expertise columns with 0  ----\nvariables <- c('high_lang', 'colle_lang', 'grad_lang',\n               'numbook_lang','numarticle_lang',\n               'high_school', 'colle_school', 'grad_school',\n               'numbook_school','numarticle_school',\n               'high_cards', 'colle_cards', 'grad_cards',\n               'numbook_cards','numarticle_cards',\n               'high_breakfast', 'colle_breakfast', 'grad_breakfast',\n               'numbook_breakfast','numarticle_breakfast',\n               'high_weddings', 'colle_weddings', 'grad_weddings',\n               'numbook_weddings','numarticle_weddings',\n               'high_teeth', 'colle_teeth', 'grad_teeth',\n               'numbook_teeth','numarticle_teeth',\n               'high_traffic', 'colle_traffic', 'grad_traffic',\n               'numbook_traffic','numarticle_traffic',\n               'high_tv', 'colle_tv', 'grad_tv',\n               'numbook_tv','numarticle_tv')\n\nexpertise3_new[variables] <- lapply(expertise3_new[variables], \n                                    function(x) ifelse(is.na(x), 0, ifelse(x=='no',0,x)))\n\n# Missing values in the dataframe  ----\napply(is.na(expertise3_new), 2, sum)\n\n                  ID                   ip             duration \n                   0                    0                    0 \n           stih_lang            r_ih_lang          stih_school \n                   0                    0                    0 \n         r_ih_school           stih_cards           r_ih_cards \n                   0                    0                    0 \n      stih_breakfast       r_ih_breakfast        stih_weddings \n                   0                    0                    0 \n       r_ih_weddings           stih_teeth           r_ih_teeth \n                   0                    1                    1 \n        stih_traffic         r_ih_traffic              stih_tv \n                   0                    0                    0 \n             r_ih_tv            know_lang          know_school \n                   0                    0                    0 \n          know_cards       know_breakfast        know_weddings \n                   0                    0                    0 \n          know_teeth         know_traffic              know_tv \n                   0                    1                    0 \n         course_lang            high_lang           colle_lang \n                   0                    0                    0 \n           grad_lang            book_lang         numbook_lang \n                   0                    0                    0 \n        article_lang      numarticle_lang        course_school \n                   0                    0                    1 \n         high_school         colle_school          grad_school \n                   0                    0                    0 \n         book_school       numbook_school       article_school \n                   0                    0                    1 \n   numarticle_school         course_cards           high_cards \n                   0                    0                    0 \n         colle_cards           grad_cards           book_cards \n                   0                    0                    0 \n       numbook_cards        article_cards     numarticle_cards \n                   0                    1                    0 \n    course_breakfast       high_breakfast      colle_breakfast \n                   0                    0                    0 \n      grad_breakfast       book_breakfast    numbook_breakfast \n                   0                    0                    0 \n   article_breakfast numarticle_breakfast      course_weddings \n                   1                    0                    0 \n       high_weddings       colle_weddings        grad_weddings \n                   0                    0                    0 \n       book_weddings     numbook_weddings     article_weddings \n                   1                    0                    0 \n numarticle_weddings         course_teeth           high_teeth \n                   0                    0                    0 \n         colle_teeth           grad_teeth           book_teeth \n                   0                    0                    0 \n       numbook_teeth        article_teeth     numarticle_teeth \n                   0                    0                    0 \n      course_traffic         high_traffic        colle_traffic \n                   0                    0                    0 \n        grad_traffic         book_traffic      numbook_traffic \n                   0                    0                    0 \n     article_traffic   numarticle_traffic            course_tv \n                   0                    0                    0 \n             high_tv             colle_tv              grad_tv \n                   0                    0                    0 \n             book_tv           numbook_tv           article_tv \n                   0                    0                    1 \n       numarticle_tv          needforcog1          needforcog2 \n                   0                    0                    0 \n         needforcog3          needforcog4          needforcog5 \n                   0                    0                    0 \n         needforcog6          needforcog7          needforcog8 \n                   0                    0                    0 \n         needforcog9         needforcog10         needforcog11 \n                   0                    0                    0 \n        needforcog12         needforcog13         needforcog14 \n                   0                    0                    0 \n        needforcog15         needforcog16         needforcog17 \n                   0                    0                    0 \n        needforcog18                  sex            education \n                   0                    0                    0 \n              income             religion             identity \n                   2                    2                    0 \n                 age       political_atti        english_level \n                   0                    0                    0 \n             ih_lang            ih_school             ih_cards \n                   0                    0                    0 \n        ih_breakfast          ih_weddings             ih_teeth \n                   0                    0                    1 \n          ih_traffic                ih_tv           needforcog \n                   0                    0                    0 \n\n# Expertise Ready Df ----\n\n# Create new data frame as analyzable \nexpertise3_new<-expertise3_new%>%\n  select(-stih_lang,-r_ih_lang,-stih_school,-r_ih_school,\n         -stih_cards,-r_ih_cards,-stih_breakfast,-r_ih_breakfast,\n         -stih_weddings,-r_ih_weddings,\n         -stih_teeth,-r_ih_teeth,-stih_traffic,-r_ih_traffic,\n         -stih_tv,-r_ih_tv,\n         -course_lang,\n         -book_lang,-article_lang,\n         -course_school,\n         -book_school,-article_school,\n         -course_cards,\n         -book_cards,-article_cards,\n         -course_breakfast,\n         -book_breakfast,-article_breakfast,\n         -course_weddings,\n         -book_weddings,-article_weddings,\n         -course_teeth,\n         -book_teeth,-article_teeth,\n         -course_traffic,\n         -book_traffic,-article_traffic,\n         -course_tv,\n         -book_tv,-article_tv,\n         -needforcog1,-needforcog2,-needforcog3,-needforcog4,-needforcog5,-needforcog6,\n         -needforcog7,-needforcog8,-needforcog9,-needforcog10,-needforcog11,-needforcog12,\n         -needforcog13,-needforcog14,-needforcog15,-needforcog16,-needforcog17,-needforcog18)\n\n\n# change the data type of the variables to numeric\nexpertise3_new <- expertise3_new %>%\n  mutate_at(vars('know_lang','know_school','know_cards','know_breakfast',\n                 'know_weddings','know_teeth','know_traffic','know_tv',\n                 'high_lang', 'colle_lang', 'grad_lang',\n                 'numbook_lang','numarticle_lang',\n                 'high_school', 'colle_school', 'grad_school',\n                 'numbook_school','numarticle_school',\n                 'high_cards', 'colle_cards', 'grad_cards',\n                 'numbook_cards','numarticle_cards',\n                 'high_breakfast', 'colle_breakfast', 'grad_breakfast',\n                 'numbook_breakfast','numarticle_breakfast',\n                 'high_weddings', 'colle_weddings', 'grad_weddings',\n                 'numbook_weddings','numarticle_weddings',\n                 'high_teeth', 'colle_teeth', 'grad_teeth',\n                 'numbook_teeth','numarticle_teeth',\n                 'high_traffic', 'colle_traffic', 'grad_traffic',\n                 'numbook_traffic','numarticle_traffic',\n                 'high_tv', 'colle_tv', 'grad_tv',\n                 'numbook_tv','numarticle_tv'), as.numeric)"
  },
  {
    "objectID": "posts/Expertise3_Data_From_Zero_2022-01-04/index.html#long-format",
    "href": "posts/Expertise3_Data_From_Zero_2022-01-04/index.html#long-format",
    "title": "Expertise3 Data Analysis from the very beginning",
    "section": "Long format",
    "text": "Long format\nLong format of expertise dataset for factor analysis\n\nlibrary('reshape2')\n\nexpertise3_factor <- melt(expertise3_new, id.vars = c(\"ID\",'duration',\"sex\" ,\"education\" ,\"income\",\"religion\",'identity','age','political_atti','english_level','needforcog'), \n                        measure.vars = c(\"ih_lang\", \"ih_school\",\"ih_cards\", \"ih_breakfast\", \"ih_weddings\", \n                                         \"ih_teeth\", \"ih_traffic\", \"ih_tv\",'know_lang','know_school','know_cards','know_breakfast',\n                                         'know_weddings','know_teeth','know_traffic','know_tv',\n                                         'high_lang', 'colle_lang', 'grad_lang',\n                                         'numbook_lang','numarticle_lang',\n                                         'high_school', 'colle_school', 'grad_school',\n                                         'numbook_school','numarticle_school',\n                                         'high_cards', 'colle_cards', 'grad_cards',\n                                         'numbook_cards','numarticle_cards',\n                                         'high_breakfast', 'colle_breakfast', 'grad_breakfast',\n                                         'numbook_breakfast','numarticle_breakfast',\n                                         'high_weddings', 'colle_weddings', 'grad_weddings',\n                                         'numbook_weddings','numarticle_weddings',\n                                         'high_teeth', 'colle_teeth', 'grad_teeth',\n                                         'numbook_teeth','numarticle_teeth',\n                                         'high_traffic', 'colle_traffic', 'grad_traffic',\n                                         'numbook_traffic','numarticle_traffic',\n                                         'high_tv', 'colle_tv', 'grad_tv',\n                                         'numbook_tv','numarticle_tv'),\n                        sep = \"_\", variable.name = \"Category\", value.name = \"Score\")\n\n# Split the Category column into two columns based on the underscore separator\nexpertise3_factor <- expertise3_factor %>% separate(Category, into = c(\"Category\", \"Score_Type\"), sep = \"_\")\n\n## spread the data from long to wide format  ----\nexpertise3_fact <- expertise3_factor %>% spread(Category, Score)\n\n# change the score type to a factor\nexpertise3_fact$Score_Type<-as.factor(expertise3_fact$Score_Type)\n\n# convert the column ih to numeric\nexpertise3_fact$ih<-as.numeric(expertise3_fact$ih)\n\n###Create another data frame to winsorize expertise variables before factor analysis\n\nexpertise3_wins_fact<-expertise3_fact"
  },
  {
    "objectID": "posts/Expertise3_Data_From_Zero_2022-01-04/index.html#factor-analysis",
    "href": "posts/Expertise3_Data_From_Zero_2022-01-04/index.html#factor-analysis",
    "title": "Expertise3 Data Analysis from the very beginning",
    "section": "Factor analysis",
    "text": "Factor analysis\nFactor analysis for expertise variables with raw scores\n\n# Factor analysis for expertise variables with raw scores ----\n\n# Import packages \n\nlibrary(psych) #PCA/EFA analysis\nlibrary(REdaS) #Produces KMO and Bartletts test\nlibrary(GPArotation)\n\n\n# Create a new dataframe that include only related variables\nfactor_exp<-expertise3_fact%>%\n  select(colle, grad, high, numarticle, numbook)\n\n# Check missing values\napply(is.na(factor_exp), 2, sum)\n\n     colle       grad       high numarticle    numbook \n         0          0          0          0          0 \n\n# Since grad classes for TV category is missing (nobody takes any class in the sample), listwise deletion is applied here.\nbart_spher(factor_exp, use = \"complete.obs\") ###### produces Bartletts test of spherecity \n\n    Bartlett's Test of Sphericity\n\nCall: bart_spher(x = factor_exp, use = \"complete.obs\")\n\n     X2 = 1028.145\n     df = 10\np-value < 2.22e-16\n\nKMO(factor_exp)       ###### Kaiser-Meyer-Olkin measure, which is above .5.\n\nKaiser-Meyer-Olkin factor adequacy\nCall: KMO(r = factor_exp)\nOverall MSA =  0.58\nMSA for each item = \n     colle       grad       high numarticle    numbook \n      0.57       0.59       0.56       0.56       0.60 \n\n# Let's check all the variables\nfa(factor_exp, nfactors = 5, rotate =  \"oblimin\" )  \n\nFactor Analysis using method =  minres\nCall: fa(r = factor_exp, nfactors = 5, rotate = \"oblimin\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n             MR1   MR2   MR3   MR4 MR5   h2   u2 com\ncolle       0.73  0.01 -0.01  0.07   0 0.56 0.44 1.0\ngrad       -0.05  0.54 -0.05  0.08   0 0.31 0.69 1.1\nhigh        0.73 -0.03  0.00 -0.06   0 0.51 0.49 1.0\nnumarticle -0.05 -0.05  0.56  0.01   0 0.28 0.72 1.0\nnumbook     0.14  0.37  0.38  0.06   0 0.57 0.43 2.3\n\n                       MR1  MR2  MR3  MR4  MR5\nSS loadings           1.11 0.53 0.52 0.08 0.00\nProportion Var        0.22 0.11 0.10 0.02 0.00\nCumulative Var        0.22 0.33 0.43 0.45 0.45\nProportion Explained  0.50 0.24 0.23 0.03 0.00\nCumulative Proportion 0.50 0.73 0.97 1.00 1.00\n\n With factor correlations of \n     MR1  MR2  MR3  MR4 MR5\nMR1 1.00 0.32 0.24 0.15   0\nMR2 0.32 1.00 0.52 0.72   0\nMR3 0.24 0.52 1.00 0.30   0\nMR4 0.15 0.72 0.30 1.00   0\nMR5 0.00 0.00 0.00 0.00   1\n\nMean item complexity =  1.3\nTest of the hypothesis that 5 factors are sufficient.\n\nThe degrees of freedom for the null model are  10  and the objective function was  0.65 with Chi Square of  1028.15\nThe degrees of freedom for the model are -5  and the objective function was  0 \n\nThe root mean square of the residuals (RMSR) is  0 \nThe df corrected root mean square of the residuals is  NA \n\nThe harmonic number of observations is  1592 with the empirical chi square  0  with prob <  NA \nThe total number of observations was  1592  with Likelihood Chi Square =  0  with prob <  NA \n\nTucker Lewis Index of factoring reliability =  1.01\nFit based upon off diagonal values = 1\nMeasures of factor score adequacy             \n                                                   MR1  MR2  MR3   MR4 MR5\nCorrelation of (regression) scores with factors   0.84 0.75 0.72  0.57   0\nMultiple R square of scores with factors          0.71 0.57 0.52  0.32   0\nMinimum correlation of possible factor scores     0.42 0.14 0.04 -0.36  -1\n\n# So we can reduce it to 2 factors\nfa(factor_exp, nfactors = 2, rotate =  \"oblimin\" )  \n\nFactor Analysis using method =  minres\nCall: fa(r = factor_exp, nfactors = 2, rotate = \"oblimin\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n             MR1   MR2   h2    u2 com\ncolle       0.06  0.72 0.55 0.455 1.0\ngrad        0.36 -0.01 0.13 0.875 1.0\nhigh       -0.05  0.73 0.51 0.488 1.0\nnumarticle  0.33 -0.06 0.10 0.900 1.1\nnumbook     0.97  0.00 0.95 0.052 1.0\n\n                       MR1  MR2\nSS loadings           1.18 1.05\nProportion Var        0.24 0.21\nCumulative Var        0.24 0.45\nProportion Explained  0.53 0.47\nCumulative Proportion 0.53 1.00\n\n With factor correlations of \n     MR1  MR2\nMR1 1.00 0.35\nMR2 0.35 1.00\n\nMean item complexity =  1\nTest of the hypothesis that 2 factors are sufficient.\n\nThe degrees of freedom for the null model are  10  and the objective function was  0.65 with Chi Square of  1028.15\nThe degrees of freedom for the model are 1  and the objective function was  0 \n\nThe root mean square of the residuals (RMSR) is  0.01 \nThe df corrected root mean square of the residuals is  0.03 \n\nThe harmonic number of observations is  1592 with the empirical chi square  3.66  with prob <  0.056 \nThe total number of observations was  1592  with Likelihood Chi Square =  5.04  with prob <  0.025 \n\nTucker Lewis Index of factoring reliability =  0.96\nRMSEA index =  0.05  and the 90 % confidence intervals are  0.014 0.098\nBIC =  -2.34\nFit based upon off diagonal values = 1\nMeasures of factor score adequacy             \n                                                   MR1  MR2\nCorrelation of (regression) scores with factors   0.97 0.84\nMultiple R square of scores with factors          0.95 0.70\nMinimum correlation of possible factor scores     0.90 0.41\n\n# Figure for the analysis\n\nM1<-fa(factor_exp, nfactors = 2, rotate =  \"oblimin\" ) ##save the analysis as the object m1\nfa.diagram(M1,main=\"Expertise Variables\")  \n\n\n\n\n\nEigenvalues\n\n#Check eigenvalues\n\nfa.parallel(factor_exp)\n\n\n\n\nParallel analysis suggests that the number of factors =  2  and the number of components =  2 \n\n\n\n\nExtracting factor values\n\nfactor_exp_score <- factanal(factor_exp, factors=2, scores=\"regression\", rotation = \"oblimin\")\n\nhead(factor_exp_score$scores)\n\n        Factor1      Factor2\n[1,] -0.2661121 -0.201277588\n[2,] -0.2661121 -0.201277588\n[3,] -0.2661121 -0.201277588\n[4,] -0.2661121 -0.201277588\n[5,]  0.5989102 -0.002729162\n[6,] -0.2661121 -0.201277588\n\nfactor_exp_comb <- bind_cols(factor_exp, data.frame(factor_exp_score$scores))\n\nfactor_exp_comb$class<-factor_exp_comb$Factor1\n\nfactor_exp_comb$media_grad<-factor_exp_comb$Factor2\n\n\n\nHistogram and descriptives for factor scores\n\ndescriptives(dat=factor_exp_comb, vars(Factor1, Factor2),\n             sd=T)\n\n\n DESCRIPTIVES\n\n Descriptives                                            \n ─────────────────────────────────────────────────────── \n                         Factor1         Factor2         \n ─────────────────────────────────────────────────────── \n   N                             1592             1592   \n   Missing                          0                0   \n   Mean                  8.761835e-16    -5.523220e-16   \n   Median                  -0.2661121       -0.2012776   \n   Standard deviation        1.025264        0.9950548   \n   Minimum                  -4.893420        -2.856758   \n   Maximum                   11.22177         20.89972   \n ─────────────────────────────────────────────────────── \n\n\n\nhist(factor_exp_comb$Factor1)\n\n\n\n\n\nhist(factor_exp_comb$Factor2)\n\n\n\n\n\n# Standardize the Selected variables  ----\nvars_to_standardize <- c('know_lang','know_school','know_cards','know_breakfast',\n                         'know_weddings','know_teeth','know_traffic','know_tv',\n                         'high_lang', 'colle_lang', 'grad_lang',\n                         'numbook_lang','numarticle_lang',\n                         'high_school', 'colle_school', 'grad_school',\n                         'numbook_school','numarticle_school',\n                         'high_cards', 'colle_cards', 'grad_cards',\n                         'numbook_cards','numarticle_cards',\n                         'high_breakfast', 'colle_breakfast', 'grad_breakfast',\n                         'numbook_breakfast','numarticle_breakfast',\n                         'high_weddings', 'colle_weddings', 'grad_weddings',\n                         'numbook_weddings','numarticle_weddings',\n                         'high_teeth', 'colle_teeth', 'grad_teeth',\n                         'numbook_teeth','numarticle_teeth',\n                         'high_traffic', 'colle_traffic', 'grad_traffic',\n                         'numbook_traffic','numarticle_traffic',\n                         'high_tv', 'colle_tv', 'grad_tv',\n                         'numbook_tv','numarticle_tv')\n\nexpertise3_new[, vars_to_standardize] <- scale(expertise3_new[, vars_to_standardize])"
  },
  {
    "objectID": "posts/Expertise3_Data_From_Zero_2022-01-04/index.html#long-format-with-factor-scores",
    "href": "posts/Expertise3_Data_From_Zero_2022-01-04/index.html#long-format-with-factor-scores",
    "title": "Expertise3 Data Analysis from the very beginning",
    "section": "Long format (with factor scores)",
    "text": "Long format (with factor scores)\nLong format of expertise dataset\n\nlibrary('reshape2')\n\nexpertise3_long <- melt(expertise3_new, id.vars = c(\"ID\",'duration',\"sex\" ,\"education\" ,\"income\",\"religion\",'identity','age','political_atti','english_level','needforcog'), \n                        measure.vars = c(\"ih_lang\", \"ih_school\",\"ih_cards\", \"ih_breakfast\", \"ih_weddings\", \n                                         \"ih_teeth\", \"ih_traffic\", \"ih_tv\",'know_lang','know_school','know_cards','know_breakfast',\n                                         'know_weddings','know_teeth','know_traffic','know_tv',\n                                         'high_lang', 'colle_lang', 'grad_lang',\n                                         'numbook_lang','numarticle_lang',\n                                         'high_school', 'colle_school', 'grad_school',\n                                         'numbook_school','numarticle_school',\n                                         'high_cards', 'colle_cards', 'grad_cards',\n                                         'numbook_cards','numarticle_cards',\n                                         'high_breakfast', 'colle_breakfast', 'grad_breakfast',\n                                         'numbook_breakfast','numarticle_breakfast',\n                                         'high_weddings', 'colle_weddings', 'grad_weddings',\n                                         'numbook_weddings','numarticle_weddings',\n                                         'high_teeth', 'colle_teeth', 'grad_teeth',\n                                         'numbook_teeth','numarticle_teeth',\n                                         'high_traffic', 'colle_traffic', 'grad_traffic',\n                                         'numbook_traffic','numarticle_traffic',\n                                         'high_tv', 'colle_tv', 'grad_tv',\n                                         'numbook_tv','numarticle_tv'),\n                        sep = \"_\", variable.name = \"Category\", value.name = \"Score\")\n\n# Split the Category column into two columns based on the underscore separator\nexpertise3_long <- expertise3_long %>% separate(Category, into = c(\"Category\", \"Score_Type\"), sep = \"_\")\n\n## spread the data from long to wide format  ----\nexpertise3_ready <- expertise3_long %>% spread(Category, Score)\n\n# change the score type to a factor\nexpertise3_ready$Score_Type<-as.factor(expertise3_ready$Score_Type)\n\n# convert the column ih to numeric\nexpertise3_ready$ih<-as.numeric(expertise3_ready$ih)\n\n# Combine factor scores with the final data ----\nexpertise3_ready<-bind_cols(expertise3_ready,factor_exp_comb$Factor1,factor_exp_comb$Factor2)\n\nNew names:\n• `` -> `...20`\n• `` -> `...21`\n\n#Rename the combined factor variables\nexpertise3_ready<-rename(expertise3_ready, classroom=...20)\nexpertise3_ready<-rename(expertise3_ready, media_grad=...21)"
  },
  {
    "objectID": "posts/Expertise3_Data_From_Zero_2022-01-04/index.html#models",
    "href": "posts/Expertise3_Data_From_Zero_2022-01-04/index.html#models",
    "title": "Expertise3 Data Analysis from the very beginning",
    "section": "Models",
    "text": "Models\nWe can start to analyze our models with using hlm:\n\n## Import packages ----\nlibrary(lme4) \n\nlibrary(lmerTest)\n\n# Center variables: need for cognition and know \n\nexpertise3_ready$know_cent <- scale(expertise3_ready$know, center = TRUE, scale = FALSE)\n\nexpertise3_ready$needforcog_cent <- scale(expertise3_ready$needforcog, center = TRUE, scale = FALSE)\n\nexpertise3_ready$class_cent <- scale(expertise3_ready$classroom, center = TRUE, scale = FALSE)\n\nexpertise3_ready$media_cent <- scale(expertise3_ready$media_grad, center = TRUE, scale = FALSE)\n\n# Check descriptives\n\ndescribe(expertise3_ready)\n\n                vars    n   mean    sd median trimmed   mad   min    max  range\nID                 1 1592 100.00 57.46 100.00  100.00 74.13  1.00 199.00 198.00\nduration*          2 1592  91.63 50.67  92.00   91.91 63.75  1.00 180.00 179.00\nsex*               3 1592   1.51  0.50   2.00    1.52  0.00  1.00   2.00   1.00\neducation*         4 1592   3.42  0.95   3.00    3.40  1.48  1.00   6.00   5.00\nincome*            5 1576  47.16 27.29  46.00   47.15 31.13  1.00  98.00  97.00\nreligion*          6 1576  25.15 15.95  19.00   25.11 19.27  1.00  55.00  54.00\nidentity*          7 1592  22.85  8.81  29.00   24.10  1.48  1.00  31.00  30.00\nage*               8 1592  16.17 10.30  13.00   15.21  8.90  1.00  43.00  42.00\npolitical_atti*    9 1592   3.93  2.15   4.00    3.81  2.97  1.00   9.00   8.00\nenglish_level*    10 1592   1.03  0.22   1.00    1.00  0.00  1.00   3.00   2.00\nneedforcog        11 1592   5.96  1.29   5.89    5.96  1.32  2.11   8.78   6.67\nScore_Type*       12 1592   4.50  2.29   4.50    4.50  2.97  1.00   8.00   7.00\ncolle             13 1592   0.00  1.00  -0.23   -0.23  0.06 -0.54   9.94  10.48\ngrad              14 1393   0.00  1.00  -0.10   -0.11  0.02 -0.15  14.04  14.19\nhigh              15 1592   0.00  1.00  -0.25   -0.22  0.13 -0.64  12.21  12.85\nih                16 1591   4.87  1.89   5.00    4.87  1.48  1.00   9.00   8.00\nknow              17 1591   0.00  1.00   0.01    0.00  1.13 -2.66   2.96   5.62\nnumarticle        18 1592   0.00  1.00  -0.12   -0.14  0.17 -0.41  13.91  14.32\nnumbook           19 1592   0.00  1.00  -0.25   -0.22  0.13 -0.49  11.22  11.71\nclassroom         20 1592   0.00  1.03  -0.27   -0.21  0.00 -4.89  11.22  16.12\nmedia_grad        21 1592   0.00  1.00  -0.20   -0.16  0.00 -2.86  20.90  23.76\nknow_cent         22 1591   0.00  1.00   0.01    0.00  1.13 -2.66   2.96   5.62\nneedforcog_cent   23 1592   0.00  1.29  -0.07    0.00  1.32 -3.85   2.82   6.67\nclass_cent        24 1592   0.00  1.03  -0.27   -0.21  0.00 -4.89  11.22  16.12\nmedia_cent        25 1592   0.00  1.00  -0.20   -0.16  0.00 -2.86  20.90  23.76\n                 skew kurtosis   se\nID               0.00    -1.20 1.44\nduration*       -0.04    -1.15 1.27\nsex*            -0.05    -2.00 0.01\neducation*       0.15    -0.17 0.02\nincome*         -0.01    -1.03 0.69\nreligion*        0.17    -1.38 0.40\nidentity*       -0.84    -0.82 0.22\nage*             0.73    -0.40 0.26\npolitical_atti*  0.36    -0.75 0.05\nenglish_level*   7.84    62.72 0.01\nneedforcog      -0.12    -0.08 0.03\nScore_Type*      0.00    -1.24 0.06\ncolle            5.16    31.98 0.03\ngrad            10.74   122.11 0.03\nhigh             5.23    37.74 0.03\nih               0.01    -0.40 0.05\nknow            -0.05    -0.62 0.03\nnumarticle      10.24   121.66 0.03\nnumbook          5.86    44.27 0.03\nclassroom        4.65    29.82 0.03\nmedia_grad      10.27   159.11 0.02\nknow_cent       -0.05    -0.62 0.03\nneedforcog_cent -0.12    -0.08 0.03\nclass_cent       4.65    29.82 0.03\nmedia_cent      10.27   159.11 0.02\n\n\n\nModel of objective expertise, perceived expertise and need for cognition without interaction\n\n## Model 1 ----\n\n# Need for cognition and perceived expertise as fixed effects\nModel.1<-lmer(ih ~class_cent + media_cent + know_cent + needforcog_cent +(1|Score_Type)+(1|ID),   \n              data=expertise3_ready)\nsummary(Model.1)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: ih ~ class_cent + media_cent + know_cent + needforcog_cent +  \n    (1 | Score_Type) + (1 | ID)\n   Data: expertise3_ready\n\nREML criterion at convergence: 6300.3\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-3.03870 -0.66109 -0.02984  0.61030  2.85090 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.4983   0.7059  \n Score_Type (Intercept) 0.3471   0.5891  \n Residual               2.6815   1.6375  \nNumber of obs: 1590, groups:  ID, 199; Score_Type, 8\n\nFixed effects:\n                  Estimate Std. Error         df t value Pr(>|t|)    \n(Intercept)        4.87046    0.21811    7.77813  22.330 2.47e-08 ***\nclass_cent         0.02900    0.04777 1569.58075   0.607   0.5439    \nmedia_cent        -0.07888    0.04758 1576.28023  -1.658   0.0976 .  \nknow_cent          0.10044    0.04962 1366.14963   2.024   0.0431 *  \nneedforcog_cent   -0.20573    0.05048  196.31885  -4.076 6.66e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) clss_c md_cnt knw_cn\nclass_cent   0.000                     \nmedia_cent   0.000  0.288              \nknow_cent    0.000 -0.107 -0.216       \nnedfrcg_cnt  0.000 -0.025 -0.052 -0.012\n\nconfint(Model.1)\n\n                       2.5 %     97.5 %\n.sig01           0.587868486  0.8255644\n.sig02           0.350482884  1.0048383\n.sigma           1.576916986  1.6990007\n(Intercept)      4.419584771  5.3213676\nclass_cent      -0.064079579  0.1231931\nmedia_cent      -0.172238855  0.0142051\nknow_cent        0.003084828  0.1980084\nneedforcog_cent -0.304733534 -0.1066888\n\n\nSo it seems that perceived knowledge and need for cognition are related to inherence significantly. Interestingly, need for cognition interaction is negatively related to inherence scores.People seek out tasks that challenge their abilities may show less inherent bias contained explanations and if they think they know that area, they agree with inherence bias included sentences more. IPeople may overestimate their abilities and they produce/comprehend the explanations with inherence bias more likely. Not suprisingly, people who are seeking for cognitive activities (need for cognition) may also show more effort for the explanations.\n\n\nModel of objective expertise, perceived expertise and need for cognition with interaction\n\n## Model 2 ----\n\nModel.2<-lmer(ih ~class_cent + media_cent + know_cent + needforcog_cent + \n                needforcog*class_cent + needforcog*media_cent + needforcog*know +\n                (1|Score_Type)+(1|ID), data=expertise3_ready)\nsummary(Model.2)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: ih ~ class_cent + media_cent + know_cent + needforcog_cent +  \n    needforcog * class_cent + needforcog * media_cent + needforcog *  \n    know + (1 | Score_Type) + (1 | ID)\n   Data: expertise3_ready\n\nREML criterion at convergence: 6302.9\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.9030 -0.6610 -0.0184  0.6123  3.1697 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.4977   0.7055  \n Score_Type (Intercept) 0.3477   0.5896  \n Residual               2.6669   1.6331  \nNumber of obs: 1590, groups:  ID, 199; Score_Type, 8\n\nFixed effects:\n                        Estimate Std. Error         df t value Pr(>|t|)    \n(Intercept)              4.86968    0.21831    7.78120  22.306 2.47e-08 ***\nclass_cent               0.35793    0.23812 1538.91170   1.503   0.1330    \nmedia_cent              -0.47494    0.30060 1563.54525  -1.580   0.1143    \nknow_cent                0.70578    0.23208 1375.72760   3.041   0.0024 ** \nneedforcog_cent         -0.21651    0.05055  198.18020  -4.283 2.87e-05 ***\nclass_cent:needforcog   -0.05398    0.03763 1535.11456  -1.435   0.1516    \nmedia_cent:needforcog    0.06650    0.04749 1565.74090   1.400   0.1617    \nneedforcog:know         -0.10285    0.03828 1415.00120  -2.687   0.0073 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) clss_c md_cnt knw_cn ndfrc_ clss_: md_cn:\nclass_cent   0.007                                          \nmedia_cent   0.019  0.211                                   \nknow_cent    0.002 -0.108 -0.238                            \nnedfrcg_cnt -0.001 -0.054 -0.021 -0.052                     \nclss_cnt:nd -0.006 -0.980 -0.186  0.104  0.050              \nmd_cnt:ndfr -0.019 -0.190 -0.987  0.248  0.012  0.174       \nnedfrcg:knw -0.002  0.105  0.250 -0.977  0.050 -0.106 -0.268\nfit warnings:\nfixed-effect model matrix is rank deficient so dropping 2 columns / coefficients\n\nconfint(Model.2)\n\n                            2.5 %      97.5 %\n.sig01                 0.58704091  0.82394772\n.sig02                 0.35088142  1.00567836\n.sigma                 1.57112583  1.69276158\n(Intercept)            4.41848217  5.32097565\nclass_cent            -0.10748632  0.82468080\nmedia_cent            -1.06371448  0.11322395\nknow_cent              0.25131285  1.15943906\nneedforcog_cent       -0.31555682 -0.11742931\nclass_cent:needforcog -0.12764196  0.01964830\nmedia_cent:needforcog -0.02645455  0.15950956\nneedforcog:know       -0.17764073 -0.02788099"
  },
  {
    "objectID": "posts/Expertise3_Data_From_Zero_2022-01-04/index.html#histograms-for-factors",
    "href": "posts/Expertise3_Data_From_Zero_2022-01-04/index.html#histograms-for-factors",
    "title": "Expertise3 Data Analysis from the very beginning",
    "section": "Histograms for Factors",
    "text": "Histograms for Factors\n\n##Histogram, descriptives and correlation matrix for new factors ----\ndescriptives(dat=factor_exp_comb, vars(Factor1, Factor2),\n             sd=T, skew =T)\n\n\n DESCRIPTIVES\n\n Descriptives                                             \n ──────────────────────────────────────────────────────── \n                          Factor1         Factor2         \n ──────────────────────────────────────────────────────── \n   N                              1592             1592   \n   Missing                           0                0   \n   Mean                   8.761835e-16    -5.523220e-16   \n   Median                   -0.2661121       -0.2012776   \n   Standard deviation         1.025264        0.9950548   \n   Minimum                   -4.893420        -2.856758   \n   Maximum                    11.22177         20.89972   \n   Skewness                   4.661496         10.28496   \n   Std. error skewness      0.06133318       0.06133318   \n ──────────────────────────────────────────────────────── \n\n\n\nhist(factor_exp_comb$Factor1)\n\n\n\n\n\nhist(factor_exp_comb$Factor2)\n\n\n\n\nSo our descriptive stats and graphs showed there is an extreme skewness due to extreme values in our factors."
  },
  {
    "objectID": "posts/Expertise3_Data_From_Zero_2022-01-04/index.html#winsorize-the-variables-at-1",
    "href": "posts/Expertise3_Data_From_Zero_2022-01-04/index.html#winsorize-the-variables-at-1",
    "title": "Expertise3 Data Analysis from the very beginning",
    "section": "Winsorize the variables at 1%",
    "text": "Winsorize the variables at 1%\n\n# winsorize the variables (at 1%)\nexpertise3_ready <- expertise3_ready%>%\n  mutate(numarticle_wins_1=Winsorize(numarticle, probs = c(0,0.99)), \n         numbook_wins_1=Winsorize(numbook, probs = c(0,0.99)),\n         high_wins_1=Winsorize(high, probs = c(0,0.99)),\n         colle_wins_1=Winsorize(colle, probs = c(0,0.99)),\n         grad_wins_1=Winsorize(grad,na.rm=TRUE, probs = c(0,0.99)),\n         classroom_wins_1=Winsorize(classroom,na.rm=TRUE, probs = c(0,0.99)),\n         media_wins_1=Winsorize(media_grad,na.rm=TRUE, probs = c(0,0.99)))\n\n#check descriptives\ndescriptives(dat=expertise3_ready, vars(classroom_wins_1,media_wins_1), median=F, n=F, missing=F, sd=T, skew =T)\n\n\n DESCRIPTIVES\n\n Descriptives                                                \n ─────────────────────────────────────────────────────────── \n                          classroom_wins_1    media_wins_1   \n ─────────────────────────────────────────────────────────── \n   Mean                        -0.01922705     -0.04075412   \n   Standard deviation            0.8935287       0.5996877   \n   Minimum                       -4.893420       -2.856758   \n   Maximum                        5.029690        3.427757   \n   Skewness                       3.471803        3.585548   \n   Std. error skewness          0.06133318      0.06133318   \n ─────────────────────────────────────────────────────────── \n\ndescriptives(dat=expertise3_ready, vars(high_wins_1, colle_wins_1), median=F, n=F, missing=F, sd=T)\n\n\n DESCRIPTIVES\n\n Descriptives                                          \n ───────────────────────────────────────────────────── \n                         high_wins_1    colle_wins_1   \n ───────────────────────────────────────────────────── \n   Mean                  -0.02454199     -0.01854773   \n   Standard deviation      0.8226327       0.8666794   \n   Minimum                -0.6447821      -0.5405821   \n   Maximum                  4.341215        5.110902   \n ───────────────────────────────────────────────────── \n\ndescriptives(dat=expertise3_ready, vars(numarticle_wins_1, numbook_wins_1, grad_wins_1), median=F, n=F, missing=F, sd=T)\n\n\n DESCRIPTIVES\n\n Descriptives                                                                 \n ──────────────────────────────────────────────────────────────────────────── \n                         numarticle_wins_1    numbook_wins_1    grad_wins_1   \n ──────────────────────────────────────────────────────────────────────────── \n   Mean                        -0.05220288       -0.02881345    -0.06591236   \n   Standard deviation            0.4758284         0.7815089      0.3238198   \n   Minimum                      -0.4101997        -0.4928124     -0.1541325   \n   Maximum                        3.159021          4.500323       2.798505   \n ──────────────────────────────────────────────────────────────────────────── \n\n\n\nhist(expertise3_ready$classroom_wins_1)\n\n\n\n\nOur first factor is negatively skewed\n\nhist(expertise3_ready$media_wins_1)\n\n\n\n\n\n# Center variables: \n\nexpertise3_ready$classroom_wins_1_cent <- scale(expertise3_ready$classroom_wins_1, center = TRUE, scale = FALSE)\n\nexpertise3_ready$media_wins_1_cent <- scale(expertise3_ready$media_wins_1, center = TRUE, scale = FALSE)"
  },
  {
    "objectID": "posts/Expertise3_Data_From_Zero_2022-01-04/index.html#hlm-models",
    "href": "posts/Expertise3_Data_From_Zero_2022-01-04/index.html#hlm-models",
    "title": "Expertise3 Data Analysis from the very beginning",
    "section": "HLM Models",
    "text": "HLM Models\nSince expertise can be objective (based on the experience like problem solving in that area or productions, year of education or spending more time about that topic) and subjective (how much people perceived themselves as knowledgeable about that topic), objective expertise or real expertise will be negatively related to inherence scores, which means that real experts, which spend more time to learn that subject through courses or pressed materials -books, magazines, articles-, will not satisfy with heuristic explanations. Experts produced more features in diagnostic categories like to describe depression, they listed more attributes for this category (Murphy & Wright, 1984). Therefore, it can be argued that they will evaluate the cases with both intrinsic and extrinsic factors, so they have less scores for inherency.\nBesides this, our expertise variables and engaging in cognitive activities may be interact because expertise sources such as books or magazines also contain products of heuristic thinking. People who have low scores in need for cognition trusted these kind of external sources besides cognitive heuristics:\n“Research relating need for cognition to other individual-differences variables provides evidence that individuals high in need for cognition naturally tend to seek, acquire, think about, and reflect back on information to make sense of stimuli, relationships, and events in their world; individuals low in need for cognition, in contrast, are more likely to rely on others (e.g., experts), cognitive heuristics, or social comparison processes to provide this structure.” (Cacioppo et al., 1996, p.243)\nAs a result of this, even if people who read more books, magazines or take more grad courses, they would probably satisfy with sentences that include inherence bias more or less depend on their need for cognition score, so the assumption is the interaction between need for cognition and this expertise factor will be significantly related to inherence scores.\n\nModel objective expertise with grad courses, books and magazines\n\n## Model objective expertise ----\n\nModel_obj<-lmer(ih ~media_wins_1_cent*needforcog_cent+media_wins_1_cent+needforcog_cent+\n                (1|Score_Type)+(1|ID), data=expertise3_ready)\nsummary(Model_obj)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: ih ~ media_wins_1_cent * needforcog_cent + media_wins_1_cent +  \n    needforcog_cent + (1 | Score_Type) + (1 | ID)\n   Data: expertise3_ready\n\nREML criterion at convergence: 6298.7\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-3.05748 -0.65994 -0.01794  0.60242  2.91751 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.5217   0.7223  \n Score_Type (Intercept) 0.3420   0.5848  \n Residual               2.6705   1.6342  \nNumber of obs: 1591, groups:  ID, 199; Score_Type, 8\n\nFixed effects:\n                                    Estimate Std. Error         df t value\n(Intercept)                          4.86046    0.21698    7.84928  22.401\nmedia_wins_1_cent                   -0.19370    0.08355 1583.44045  -2.318\nneedforcog_cent                     -0.19923    0.05118  198.62257  -3.893\nmedia_wins_1_cent:needforcog_cent    0.11493    0.06318 1574.07118   1.819\n                                  Pr(>|t|)    \n(Intercept)                       2.14e-08 ***\nmedia_wins_1_cent                 0.020550 *  \nneedforcog_cent                   0.000135 ***\nmedia_wins_1_cent:needforcog_cent 0.069067 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) md__1_ ndfrc_\nmd_wns_1_cn  0.009              \nnedfrcg_cnt  0.000 -0.070       \nmd_wns_1_:_ -0.023 -0.390  0.009\n\n\nThe results showed that not the interaction but main effects significantly related to inherence scores. It’s maybe because of low power to detect interaction or because people with high need for cognition scores also take these courses and read these books due to their tendencies, so there is not an interaction. Media and grad courses expertise is related to inherence scores negatively, adjusting for need for cognition scores. I think these results may tap to the role of memory limitations in heuristic thinking. If experts can produce more hypotheses with using their long term memory, the recent memory traces would be remembered probably better. Especially grad courses and also books and magazines seem to related this. Furthermore, especially grad courses again, can be more challenging for the related area. For example, a course related to language may be challenging or understanding books required more attention to details and different backgrounds about these topics so these people already have an expanded mental shotgun area/stage. Inherence bias occurs as a result of memory limitations, so having active memory traces (and broad mental shotgun representations) may provide these people to think beyond heuristic thinking with inherence bias.\nI would expect the same result with high school and college courses, the relation may be more weak because these courses may not be recent, so let’s check the model:\n\n\nModel objective expertise with high school and college courses\n\nModel_obj2<-lmer(ih ~classroom_wins_1_cent * needforcog_cent +classroom_wins_1_cent + needforcog_cent+(1|Score_Type)+(1|ID), data=expertise3_ready)\nsummary(Model_obj2)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: \nih ~ classroom_wins_1_cent * needforcog_cent + classroom_wins_1_cent +  \n    needforcog_cent + (1 | Score_Type) + (1 | ID)\n   Data: expertise3_ready\n\nREML criterion at convergence: 6299.3\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-3.07377 -0.66531 -0.01234  0.60541  2.92029 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.5238   0.7237  \n Score_Type (Intercept) 0.3451   0.5875  \n Residual               2.6679   1.6334  \nNumber of obs: 1591, groups:  ID, 199; Score_Type, 8\n\nFixed effects:\n                                        Estimate Std. Error         df t value\n(Intercept)                              4.87082    0.21783    7.82515  22.361\nclassroom_wins_1_cent                    0.05744    0.05285 1560.84930   1.087\nneedforcog_cent                         -0.21340    0.05116  197.59997  -4.171\nclassroom_wins_1_cent:needforcog_cent   -0.10805    0.04312 1540.95311  -2.506\n                                      Pr(>|t|)    \n(Intercept)                           2.26e-08 ***\nclassroom_wins_1_cent                   0.2773    \nneedforcog_cent                       4.53e-05 ***\nclassroom_wins_1_cent:needforcog_cent   0.0123 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cl__1_ ndfrc_\nclssrm_w_1_  0.000              \nnedfrcg_cnt  0.000 -0.009       \nclssr__1_:_ -0.002 -0.039  0.051\n\n\nThese results showed that the interaction is significant, so the relation between class and inherence scores is depend on need for cognition scores. However, since the trend of the relation between class and inherence scores is positive, this interaction seems negative, so need for cognition mitigated this relation. Commenting this counter intuitive result may be a better way to understand this interaction:\nMain effect of class is not significant (the relation between high school and undergraduate courses and inherence scores). This may be as a result of not challenging nature of these courses or they may not be remembered very well, if these people didn’t interest in them later. The positive trend may be a product of perceived expertise, so people who take these courses think themselves as experts but they are not. However if they have tendency to engage in cognitive activities, they still trust cognitive heuristics less, so need for cognition interact with this relation.\nWhat about perceived expertise? I think perceived expertise may be misleading for people because especially for daily topics like breakfast, they may think that they know enough but their knowledge is limited (and they produce less diverse facts during mental shotgun stage). However, the need for cognition also moderates this relation. Let’s check:\n\n\nModel of perceived expertise and need for cognition\n\nModel_perc<-lmer(ih ~know_cent*needforcog_cent+ know_cent + needforcog_cent +\n                (1|Score_Type)+(1|ID), data=expertise3_ready)\nsummary(Model_perc)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: ih ~ know_cent * needforcog_cent + know_cent + needforcog_cent +  \n    (1 | Score_Type) + (1 | ID)\n   Data: expertise3_ready\n\nREML criterion at convergence: 6293.4\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.94526 -0.66336 -0.01584  0.61110  2.90351 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.4960   0.7043  \n Score_Type (Intercept) 0.3643   0.6036  \n Residual               2.6744   1.6354  \nNumber of obs: 1590, groups:  ID, 199; Score_Type, 8\n\nFixed effects:\n                            Estimate Std. Error         df t value Pr(>|t|)    \n(Intercept)                  4.87459    0.22298    7.75612  21.861 3.01e-08 ***\nknow_cent                    0.08693    0.04832 1357.80267   1.799  0.07227 .  \nneedforcog_cent             -0.21666    0.05038  196.42283  -4.301 2.68e-05 ***\nknow_cent:needforcog_cent   -0.09801    0.03666 1399.78419  -2.674  0.00759 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) knw_cn ndfrc_\nknow_cent    0.000              \nnedfrcg_cnt -0.001 -0.025       \nknw_cnt:nd_ -0.007 -0.012  0.053\n\n\nThese results are similar to previous one and the relation between perceived knowledge and inherence scores moderated by need for cognition, which means people may overinterpret their knowledge and tend to think heuristically more due to this, but their tendency to engage in cognitive activities change this relation, if they like to engage in cognitive activities more, they satisfy with inherent explanations less even if they overestimate their knowledge about that area. This result partially support and partially conflict with Pennycook et al. (2017). Partially supported because just like that research, we found that people are unaware of their own inherence bias and this may be one of the reason behind inherence bias. However, they also found that Dunning-Kruger effects influence their need for cognition scores, so they also overestimate their need for cognition scores. Unlike my assumption here (the mitigating factor in engaging cognitive activities), their finding taps that perceived knowledge should be positively related to need for cognition:\n\ncorrMatrix(expertise3_ready,\n           vars = vars(know_cent,needforcog_cent), flag = TRUE)\n\n\n CORRELATION MATRIX\n\n Correlation Matrix                                                 \n ────────────────────────────────────────────────────────────────── \n                                     know_cent    needforcog_cent   \n ────────────────────────────────────────────────────────────────── \n   know_cent          Pearson's r            —                      \n                      p-value                —                      \n                                                                    \n   needforcog_cent    Pearson's r    0.0323268                  —   \n                      p-value        0.1974834                  —   \n ────────────────────────────────────────────────────────────────── \n   Note. * p < .05, ** p < .01, *** p < .001\n\n\nHowever our scores showed that they are not related significantly. After all of these let’s check our first model 1 and model 2 to adjust other variables:\n\n\nModel of objective expertise, perceived expertise and need for cognition without interaction\n\nModel.1.1<-lmer(ih ~classroom_wins_1_cent + media_wins_1_cent + know_cent +needforcog_cent +(1|Score_Type)+(1|ID),   \n              data=expertise3_ready)\nsummary(Model.1.1)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: ih ~ classroom_wins_1_cent + media_wins_1_cent + know_cent +  \n    needforcog_cent + (1 | Score_Type) + (1 | ID)\n   Data: expertise3_ready\n\nREML criterion at convergence: 6297.9\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-3.02285 -0.66284 -0.02625  0.61186  2.83745 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.4951   0.7037  \n Score_Type (Intercept) 0.3479   0.5898  \n Residual               2.6810   1.6374  \nNumber of obs: 1590, groups:  ID, 199; Score_Type, 8\n\nFixed effects:\n                        Estimate Std. Error         df t value Pr(>|t|)    \n(Intercept)              4.87045    0.21832    7.76987  22.309 2.52e-08 ***\nclassroom_wins_1_cent    0.01370    0.05555 1569.38189   0.247   0.8053    \nmedia_wins_1_cent       -0.17044    0.08309 1567.81620  -2.051   0.0404 *  \nknow_cent                0.11019    0.05019 1355.48286   2.195   0.0283 *  \nneedforcog_cent         -0.20200    0.05045  197.08317  -4.004 8.82e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cl__1_ md__1_ knw_cn\nclssrm_w_1_  0.000                     \nmd_wns_1_cn  0.000  0.304              \nknow_cent    0.000 -0.128 -0.263       \nnedfrcg_cnt  0.000 -0.028 -0.075 -0.004\n\n\nThe same comments can be done also here, our results are not different than single models. This model may be better because it also adjusts perceived expertise and need for cognition, when the objective expertise variables are explained and vice versa. Therefore, our media and grad class related factor is still negatively related to inherence score when it was adjusted for perceived knowledge, class and need for cognition. Let’s check the model with interaction:\n\n\nModel of objective expertise, perceived expertise and need for cognition with interaction\n\nModel.2.1<-lmer(ih ~classroom_wins_1_cent + media_wins_1_cent + know_cent+needforcog_cent + classroom_wins_1_cent*needforcog_cent + media_wins_1_cent*needforcog_cent + know_cent*needforcog_cent+(1|Score_Type)+(1|ID),   \n              data=expertise3_ready)\nsummary(Model.2.1)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: ih ~ classroom_wins_1_cent + media_wins_1_cent + know_cent +  \n    needforcog_cent + classroom_wins_1_cent * needforcog_cent +  \n    media_wins_1_cent * needforcog_cent + know_cent * needforcog_cent +  \n    (1 | Score_Type) + (1 | ID)\n   Data: expertise3_ready\n\nREML criterion at convergence: 6296.3\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.86875 -0.65764 -0.02032  0.60869  2.94463 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.4978   0.7056  \n Score_Type (Intercept) 0.3466   0.5888  \n Residual               2.6592   1.6307  \nNumber of obs: 1590, groups:  ID, 199; Score_Type, 8\n\nFixed effects:\n                                        Estimate Std. Error         df t value\n(Intercept)                            4.865e+00  2.180e-01  7.783e+00  22.316\nclassroom_wins_1_cent                  8.854e-03  5.611e-02  1.570e+03   0.158\nmedia_wins_1_cent                     -1.986e-01  9.123e-02  1.570e+03  -2.177\nknow_cent                              1.021e-01  5.010e-02  1.365e+03   2.039\nneedforcog_cent                       -2.148e-01  5.063e-02  1.993e+02  -4.242\nclassroom_wins_1_cent:needforcog_cent -7.424e-02  4.461e-02  1.547e+03  -1.664\nmedia_wins_1_cent:needforcog_cent      1.319e-01  6.806e-02  1.575e+03   1.938\nknow_cent:needforcog_cent             -1.048e-01  3.866e-02  1.408e+03  -2.712\n                                      Pr(>|t|)    \n(Intercept)                           2.46e-08 ***\nclassroom_wins_1_cent                  0.87462    \nmedia_wins_1_cent                      0.02960 *  \nknow_cent                              0.04166 *  \nneedforcog_cent                       3.38e-05 ***\nclassroom_wins_1_cent:needforcog_cent  0.09626 .  \nmedia_wins_1_cent:needforcog_cent      0.05277 .  \nknow_cent:needforcog_cent              0.00678 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cl__1_ md__1_ knw_cn ndfrc_ c__1_: m__1_:\nclssrm_w_1_  0.004                                          \nmd_wns_1_cn  0.010  0.339                                   \nknow_cent    0.001 -0.121 -0.226                            \nnedfrcg_cnt -0.001 -0.034 -0.083 -0.002                     \nclssr__1_:_ -0.007 -0.093 -0.160 -0.001  0.054              \nmd_wns_1_:_ -0.023 -0.150 -0.394 -0.040  0.010  0.245       \nknw_cnt:nd_  0.000  0.017 -0.005  0.034  0.055 -0.133 -0.288\n\n\nThis total model is still similar to the single models unlike the interactions between objective expertise and need for cognition. Maybe we don’t have enough power to detect these interactions due to sample size. We can still make the similar comments above I guess.\n#Winsorize at 1% the raw variables before factor analysis We can see that there are extreme values for our factor 1 (high+colle) and factor 2 (media+grad). We can deal these values with winsorization:\n\n# winsorize the variables (at 1%)\nexpertise3_wins_fact <- expertise3_wins_fact%>%\n  mutate(numarticle_w1=Winsorize(numarticle, probs = c(0,0.99)), \n         numbook_w1=Winsorize(numbook, probs = c(0,0.99)),\n         high_w1=Winsorize(high, probs = c(0,0.99)),\n         colle_w1=Winsorize(colle, probs = c(0,0.99)),\n         grad_w1=Winsorize(grad,na.rm=TRUE, probs = c(0,0.99)))\n\n#check descriptives\ndescriptives(dat=expertise3_wins_fact, vars(high_w1, colle_w1), median=F, n=F, missing=F, sd=T)\n\n\n DESCRIPTIVES\n\n Descriptives                                     \n ──────────────────────────────────────────────── \n                         high_w1      colle_w1    \n ──────────────────────────────────────────────── \n   Mean                  0.3216080    0.3071608   \n   Standard deviation    0.9623120    0.9595196   \n   Minimum                0.000000     0.000000   \n   Maximum                6.000000     6.000000   \n ──────────────────────────────────────────────── \n\ndescriptives(dat=expertise3_wins_fact, vars(numarticle_w1, numbook_w1, grad_w1), median=F, n=F, missing=F, sd=T)\n\n\n DESCRIPTIVES\n\n Descriptives                                                        \n ─────────────────────────────────────────────────────────────────── \n                         numarticle_w1    numbook_w1    grad_w1      \n ─────────────────────────────────────────────────────────────────── \n   Mean                       5.582915      1.098618    0.03203518   \n   Standard deviation         14.21353      3.060414     0.2316370   \n   Minimum                    0.000000      0.000000      0.000000   \n   Maximum                    100.0000      20.00000      2.000000   \n ─────────────────────────────────────────────────────────────────── \n\n\nOur minimum and maximum values seems better, let’s check histograms:"
  },
  {
    "objectID": "posts/Expertise3_Data_From_Zero_2022-01-04/index.html#factor-analysis-with-winsorized-variables",
    "href": "posts/Expertise3_Data_From_Zero_2022-01-04/index.html#factor-analysis-with-winsorized-variables",
    "title": "Expertise3 Data Analysis from the very beginning",
    "section": "Factor analysis with winsorized variables",
    "text": "Factor analysis with winsorized variables\nFactor analysis for expertise variables with raw scores\n\n# Factor analysis for expertise variables with raw scores ----\n\n# Import packages \n\nlibrary(psych) #PCA/EFA analysis\nlibrary(REdaS) #Produces KMO and Bartletts test\nlibrary(GPArotation)\n\n\n# Create a new dataframe that include only related variables\nfactor_exp1<-expertise3_wins_fact%>%\n  select(colle_w1, grad_w1, high_w1, numarticle_w1, numbook_w1)\n\n\n# Since grad classes for TV category is missing (nobody takes any class in the sample), listwise deletion is applied here.\nbart_spher(factor_exp1, use = \"complete.obs\") ###### produces Bartletts test of spherecity \n\n    Bartlett's Test of Sphericity\n\nCall: bart_spher(x = factor_exp1, use = \"complete.obs\")\n\n     X2 = 1655.614\n     df = 10\np-value < 2.22e-16\n\n#Check eigenvalues\n\nfa.parallel(factor_exp1)\n\n\n\n\nParallel analysis suggests that the number of factors =  3  and the number of components =  2 \n\n\n\n# So we can reduce it to 2 factors\nfa(factor_exp1, nfactors = 3, rotate =  \"oblimin\" )  \n\nFactor Analysis using method =  minres\nCall: fa(r = factor_exp1, nfactors = 3, rotate = \"oblimin\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n                MR1   MR2   MR3   h2   u2 com\ncolle_w1      -0.01  0.72  0.13 0.64 0.36 1.1\ngrad_w1        0.17  0.44 -0.31 0.24 0.76 2.2\nhigh_w1        0.09  0.22  0.60 0.59 0.41 1.3\nnumarticle_w1  0.76 -0.04 -0.10 0.55 0.45 1.0\nnumbook_w1     0.77  0.03  0.12 0.66 0.34 1.1\n\n                       MR1  MR2  MR3\nSS loadings           1.27 0.86 0.55\nProportion Var        0.25 0.17 0.11\nCumulative Var        0.25 0.43 0.54\nProportion Explained  0.47 0.32 0.20\nCumulative Proportion 0.47 0.80 1.00\n\n With factor correlations of \n     MR1  MR2  MR3\nMR1 1.00 0.52 0.09\nMR2 0.52 1.00 0.56\nMR3 0.09 0.56 1.00\n\nMean item complexity =  1.3\nTest of the hypothesis that 3 factors are sufficient.\n\nThe degrees of freedom for the null model are  10  and the objective function was  1.04 with Chi Square of  1655.61\nThe degrees of freedom for the model are -2  and the objective function was  0 \n\nThe root mean square of the residuals (RMSR) is  0 \nThe df corrected root mean square of the residuals is  NA \n\nThe harmonic number of observations is  1592 with the empirical chi square  0  with prob <  NA \nThe total number of observations was  1592  with Likelihood Chi Square =  0  with prob <  NA \n\nTucker Lewis Index of factoring reliability =  1.006\nFit based upon off diagonal values = 1\nMeasures of factor score adequacy             \n                                                   MR1  MR2  MR3\nCorrelation of (regression) scores with factors   0.88 0.85 0.78\nMultiple R square of scores with factors          0.77 0.73 0.60\nMinimum correlation of possible factor scores     0.54 0.45 0.20\n\n# Figure for the analysis\n\nM2<-fa(factor_exp1, nfactors = 3, rotate =  \"oblimin\" ) ##save the analysis as the object m1\nfa.diagram(M2,main=\"Expert Variables\")  \n\n\n\n\nEigenvalues suggest 2 components here\n\nExtracting components\n\nPrC_expert<- principal(factor_exp1, scores =TRUE, rotate= \"varimax\", nfactors=2, method=\"regression\")\n\nloadings (PrC_expert)\n\n\nLoadings:\n              RC1   RC2  \ncolle_w1      0.241 0.821\ngrad_w1       0.635      \nhigh_w1             0.894\nnumarticle_w1 0.849      \nnumbook_w1    0.760 0.350\n\n                 RC1   RC2\nSS loadings    1.759 1.600\nProportion Var 0.352 0.320\nCumulative Var 0.352 0.672\n\n\n\n#Combining data frames\n\nexpertise3_wins_fact1 <- cbind(expertise3_wins_fact, PrC_expert$scores)\n\nCheck the correlation matrix again, it’s the same above:\n\ncorrMatrix(data = expertise3_wins_fact1, vars = vars(colle_w1, grad_w1, high_w1, numarticle_w1, numbook_w1, RC1, RC2))\n\n\n CORRELATION MATRIX\n\n Correlation Matrix                                                                                                                   \n ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n                                   colle_w1      grad_w1       high_w1       numarticle_w1    numbook_w1    RC1           RC2         \n ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n   colle_w1         Pearson's r             —                                                                                         \n                    p-value                 —                                                                                         \n                                                                                                                                      \n   grad_w1          Pearson's r     0.2469764             —                                                                           \n                    p-value        < .0000001             —                                                                           \n                                                                                                                                      \n   high_w1          Pearson's r     0.5239629     0.0806383             —                                                             \n                    p-value        < .0000001     0.0012811             —                                                             \n                                                                                                                                      \n   numarticle_w1    Pearson's r     0.2062351     0.2759111     0.1050732                —                                            \n                    p-value        < .0000001    < .0000001     0.0000266                —                                            \n                                                                                                                                      \n   numbook_w1       Pearson's r     0.3807299     0.2925613     0.3087127        0.5716079             —                              \n                    p-value        < .0000001    < .0000001    < .0000001       < .0000001             —                              \n                                                                                                                                      \n   RC1              Pearson's r     0.2406258     0.6345799     0.0175533        0.8490179     0.7600788             —                \n                    p-value        < .0000001    < .0000001     0.4840040       < .0000001    < .0000001             —                \n                                                                                                                                      \n   RC2              Pearson's r     0.8206051     0.0651383     0.8941356        0.0297629     0.3496585    -0.0000000            —   \n                    p-value        < .0000001     0.0093297    < .0000001        0.2352812    < .0000001     1.0000000            —   \n ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/Expertise3_Data_From_Zero_2022-01-04/index.html#histograms-of-these-components",
    "href": "posts/Expertise3_Data_From_Zero_2022-01-04/index.html#histograms-of-these-components",
    "title": "Expertise3 Data Analysis from the very beginning",
    "section": "Histograms of these components",
    "text": "Histograms of these components\n\n##Histogram, descriptives and correlation matrix for new factors ----\ndescriptives(dat=expertise3_wins_fact1, vars(RC1, RC2),\n             sd=T, skew =T)\n\n\n DESCRIPTIVES\n\n Descriptives                                              \n ───────────────────────────────────────────────────────── \n                          RC1              RC2             \n ───────────────────────────────────────────────────────── \n   N                               1592             1592   \n   Missing                            0                0   \n   Mean                   -4.315364e-16    -7.624415e-16   \n   Median                    -0.3404724       -0.3261946   \n   Standard deviation          1.000000         1.000000   \n   Minimum                    -1.478736        -1.747536   \n   Maximum                     9.432424         7.325695   \n   Skewness                    4.769103         3.671222   \n   Std. error skewness       0.06133318       0.06133318   \n ───────────────────────────────────────────────────────── \n\n\n\nhist(expertise3_wins_fact1$RC1)\n\n\n\n\n\nhist(expertise3_wins_fact1$RC2)"
  },
  {
    "objectID": "posts/Expertise3_Data_From_Zero_2022-01-04/index.html#long-format-with-component-scores",
    "href": "posts/Expertise3_Data_From_Zero_2022-01-04/index.html#long-format-with-component-scores",
    "title": "Expertise3 Data Analysis from the very beginning",
    "section": "Long format (with component scores)",
    "text": "Long format (with component scores)\nLong format of expertise dataset\n\nlibrary('reshape2')\n\nexpertise3_wins_comp <- melt(expertise3_new, id.vars = c(\"ID\",'duration',\"sex\" ,\"education\" ,\"income\",\"religion\",'identity','age','political_atti','english_level','needforcog'), \n                        measure.vars = c(\"ih_lang\", \"ih_school\",\"ih_cards\", \"ih_breakfast\", \"ih_weddings\", \n                                         \"ih_teeth\", \"ih_traffic\", \"ih_tv\",'know_lang','know_school','know_cards','know_breakfast',\n                                         'know_weddings','know_teeth','know_traffic','know_tv',\n                                         'high_lang', 'colle_lang', 'grad_lang',\n                                         'numbook_lang','numarticle_lang',\n                                         'high_school', 'colle_school', 'grad_school',\n                                         'numbook_school','numarticle_school',\n                                         'high_cards', 'colle_cards', 'grad_cards',\n                                         'numbook_cards','numarticle_cards',\n                                         'high_breakfast', 'colle_breakfast', 'grad_breakfast',\n                                         'numbook_breakfast','numarticle_breakfast',\n                                         'high_weddings', 'colle_weddings', 'grad_weddings',\n                                         'numbook_weddings','numarticle_weddings',\n                                         'high_teeth', 'colle_teeth', 'grad_teeth',\n                                         'numbook_teeth','numarticle_teeth',\n                                         'high_traffic', 'colle_traffic', 'grad_traffic',\n                                         'numbook_traffic','numarticle_traffic',\n                                         'high_tv', 'colle_tv', 'grad_tv',\n                                         'numbook_tv','numarticle_tv'),\n                        sep = \"_\", variable.name = \"Category\", value.name = \"Score\")\n\n# Split the Category column into two columns based on the underscore separator\nexpertise3_wins_comp <- expertise3_wins_comp %>% separate(Category, into = c(\"Category\", \"Score_Type\"), sep = \"_\")\n\n## spread the data from long to wide format  ----\nexpertise3_win1_comp <- expertise3_wins_comp %>% spread(Category, Score)\n\n# change the score type to a factor\nexpertise3_win1_comp$Score_Type<-as.factor(expertise3_win1_comp$Score_Type)\n\n# convert the column ih to numeric\nexpertise3_win1_comp$ih<-as.numeric(expertise3_win1_comp$ih)\n\n# Combine factor scores with the final data ----\nexpertise3_win1_comp<-bind_cols(expertise3_win1_comp,expertise3_wins_fact1$RC1,expertise3_wins_fact1$RC2)\n\n#Rename the combined factor variables\nexpertise3_win1_comp<-rename(expertise3_win1_comp, PC1=...20)\nexpertise3_win1_comp<-rename(expertise3_win1_comp, PC2=...21)\n\n\n# Center variables: \n\nexpertise3_win1_comp$know_cent <- scale(expertise3_win1_comp$know, center = TRUE, scale = FALSE)\n\nexpertise3_win1_comp$needforcog_cent <- scale(expertise3_win1_comp$needforcog, center = TRUE, scale = FALSE)\n\nexpertise3_win1_comp$PC1_w1_cent <- scale(expertise3_win1_comp$PC1, center = TRUE, scale = FALSE)\n\nexpertise3_win1_comp$PC2_w2_cent <- scale(expertise3_win1_comp$PC2, center = TRUE, scale = FALSE)"
  },
  {
    "objectID": "posts/Expertise3_Data_From_Zero_2022-01-04/index.html#hlm-models-1",
    "href": "posts/Expertise3_Data_From_Zero_2022-01-04/index.html#hlm-models-1",
    "title": "Expertise3 Data Analysis from the very beginning",
    "section": "HLM Models",
    "text": "HLM Models\n\nModel objective expertise with grad courses, books and magazines\n\n## Model objective expertise ----\n\nModel_obj<-lmer(ih ~PC1_w1_cent*needforcog_cent+PC1_w1_cent+needforcog_cent+\n                (1|Score_Type)+(1|ID), data=expertise3_win1_comp)\nsummary(Model_obj)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: ih ~ PC1_w1_cent * needforcog_cent + PC1_w1_cent + needforcog_cent +  \n    (1 | Score_Type) + (1 | ID)\n   Data: expertise3_win1_comp\n\nREML criterion at convergence: 6305.3\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-3.09646 -0.65141 -0.01162  0.60478  2.89691 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.5242   0.7240  \n Score_Type (Intercept) 0.3478   0.5897  \n Residual               2.6776   1.6363  \nNumber of obs: 1591, groups:  ID, 199; Score_Type, 8\n\nFixed effects:\n                              Estimate Std. Error         df t value Pr(>|t|)\n(Intercept)                    4.86635    0.21867    7.83496  22.255  2.3e-08\nPC1_w1_cent                   -0.07447    0.05599 1532.07904  -1.330 0.183697\nneedforcog_cent               -0.20158    0.05130  198.61611  -3.929 0.000118\nPC1_w1_cent:needforcog_cent    0.02305    0.03721 1578.32515   0.619 0.535820\n                               \n(Intercept)                 ***\nPC1_w1_cent                    \nneedforcog_cent             ***\nPC1_w1_cent:needforcog_cent    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) PC1_w1_ ndfrc_\nPC1_w1_cent  0.012               \nnedfrcg_cnt  0.000 -0.070        \nPC1_w1_cn:_ -0.024 -0.521   0.006\n\n\n\n\nModel objective expertise with high school and college courses\n\nModel_obj2<-lmer(ih ~PC2_w2_cent * needforcog_cent +PC2_w2_cent + needforcog_cent+(1|Score_Type)+(1|ID), data=expertise3_win1_comp)\nsummary(Model_obj2)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: ih ~ PC2_w2_cent * needforcog_cent + PC2_w2_cent + needforcog_cent +  \n    (1 | Score_Type) + (1 | ID)\n   Data: expertise3_win1_comp\n\nREML criterion at convergence: 6300.7\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-3.09277 -0.66309 -0.01215  0.60398  2.91818 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.5198   0.7210  \n Score_Type (Intercept) 0.3404   0.5834  \n Residual               2.6715   1.6345  \nNumber of obs: 1591, groups:  ID, 199; Score_Type, 8\n\nFixed effects:\n                              Estimate Std. Error         df t value Pr(>|t|)\n(Intercept)                    4.87325    0.21643    7.83004  22.516 2.12e-08\nPC2_w2_cent                    0.08026    0.04831 1567.99193   1.661   0.0968\nneedforcog_cent               -0.21295    0.05105  197.64379  -4.171 4.54e-05\nPC2_w2_cent:needforcog_cent   -0.07549    0.03797 1558.32191  -1.988   0.0470\n                               \n(Intercept)                 ***\nPC2_w2_cent                 .  \nneedforcog_cent             ***\nPC2_w2_cent:needforcog_cent *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) PC2_w2_ ndfrc_\nPC2_w2_cent  0.001               \nnedfrcg_cnt  0.000 -0.030        \nPC2_w2_cn:_ -0.008 -0.068   0.042\n\n\n\n\nModel of perceived expertise and need for cognition\n\nModel_perc<-lmer(ih ~know_cent*needforcog_cent+ know_cent + needforcog_cent +\n                (1|Score_Type)+(1|ID), data=expertise3_win1_comp)\nsummary(Model_perc)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: ih ~ know_cent * needforcog_cent + know_cent + needforcog_cent +  \n    (1 | Score_Type) + (1 | ID)\n   Data: expertise3_win1_comp\n\nREML criterion at convergence: 6293.4\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.94526 -0.66336 -0.01584  0.61110  2.90351 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.4960   0.7043  \n Score_Type (Intercept) 0.3643   0.6036  \n Residual               2.6744   1.6354  \nNumber of obs: 1590, groups:  ID, 199; Score_Type, 8\n\nFixed effects:\n                            Estimate Std. Error         df t value Pr(>|t|)    \n(Intercept)                  4.87459    0.22298    7.75612  21.861 3.01e-08 ***\nknow_cent                    0.08693    0.04832 1357.80267   1.799  0.07227 .  \nneedforcog_cent             -0.21666    0.05038  196.42283  -4.301 2.68e-05 ***\nknow_cent:needforcog_cent   -0.09801    0.03666 1399.78419  -2.674  0.00759 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) knw_cn ndfrc_\nknow_cent    0.000              \nnedfrcg_cnt -0.001 -0.025       \nknw_cnt:nd_ -0.007 -0.012  0.053\n\n\n\ncorrMatrix(expertise3_win1_comp,\n           vars = vars(know_cent,needforcog_cent), flag = TRUE)\n\n\n CORRELATION MATRIX\n\n Correlation Matrix                                                 \n ────────────────────────────────────────────────────────────────── \n                                     know_cent    needforcog_cent   \n ────────────────────────────────────────────────────────────────── \n   know_cent          Pearson's r            —                      \n                      p-value                —                      \n                                                                    \n   needforcog_cent    Pearson's r    0.0323268                  —   \n                      p-value        0.1974834                  —   \n ────────────────────────────────────────────────────────────────── \n   Note. * p < .05, ** p < .01, *** p < .001\n\n\n\n\nModel of objective expertise, perceived expertise and need for cognition without interaction\n\nModel.1.1<-lmer(ih ~PC1_w1_cent + PC2_w2_cent + know_cent +needforcog_cent +(1|Score_Type)+(1|ID),   \n              data=expertise3_win1_comp)\nsummary(Model.1.1)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: ih ~ PC1_w1_cent + PC2_w2_cent + know_cent + needforcog_cent +  \n    (1 | Score_Type) + (1 | ID)\n   Data: expertise3_win1_comp\n\nREML criterion at convergence: 6299.9\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-3.04789 -0.66791 -0.03113  0.60644  2.84714 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.4987   0.7062  \n Score_Type (Intercept) 0.3401   0.5832  \n Residual               2.6813   1.6375  \nNumber of obs: 1590, groups:  ID, 199; Score_Type, 8\n\nFixed effects:\n                  Estimate Std. Error         df t value Pr(>|t|)    \n(Intercept)        4.87050    0.21611    7.78797  22.537 2.26e-08 ***\nPC1_w1_cent       -0.07868    0.04938 1492.80341  -1.593   0.1113    \nPC2_w2_cent        0.05708    0.04892 1567.08966   1.167   0.2435    \nknow_cent          0.09657    0.05072 1344.82431   1.904   0.0571 .  \nneedforcog_cent   -0.20471    0.05059  197.51174  -4.047 7.45e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) PC1_1_ PC2_2_ knw_cn\nPC1_w1_cent  0.000                     \nPC2_w2_cent  0.000  0.070              \nknow_cent    0.000 -0.260 -0.167       \nnedfrcg_cnt  0.000 -0.078 -0.030  0.001\n\n\n\n\nModel of objective expertise, perceived expertise and need for cognition with interaction\n\nModel.2.1<-lmer(ih ~PC1_w1_cent + PC2_w2_cent  + know_cent+needforcog_cent + PC1_w1_cent*needforcog_cent + PC2_w2_cent*needforcog_cent + know_cent*needforcog_cent+(1|Score_Type)+(1|ID),   \n              data=expertise3_win1_comp)\nsummary(Model.2.1)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: ih ~ PC1_w1_cent + PC2_w2_cent + know_cent + needforcog_cent +  \n    PC1_w1_cent * needforcog_cent + PC2_w2_cent * needforcog_cent +  \n    know_cent * needforcog_cent + (1 | Score_Type) + (1 | ID)\n   Data: expertise3_win1_comp\n\nREML criterion at convergence: 6304.6\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.94328 -0.65638 -0.01338  0.60696  2.89992 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.5000   0.7071  \n Score_Type (Intercept) 0.3397   0.5828  \n Residual               2.6691   1.6337  \nNumber of obs: 1590, groups:  ID, 199; Score_Type, 8\n\nFixed effects:\n                              Estimate Std. Error         df t value Pr(>|t|)\n(Intercept)                    4.87079    0.21607    7.79504  22.543 2.23e-08\nPC1_w1_cent                   -0.08004    0.05771 1486.77758  -1.387   0.1657\nPC2_w2_cent                    0.06017    0.04926 1568.51514   1.222   0.2220\nknow_cent                      0.09172    0.05068 1352.14067   1.810   0.0705\nneedforcog_cent               -0.21713    0.05075  199.21963  -4.278 2.92e-05\nPC1_w1_cent:needforcog_cent    0.04532    0.03872 1559.22145   1.170   0.2420\nPC2_w2_cent:needforcog_cent   -0.05653    0.03918 1561.39689  -1.443   0.1492\nknow_cent:needforcog_cent     -0.09392    0.03909 1396.96749  -2.403   0.0164\n                               \n(Intercept)                 ***\nPC1_w1_cent                    \nPC2_w2_cent                    \nknow_cent                   .  \nneedforcog_cent             ***\nPC1_w1_cent:needforcog_cent    \nPC2_w2_cent:needforcog_cent    \nknow_cent:needforcog_cent   *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) PC1_w1_ PC2_w2_ knw_cn ndfrc_ PC1_1_: PC2_2_:\nPC1_w1_cent  0.013                                              \nPC2_w2_cent  0.003  0.125                                       \nknow_cent    0.000 -0.220  -0.165                               \nnedfrcg_cnt -0.001 -0.077  -0.034   0.003                       \nPC1_w1_cn:_ -0.023 -0.483  -0.108  -0.012 -0.011                \nPC2_w2_cn:_ -0.006 -0.102  -0.062  -0.011  0.046 -0.038         \nknw_cnt:nd_  0.000 -0.001   0.001   0.041  0.060 -0.251  -0.163 \n\n\n#Winsorize at 1% the raw variables for our new expertise variables\n\n# winsorize the variables (at 1%)\nexpertise3_wins_average <- expertise3_wins_fact%>%\n  mutate(numarticle_w1=Winsorize(numarticle, probs = c(0,0.99)), \n         numbook_w1=Winsorize(numbook, probs = c(0,0.99)),\n         high_w1=Winsorize(high, probs = c(0,0.99)),\n         colle_w1=Winsorize(colle, probs = c(0,0.99)),\n         grad_w1=Winsorize(grad,na.rm=TRUE, probs = c(0,0.99)))\n\n\n# Standardize the Selected variables  ----\nvars_to_standardize <- c(\"know\", \"numarticle_w1\",\"numbook_w1\", \"high_w1\",\"colle_w1\", \"grad_w1\")\n\n\nexpertise3_wins_average<-expertise3_wins_average %>% \n  group_by(Score_Type) %>% \n  mutate_at(vars(vars_to_standardize), scale)"
  },
  {
    "objectID": "posts/Expertise6_Data_From_Zero_2022-01-16/index.html",
    "href": "posts/Expertise6_Data_From_Zero_2022-01-16/index.html",
    "title": "Expertise6 Data Analysis from the very beginning",
    "section": "",
    "text": "This post includes the trial analyses of an example data related to expertise.\nNotes: Need for closure scores haven’t been calculated, the reverse items will be checked. Notes: For the long format, internet article, documentaries and podcast series should be added."
  },
  {
    "objectID": "posts/Expertise6_Data_From_Zero_2022-01-16/index.html#correlation-for-ih-scores",
    "href": "posts/Expertise6_Data_From_Zero_2022-01-16/index.html#correlation-for-ih-scores",
    "title": "Expertise6 Data Analysis from the very beginning",
    "section": "Correlation for ih scores",
    "text": "Correlation for ih scores\nCheck the correlations between inherence (the variables starting with st) and reverse inherence (the variables starting with r) scores to check whether it’s appropriate for averaging\n\n# Correlations between ih scores ----\n\n# Create a list of variable names\nvariables <- c(\"stih_food\", \"r_ih_food\", \"stih_sports\", \"r_ih_sports\", \"stih_school\", \"r_ih_school\", \"stih_architect\", \"r_ih_architect\", \"stih_product\", \"r_ih_product\", \"stih_langu\", \"r_ih_langu\", \"stih_network\", \"r_ih_network\", \"stih_anthro\", \"r_ih_anthro\")\n\n# Initialize an empty data frame to store the correlation coefficients\ncorrelations <- data.frame(variable1 = character(), variable2 = character(), correlation = numeric(), p.value = numeric(), conf.int = character())\n\n# Iterate over the pairs of variables\nfor (i in seq(1, length(variables), 2)) {\n  j <- i + 1\n  \n  # Calculate the Pearson correlation coefficient and test the statistical significance\n  correlation_test <- cor.test(expertise6_new[, variables[i]], expertise6_new[, variables[j]], method = \"pearson\")\n  \n  # Add the correlation coefficient, p-value, and confidence interval to the data frame\n  correlations <- rbind(correlations, data.frame(variable1 = variables[i], variable2 = variables[j], correlation = correlation_test$estimate, p.value = correlation_test$p.value, conf.int = paste(correlation_test$conf.int[1], correlation_test$conf.int[2], sep = \" - \")))}\n\n## View the correlation coefficients and statistical measures ----\ncorrelations\n\n          variable1      variable2 correlation      p.value\ncor       stih_food      r_ih_food  -0.4463858 8.615861e-11\ncor1    stih_sports    r_ih_sports  -0.5753914 2.560996e-18\ncor2    stih_school    r_ih_school  -0.7267533 8.094747e-33\ncor3 stih_architect r_ih_architect  -0.6411731 1.275191e-23\ncor4   stih_product   r_ih_product  -0.6038601 1.834983e-20\ncor5     stih_langu     r_ih_langu  -0.5635496 1.740797e-17\ncor6   stih_network   r_ih_network  -0.5898848 2.202312e-19\ncor7    stih_anthro    r_ih_anthro  -0.5442449 3.376865e-16\n                                    conf.int\ncor  -0.553035668300546 - -0.325343101426943\ncor1 -0.662979980870432 - -0.472263234194049\ncor2  -0.787333698376605 - -0.65227324221234\ncor3 -0.717624383417027 - -0.549452592494561\ncor4 -0.686743816747194 - -0.505475832894281\ncor5 -0.653042937790108 - -0.458533845459347\ncor6  -0.675100225685499 - -0.48913522712539\ncor7 -0.636777009250974 - -0.436258848944745"
  },
  {
    "objectID": "posts/Expertise6_Data_From_Zero_2022-01-16/index.html#ih-scores-calculation",
    "href": "posts/Expertise6_Data_From_Zero_2022-01-16/index.html#ih-scores-calculation",
    "title": "Expertise6 Data Analysis from the very beginning",
    "section": "IH scores calculation",
    "text": "IH scores calculation\nIt seems that each pairs have negative significant correlation, so we can take the average scores to calculate inherence scores\n\n## Average of ih scores  ----\n#It seems that each pairs have negative significant correlation, so we can take the average scores to measure inherence scores\nexpertise6_new <- expertise6_new %>%\n  mutate(ih_food = (stih_food + (10 - r_ih_food))/2,\n         ih_sports = (stih_sports + (10 - r_ih_sports)) / 2,\n         ih_school = (stih_school + (10 - r_ih_school)) / 2,\n         ih_architect = (stih_architect + (10 - r_ih_architect)) / 2,\n         ih_product = (stih_product + (10 - r_ih_product)) / 2,\n         ih_langu = (stih_langu + (10 - r_ih_langu)) / 2,\n         ih_network = (stih_network + (10-r_ih_network)) / 2,\n         ih_anthro = (stih_anthro + (10-r_ih_anthro)) / 2 )"
  },
  {
    "objectID": "posts/Expertise6_Data_From_Zero_2022-01-16/index.html#need-for-cognition-scores-calculation",
    "href": "posts/Expertise6_Data_From_Zero_2022-01-16/index.html#need-for-cognition-scores-calculation",
    "title": "Expertise6 Data Analysis from the very beginning",
    "section": "Need for Cognition scores calculation",
    "text": "Need for Cognition scores calculation\nCalculate “Need for cognition” scale scores\n\n# Need for cognition scale scores  ----\n\n# change the data type of the variables to numeric\nexpertise6_new <- expertise6_new %>%\n  mutate_at(vars( 'needforclo1','needforclo2','needforclo3','needforclo4','needforclo5',\n                        'needforclo6','needforclo7','needforclo8','needforclo9','needforclo10',\n                        'needforclo11','needforclo12','needforclo13','needforclo14',\n                        'needforclo15','needforclo16','needforclo17','needforclo18',\n                        'needforclo19','needforclo20','needforclo21','needforclo22',\n                        'needforclo23','needforclo24','needforclo25','needforclo26',\n                        'needforclo27','needforclo28','needforclo29','needforclo30',\n                        'needforclo31','needforclo32','needforclo33','needforclo34',\n                        'needforclo35','needforclo36','needforclo37','needforclo38',\n                        'needforclo39','needforclo40','needforclo41','needforclo42'), as.numeric)\n\n## Calculate needforclo scores  ----\n#add a new variable called needforclo, which is the sum of all the need for cognition items, the items are weighted according to the scoring key\nexpertise6_new <- expertise6_new %>%\n  group_by(ID)%>%\n  mutate(needforclo=(needforclo1+needforclo2+needforclo3+needforclo4+\n                       (10-needforclo5)+needforclo6+needforclo7+(10-needforclo8)+\n                       (10-needforclo9)+needforclo10+ needforclo11+needforclo12+\n                       needforclo13+needforclo14+needforclo15+needforclo16+\n                       needforclo17+needforclo18+needforclo19+(10-needforclo20)+\n                       (10-needforclo21)+(10-needforclo22)+needforclo23+(10-needforclo24)+\n                       (10-needforclo25)+needforclo26+needforclo27+(10-needforclo28)+\n                       needforclo29+needforclo30+(10-needforclo31)+(10-needforclo32)+\n                       needforclo33+needforclo34+needforclo35+(10-needforclo36)+\n                       needforclo37+(10-needforclo38)+needforclo39+(10-needforclo40)+\n                       (10-needforclo41)+needforclo42)/42)"
  },
  {
    "objectID": "posts/Expertise6_Data_From_Zero_2022-01-16/index.html#data-preparation",
    "href": "posts/Expertise6_Data_From_Zero_2022-01-16/index.html#data-preparation",
    "title": "Expertise6 Data Analysis from the very beginning",
    "section": "Data preparation",
    "text": "Data preparation\nPrepare the expertise scores and other scores ready for analyses\n\n# Replace expertise variables' NA values in the expertise columns with 0  ----\nvariables <- c('know_food','know_sports','know_school','know_architect',\n                        'know_product','know_langu','know_network','know_anthro',\n                        'high_food', 'colle_food', 'grad_food',\n                        'numbook_food','numarticle_food',\n                        'numinarti_food', 'numdocu_food',\n                        'numradio_food',\n                        'high_sports', 'colle_sports', 'grad_sports',\n                        'numbook_sports','numarticle_sports',\n                        'numinarti_sports', 'numdocu_sports',\n                        'numradio_sports',\n                        'high_school', 'colle_school', 'grad_school',\n                        'numbook_school','numarticle_school',\n                        'numinarti_school','numdocu_school',\n                        'numradio_school',\n                        'high_architect', 'colle_architect','grad_architect',\n                        'numbook_architect',\n                        'numarticle_architect',\n                        'numinarti_architect', \n                        'numdocu_architect',\n                        'numradio_architect',\n                        'high_product', 'colle_product', 'grad_product',\n                        'numbook_product',\n                        'numarticle_product',\n                       'numinarti_product','numdocu_product',\n                        'numradio_product',\n                        'high_langu', 'colle_langu', 'grad_langu',\n                        'numbook_langu','numarticle_langu',\n                        'numinarti_langu', 'numdocu_langu',\n                        'numradio_langu',\n                        'high_network', 'colle_network', 'grad_network',\n                        'numbook_network','numarticle_network',\n                        'numinarti_network', 'numdocu_network',\n                        'numradio_network',\n                        'high_anthro', 'colle_anthro','grad_anthro',\n                        'numbook_anthro','numarticle_anthro',\n                        'numinarti_anthro', 'numdocu_anthro',\n                        'numradio_anthro')\n\nexpertise6_new <- expertise6_new %>%\n  mutate_at(vars('know_food','know_sports','know_school','know_architect',\n                        'know_product','know_langu','know_network','know_anthro',\n                        'high_food', 'colle_food', 'grad_food',\n                        'numbook_food','numarticle_food',\n                        'numinarti_food', 'numdocu_food',\n                        'numradio_food',\n                        'high_sports', 'colle_sports', 'grad_sports',\n                        'numbook_sports','numarticle_sports',\n                        'numinarti_sports', 'numdocu_sports',\n                        'numradio_sports',\n                        'high_school', 'colle_school', 'grad_school',\n                        'numbook_school','numarticle_school',\n                        'numinarti_school','numdocu_school',\n                        'numradio_school',\n                        'high_architect', 'colle_architect','grad_architect',\n                        'numbook_architect',\n                        'numarticle_architect',\n                        'numinarti_architect', \n                        'numdocu_architect',\n                        'numradio_architect',\n                        'high_product', 'colle_product', 'grad_product',\n                        'numbook_product',\n                        'numarticle_product',\n                       'numinarti_product','numdocu_product',\n                        'numradio_product',\n                        'high_langu', 'colle_langu', 'grad_langu',\n                        'numbook_langu','numarticle_langu',\n                        'numinarti_langu', 'numdocu_langu',\n                        'numradio_langu',\n                        'high_network', 'colle_network', 'grad_network',\n                        'numbook_network','numarticle_network',\n                        'numinarti_network', 'numdocu_network',\n                        'numradio_network',\n                        'high_anthro', 'colle_anthro','grad_anthro',\n                        'numbook_anthro','numarticle_anthro',\n                        'numinarti_anthro', 'numdocu_anthro',\n                        'numradio_anthro'), as.numeric)\n\n\n#expertise6_new[variables] <- lapply(expertise6_new[variables], \n                                    #function(x) ifelse(is.na(x), 0, ifelse(x=='no',0, x)))\n\nexpertise6_new[variables] <- lapply(expertise6_new[variables], function(x) replace(x, is.na(x) | !is.numeric(x) , 0))"
  },
  {
    "objectID": "posts/Expertise6_Data_From_Zero_2022-01-16/index.html#long-format",
    "href": "posts/Expertise6_Data_From_Zero_2022-01-16/index.html#long-format",
    "title": "Expertise6 Data Analysis from the very beginning",
    "section": "Long format",
    "text": "Long format\nLong format of expertise dataset for factor analysis\n\nexpertise6_factor <- melt(expertise6_new, id.vars = c(\"ID\",'duration',\"sex\" ,\"education\" ,\"income\",\"religion\",'identity','age','political_atti','english_level','needforclo'), \n                        measure.vars = c(\"ih_food\", \"ih_sports\",\"ih_school\",\"ih_architect\",\n                                         \"ih_product\",\"ih_langu\",\"ih_network\",\"ih_anthro\",\n                       'know_food','know_sports','know_school','know_architect',\n                        'know_product','know_langu','know_network','know_anthro',\n                        'high_food', 'colle_food', 'grad_food',\n                        'numbook_food','numarticle_food',\n                        'numinarti_food', 'numdocu_food',\n                        'numradio_food',\n                        'high_sports', 'colle_sports', 'grad_sports',\n                        'numbook_sports','numarticle_sports',\n                        'numinarti_sports', 'numdocu_sports',\n                        'numradio_sports',\n                        'high_school', 'colle_school', 'grad_school',\n                        'numbook_school','numarticle_school',\n                        'numinarti_school','numdocu_school',\n                        'numradio_school',\n                        'high_architect', 'colle_architect','grad_architect',\n                        'numbook_architect',\n                        'numarticle_architect',\n                        'numinarti_architect', \n                        'numdocu_architect',\n                        'numradio_architect',\n                        'high_product', 'colle_product', 'grad_product',\n                        'numbook_product',\n                        'numarticle_product',\n                       'numinarti_product','numdocu_product',\n                        'numradio_product',\n                        'high_langu', 'colle_langu', 'grad_langu',\n                        'numbook_langu','numarticle_langu',\n                        'numinarti_langu', 'numdocu_langu',\n                        'numradio_langu',\n                        'high_network', 'colle_network', 'grad_network',\n                        'numbook_network','numarticle_network',\n                        'numinarti_network', 'numdocu_network',\n                        'numradio_network',\n                        'high_anthro', 'colle_anthro','grad_anthro',\n                        'numbook_anthro','numarticle_anthro',\n                        'numinarti_anthro', 'numdocu_anthro',\n                        'numradio_anthro'),\n                        sep = \"_\", variable.name = \"Category\", value.name = \"Score\")\n\n# Split the Category column into two columns based on the underscore separator\nexpertise6_factor <- expertise6_factor %>% separate(Category, into = c(\"Category\", \"Score_Type\"), sep = \"_\")\n\n## spread the data from long to wide format  ----\nexpertise6_fact <- expertise6_factor %>% spread(Category, Score)\n\n# change the score type to a factor\nexpertise6_fact$Score_Type<-as.factor(expertise6_fact$Score_Type)\n\n# convert the column ih to numeric\nexpertise6_fact$ih<-as.numeric(expertise6_fact$ih)\n\n###Create another data frame to winsorize expertise variables before factor analysis\n\nexpertise6_wins_fact<-expertise6_fact"
  },
  {
    "objectID": "posts/Expertise6_Data_From_Zero_2022-01-16/index.html#detect-the-id-of-missing-values-of-expertise-scores",
    "href": "posts/Expertise6_Data_From_Zero_2022-01-16/index.html#detect-the-id-of-missing-values-of-expertise-scores",
    "title": "Expertise6 Data Analysis from the very beginning",
    "section": "Detect the ID of missing values of expertise scores",
    "text": "Detect the ID of missing values of expertise scores\n\nmissing_ids <- unlist(mapply(function(x) expertise6_new$ID[which(is.na(expertise6_new[x]))], c('know_food','know_sports','know_school','know_architect',\n                        'know_product','know_langu','know_network','know_anthro',\n                        'high_food', 'colle_food', 'grad_food',\n                        'numbook_food','numarticle_food',\n                        'numinarti_food', 'numdocu_food',\n                        'numradio_food',\n                        'high_sports', 'colle_sports', 'grad_sports',\n                        'numbook_sports','numarticle_sports',\n                        'numinarti_sports', 'numdocu_sports',\n                        'numradio_sports',\n                        'high_school', 'colle_school', 'grad_school',\n                        'numbook_school','numarticle_school',\n                        'numinarti_school','numdocu_school',\n                        'numradio_school',\n                        'high_architect', 'colle_architect','grad_architect',\n                        'numbook_architect',\n                        'numarticle_architect',\n                        'numinarti_architect', \n                        'numdocu_architect',\n                        'numradio_architect',\n                        'high_product', 'colle_product', 'grad_product',\n                        'numbook_product',\n                        'numarticle_product',\n                       'numinarti_product','numdocu_product',\n                        'numradio_product',\n                        'high_langu', 'colle_langu', 'grad_langu',\n                        'numbook_langu','numarticle_langu',\n                        'numinarti_langu', 'numdocu_langu',\n                        'numradio_langu',\n                        'high_network', 'colle_network', 'grad_network',\n                        'numbook_network','numarticle_network',\n                        'numinarti_network', 'numdocu_network',\n                        'numradio_network',\n                        'high_anthro', 'colle_anthro','grad_anthro',\n                        'numbook_anthro','numarticle_anthro',\n                        'numinarti_anthro', 'numdocu_anthro',\n                        'numradio_anthro')))\n\ncat(ifelse(length(missing_ids[missing_ids > 0]) > 0, paste(\"The following IDs are missing:\",missing_ids[missing_ids != 0]), \"There is no missing IDs.\"),\"\\n\")\n\nThere is no missing IDs. \n\n\n\n# Expertise Ready Df ----\n\n#','high_food','colle_food','grad_food','numbook_food','numarticle_food','numinarti_food','numdocu_food','numradio_food','high_sports','colle_sports','grad_sports','numbook_sports','numarticle_sports','numinarti_sports','numdocu_sports','numradio_sports','high_school','colle_school','grad_school','numbook_school','numarticle_school','numinarti_school','numdocu_school','numradio_school','high_architect','colle_architect','grad_architect','numbook_architect','numarticle_architect','numinarti_architect','numdocu_architect','numradio_architect','high_product','colle_product','grad_product','numbook_product','numarticle_product','numinarti_product','numdocu_product','numradio_product','high_langu','colle_langu','grad_langu','numbook_langu','numarticle_langu','numinarti_langu','numdocu_langu','numradio_langu','high_network','colle_network','grad_network','numbook_network','numarticle_network','numinarti_network','numdocu_network','numradio_network','high_anthro','colle_anthro','grad_anthro','numbook_anthro','numarticle_anthro','numinarti_anthro','numdocu_anthro','numradio_anthro\n\n# change the data type of the variables to numeric\nexpertise6_new<-expertise6_new%>%mutate_at(vars('know_food','know_sports','know_school','know_architect','know_product','know_langu','know_network','know_anthro'),as.numeric)"
  },
  {
    "objectID": "posts/Expertise6_Data_From_Zero_2022-01-16/index.html#winsorize-the-variables-at-1",
    "href": "posts/Expertise6_Data_From_Zero_2022-01-16/index.html#winsorize-the-variables-at-1",
    "title": "Expertise6 Data Analysis from the very beginning",
    "section": "Winsorize the variables at 1%",
    "text": "Winsorize the variables at 1%\n\n# winsorize the variables (at 1%)\nexpertise6_fact1 <- expertise6_fact%>%\n  mutate(numarticle=Winsorize(numarticle, probs = c(0,0.99)), \n         numbook=Winsorize(numbook, probs = c(0,0.99)),\n         high=Winsorize(high, probs = c(0,0.99)),\n         colle=Winsorize(colle, probs = c(0,0.99)),\n         grad=Winsorize(grad,na.rm=TRUE, probs = c(0,0.99)),\n         numdocu=Winsorize(numdocu,na.rm=TRUE, probs = c(0,0.99)),\n         numinarti=Winsorize(numinarti,na.rm=TRUE, probs = c(0,0.99)),\n         numradio=Winsorize(numradio,na.rm=TRUE, probs = c(0,0.99)))"
  },
  {
    "objectID": "posts/Expertise6_Data_From_Zero_2022-01-16/index.html#factor-analysis",
    "href": "posts/Expertise6_Data_From_Zero_2022-01-16/index.html#factor-analysis",
    "title": "Expertise6 Data Analysis from the very beginning",
    "section": "Factor analysis",
    "text": "Factor analysis\n\n# Factor analysis for expertise variables with raw scores ----\n\n# Import packages \n\nlibrary(psych) #PCA/EFA analysis\nlibrary(REdaS) #Produces KMO and Bartletts test\n\nLoading required package: grid\n\nlibrary(GPArotation)\n\n\n# Create a new dataframe that include only related variables\nfactor_exp<-expertise6_fact1%>%\n  select(colle, grad, high, numarticle, numbook, numdocu, numinarti, numradio)\n\n# Check missing values\napply(is.na(factor_exp), 2, sum)\n\n     colle       grad       high numarticle    numbook    numdocu  numinarti \n         0          0          0          0          0          0          0 \n  numradio \n         0 \n\n\nFactor analysis for expertise variables with raw scores\n\n# Since grad classes for TV category is missing (nobody takes any class in the sample), listwise deletion is applied here.\nbart_spher(factor_exp, use = \"complete.obs\") ###### produces Bartletts test of spherecity \n\n    Bartlett's Test of Sphericity\n\nCall: bart_spher(x = factor_exp, use = \"complete.obs\")\n\n     X2 = 3334.573\n     df = 28\np-value < 2.22e-16\n\nKMO(factor_exp)       ###### Kaiser-Meyer-Olkin measure, which is above .5.\n\nKaiser-Meyer-Olkin factor adequacy\nCall: KMO(r = factor_exp)\nOverall MSA =  0.79\nMSA for each item = \n     colle       grad       high numarticle    numbook    numdocu  numinarti \n      0.77       0.83       0.70       0.79       0.81       0.83       0.77 \n  numradio \n      0.84 \n\n#Check eigenvalues\n\nfa.parallel(factor_exp)\n\n\n\n\nParallel analysis suggests that the number of factors =  3  and the number of components =  2 \n\n\n\nFactor analysis\n\n# So we can reduce it to 2 factors\nfa(factor_exp, nfactors = 2, rotate =  \"oblimin\" )  \n\nFactor Analysis using method =  minres\nCall: fa(r = factor_exp, nfactors = 2, rotate = \"oblimin\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n             MR1   MR2   h2   u2 com\ncolle      -0.04  0.83 0.66 0.34 1.0\ngrad        0.09  0.35 0.16 0.84 1.1\nhigh       -0.03  0.40 0.15 0.85 1.0\nnumarticle  0.77  0.06 0.63 0.37 1.0\nnumbook     0.44  0.47 0.62 0.38 2.0\nnumdocu     0.58 -0.06 0.30 0.70 1.0\nnuminarti   0.89 -0.04 0.76 0.24 1.0\nnumradio    0.30  0.04 0.10 0.90 1.0\n\n                       MR1  MR2\nSS loadings           2.10 1.30\nProportion Var        0.26 0.16\nCumulative Var        0.26 0.42\nProportion Explained  0.62 0.38\nCumulative Proportion 0.62 1.00\n\n With factor correlations of \n     MR1  MR2\nMR1 1.00 0.49\nMR2 0.49 1.00\n\nMean item complexity =  1.2\nTest of the hypothesis that 2 factors are sufficient.\n\nThe degrees of freedom for the null model are  28  and the objective function was  2.18 with Chi Square of  3334.57\nThe degrees of freedom for the model are 13  and the objective function was  0.08 \n\nThe root mean square of the residuals (RMSR) is  0.03 \nThe df corrected root mean square of the residuals is  0.05 \n\nThe harmonic number of observations is  1536 with the empirical chi square  90.74  with prob <  1e-13 \nThe total number of observations was  1536  with Likelihood Chi Square =  123.66  with prob <  3.8e-20 \n\nTucker Lewis Index of factoring reliability =  0.928\nRMSEA index =  0.074  and the 90 % confidence intervals are  0.063 0.087\nBIC =  28.27\nFit based upon off diagonal values = 0.99\nMeasures of factor score adequacy             \n                                                   MR1  MR2\nCorrelation of (regression) scores with factors   0.93 0.88\nMultiple R square of scores with factors          0.87 0.77\nMinimum correlation of possible factor scores     0.73 0.54\n\n# Figure for the analysis\n\nM1<-fa(factor_exp, nfactors = 2, rotate =  \"oblimin\" ) ##save the analysis as the object m1\nfa.diagram(M1,main=\"Expertise Variables\")  \n\n\n\n\nSo here we have two factors and we can investigate them as classes and media-literature part, let’s extract values for now, but creating these categories with averaging related variables may be a better way for the sake of conceptual understanding.\n\n\nExtracting factor values\n\nfactor_exp_score <- factanal(factor_exp, factors=2, scores=\"regression\", rotation = \"oblimin\", na.rm=TRUE)\n\nhead(factor_exp_score$scores)\n\n         Factor1    Factor2\n[1,]  0.02119905 -0.4233947\n[2,]  0.22119338 -0.4679913\n[3,] -0.14582577 -0.3159511\n[4,]  0.17926929 -0.5193750\n[5,] -0.26945985 -0.2581370\n[6,]  0.82731651 -0.4607261\n\nfactor_exp_comb <- bind_cols(factor_exp, data.frame(factor_exp_score$scores))\n\nfactor_exp_comb$class<-factor_exp_comb$Factor1\n\nfactor_exp_comb$media_grad<-factor_exp_comb$Factor2\n\n\n\nHistogram and descriptives for factor scores\n\ndescriptives(dat=factor_exp_comb, vars(Factor1, Factor2),\n             sd=T)\n\n\n DESCRIPTIVES\n\n Descriptives                                            \n ─────────────────────────────────────────────────────── \n                         Factor1          Factor2        \n ─────────────────────────────────────────────────────── \n   N                              1536            1536   \n   Missing                           0               0   \n   Mean                  -5.305905e-16    8.812395e-16   \n   Median                   -0.2694598      -0.2581370   \n   Standard deviation        0.9977506       0.9220806   \n   Minimum                   -2.856278       -3.778751   \n   Maximum                    7.978103        6.476149   \n ─────────────────────────────────────────────────────── \n\n\n\nhist(factor_exp_comb$Factor1)\n\n\n\n\n\nhist(factor_exp_comb$Factor2)"
  },
  {
    "objectID": "posts/Expertise6_Data_From_Zero_2022-01-16/index.html#winsorize-the-variables-at-1-1",
    "href": "posts/Expertise6_Data_From_Zero_2022-01-16/index.html#winsorize-the-variables-at-1-1",
    "title": "Expertise6 Data Analysis from the very beginning",
    "section": "Winsorize the variables at 1%",
    "text": "Winsorize the variables at 1%\n\n# winsorize the variables (at 1%)\nexpertise6_ready <- expertise6_fact%>%\n  mutate(numarticle_wins_1=Winsorize(numarticle, probs = c(0,0.99)), \n         numbook_wins_1=Winsorize(numbook, probs = c(0,0.99)),\n         high_wins_1=Winsorize(high, probs = c(0,0.99)),\n         colle_wins_1=Winsorize(colle, probs = c(0,0.99)),\n         grad_wins_1=Winsorize(grad,na.rm=TRUE, probs = c(0,0.99)),\n         docu_wins_1=Winsorize(numdocu,na.rm=TRUE, probs = c(0,0.99)),\n         inarti_wins_1=Winsorize(numinarti,na.rm=TRUE, probs = c(0,0.99)),\n         radio_wins_1=Winsorize(numradio,na.rm=TRUE, probs = c(0,0.99)))\n\n#check descriptives\ndescriptives(dat=expertise6_ready, vars(docu_wins_1,inarti_wins_1, radio_wins_1 ), median=F, n=F, missing=T, sd=T, skew =T)\n\n\n DESCRIPTIVES\n\n Descriptives                                                            \n ─────────────────────────────────────────────────────────────────────── \n                          docu_wins_1    inarti_wins_1    radio_wins_1   \n ─────────────────────────────────────────────────────────────────────── \n   Missing                          0                0               0   \n   Mean                      2.194010         9.023438       0.7988281   \n   Standard deviation        5.699259         26.33146        2.767993   \n   Minimum                   0.000000         0.000000        0.000000   \n   Maximum                   38.25000         191.2500        20.00000   \n   Skewness                  4.286225         4.959908        5.015617   \n   Std. error skewness     0.06243908       0.06243908      0.06243908   \n ─────────────────────────────────────────────────────────────────────── \n\ndescriptives(dat=expertise6_ready, vars(high_wins_1, colle_wins_1), median=F, n=F, missing=T, sd=T)\n\n\n DESCRIPTIVES\n\n Descriptives                                          \n ───────────────────────────────────────────────────── \n                         high_wins_1    colle_wins_1   \n ───────────────────────────────────────────────────── \n   Missing                         0               0   \n   Mean                    0.3147135       0.5794271   \n   Standard deviation      0.8859480        1.522536   \n   Minimum                  0.000000        0.000000   \n   Maximum                  4.650000        10.00000   \n ───────────────────────────────────────────────────── \n\ndescriptives(dat=expertise6_ready, vars(numarticle_wins_1, numbook_wins_1, grad_wins_1), median=F, n=F, missing=T, sd=T)\n\n\n DESCRIPTIVES\n\n Descriptives                                                                 \n ──────────────────────────────────────────────────────────────────────────── \n                         numarticle_wins_1    numbook_wins_1    grad_wins_1   \n ──────────────────────────────────────────────────────────────────────────── \n   Missing                               0                 0              0   \n   Mean                           5.916667          1.592448     0.07291667   \n   Standard deviation             16.05380          4.371575      0.4732818   \n   Minimum                        0.000000          0.000000       0.000000   \n   Maximum                        100.0000          30.00000       4.000000   \n ──────────────────────────────────────────────────────────────────────────── \n\n\n\n# Standardize the Selected variables  ----\nvars_to_standardize <- c(\"know\", \"docu_wins_1\",\"inarti_wins_1\", \"radio_wins_1\",\"high_wins_1\", \"colle_wins_1\",\n                         \"numarticle_wins_1\", \"numbook_wins_1\", \"grad_wins_1\")\n\n\n\nexpertise6_ready<-expertise6_ready %>% \n  group_by(Score_Type) %>% \n  mutate_at(vars(vars_to_standardize), scale)\n\n\nsum(is.na(expertise6_ready$grad_wins_1))\n\n[1] 384"
  },
  {
    "objectID": "posts/Expertise6_Data_From_Zero_2022-01-16/index.html#take-the-means-of-winsorized-variables-to-create-our-new-categories-media-and-class",
    "href": "posts/Expertise6_Data_From_Zero_2022-01-16/index.html#take-the-means-of-winsorized-variables-to-create-our-new-categories-media-and-class",
    "title": "Expertise6 Data Analysis from the very beginning",
    "section": "Take the means of winsorized variables to create our new categories: media and class",
    "text": "Take the means of winsorized variables to create our new categories: media and class\n\nexpertise6_ready <- expertise6_ready %>% replace_na(list(colle_wins_1 = NA, high_wins_1 = NA, grad_wins_1 = NA, docu_wins_1= NA,inarti_wins_1 = NA, radio_wins_1 = NA, numarticle_wins_1 = NA, numbook_wins_1 = NA))\n\n\nexpertise6_ready <- expertise6_ready %>% mutate(class_wins1 = mapply(function(x, y, z) {\n  ifelse(is.na(x), (y + z)/2, \n         ifelse(is.na(y), (x + z)/2, \n                ifelse(is.na(z), (x + y)/2, (x + y + z)/3)))}, \n  grad_wins_1, colle_wins_1, high_wins_1))\n\nexpertise6_ready <- expertise6_ready %>% mutate(media_wins1 = mapply(function(x, y, z, t, r) {\n  ifelse(is.na(x), (y + z + t + r)/4, \n         ifelse(is.na(y), (x + z + t + r)/4,\n                ifelse(is.na(z), (x + y + t + r)/4, \n                       ifelse(is.na(t),(x + y + z + r)/4, \n                              ifelse(is.na(r),(x + y + z + t)/4,\n                                     (x + y + z + t + r)/5)))))}, \n  docu_wins_1,inarti_wins_1, radio_wins_1, numbook_wins_1, numarticle_wins_1))"
  },
  {
    "objectID": "posts/Expertise6_Data_From_Zero_2022-01-16/index.html#center-the-variables",
    "href": "posts/Expertise6_Data_From_Zero_2022-01-16/index.html#center-the-variables",
    "title": "Expertise6 Data Analysis from the very beginning",
    "section": "Center the variables",
    "text": "Center the variables\n\nexpertise6_ready$know_cent <- scale(expertise6_ready$know, center = TRUE, scale = FALSE)\n\nexpertise6_ready$needforclo_cent <- scale(expertise6_ready$needforclo, center = TRUE, scale = FALSE)\n\nexpertise6_ready$class_wins1_cent <- scale(expertise6_ready$class_wins1, center = TRUE, scale = FALSE)\n\nexpertise6_ready$media_wins1_cent <- scale(expertise6_ready$media_wins1, center = TRUE, scale = FALSE)\n\nDescriptives\n\ndescribe(expertise6_ready)\n\n                  vars    n  mean    sd median trimmed   mad   min     max\nID                   1 1536 96.50 55.44  96.50   96.50 71.16  1.00  192.00\nduration*            2 1536 92.35 52.86  92.50   92.45 68.94  1.00  182.00\nsex*                 3 1536  1.64  0.48   2.00    1.68  0.00  1.00    2.00\neducation*           4 1536  3.62  0.94   4.00    3.60  1.48  1.00    6.00\nincome*              5 1512 49.86 27.76  51.00   49.99 34.10  1.00  101.00\nreligion*            6 1536 22.26 12.86  17.00   21.98 16.31  1.00   51.00\nidentity*            7 1528 21.27 10.66  29.00   22.29  1.48  1.00   36.00\nage*                 8 1528 19.20 12.11  17.00   18.35 13.34  1.00   46.00\npolitical_atti*      9 1536  4.21  2.16   4.00    4.12  2.97  1.00    9.00\nenglish_level*      10 1536  1.01  0.10   1.00    1.00  0.00  1.00    2.00\nneedforclo          11 1528  5.63  0.69   5.69    5.63  0.71  3.79    7.36\nScore_Type*         12 1536  4.50  2.29   4.50    4.50  2.97  1.00    8.00\ncolle               13 1536  2.21 52.06   0.00    0.21  0.00  0.00 2000.00\ngrad                14 1536  0.94 26.07   0.00    0.00  0.00  0.00 1000.00\nhigh                15 1536  0.66 10.56   0.00    0.06  0.00  0.00  400.00\nih                  16 1536  5.11  2.14   5.00    5.16  2.97  1.00    9.00\nknow                17 1536  0.00  1.00  -0.17   -0.06  1.10 -2.20    3.05\nnumarticle          18 1536  7.38 36.32   0.00    2.09  0.00  0.00 1000.00\nnumbook             19 1536  2.07 12.09   0.00    0.55  0.00  0.00  400.00\nnumdocu             20 1536  3.70 36.82   0.00    0.79  0.00  0.00 1000.00\nnuminarti           21 1536 13.56 82.66   0.00    2.79  0.00  0.00 2000.00\nnumradio            22 1536  1.21  7.13   0.00    0.10  0.00  0.00  100.00\nnumarticle_wins_1   23 1536  0.00  1.00  -0.31   -0.23  0.13 -0.49    9.84\nnumbook_wins_1      24 1536  0.00  1.00  -0.29   -0.22  0.19 -0.55   11.10\nhigh_wins_1         25 1536  0.00  1.00  -0.27   -0.24  0.08 -0.82    7.96\ncolle_wins_1        26 1536  0.00  1.00  -0.26   -0.22  0.19 -0.66    9.84\ngrad_wins_1         27 1152  0.00  1.00  -0.12   -0.15  0.07 -0.31   13.78\ndocu_wins_1         28 1536  0.00  1.00  -0.34   -0.23  0.17 -0.59   10.47\ninarti_wins_1       29 1536  0.00  1.00  -0.29   -0.22  0.13 -0.45   10.74\nradio_wins_1        30 1536  0.00  1.00  -0.25   -0.23  0.06 -0.43   10.69\nclass_wins1         31 1536  0.00  0.72  -0.25   -0.16  0.08 -0.58    7.38\nmedia_wins1         32 1536  0.00  0.73  -0.26   -0.17  0.16 -0.48    6.57\nknow_cent           33 1536  0.00  1.00  -0.17   -0.06  1.10 -2.20    3.05\nneedforclo_cent     34 1528  0.00  0.69   0.06    0.00  0.71 -1.84    1.73\nclass_wins1_cent    35 1536  0.00  0.72  -0.25   -0.16  0.08 -0.58    7.38\nmedia_wins1_cent    36 1536  0.00  0.73  -0.26   -0.17  0.16 -0.48    6.57\n                    range  skew kurtosis   se\nID                 191.00  0.00    -1.20 1.41\nduration*          181.00 -0.01    -1.23 1.35\nsex*                 1.00 -0.59    -1.66 0.01\neducation*           5.00  0.33     0.17 0.02\nincome*            100.00 -0.01    -1.06 0.71\nreligion*           50.00  0.19    -1.09 0.33\nidentity*           35.00 -0.59    -1.40 0.27\nage*                45.00  0.51    -0.83 0.31\npolitical_atti*      8.00  0.34    -0.76 0.06\nenglish_level*       1.00  9.63    90.89 0.00\nneedforclo           3.57 -0.05    -0.24 0.02\nScore_Type*          7.00  0.00    -1.24 0.06\ncolle             2000.00 37.08  1410.78 1.33\ngrad              1000.00 36.95  1404.30 0.67\nhigh               400.00 35.76  1333.98 0.27\nih                   8.00 -0.12    -0.85 0.05\nknow                 5.25  0.46    -0.47 0.03\nnumarticle        1000.00 17.54   410.05 0.93\nnumbook            400.00 24.68   771.98 0.31\nnumdocu           1000.00 25.86   695.08 0.94\nnuminarti         2000.00 15.21   284.41 2.11\nnumradio           100.00 11.51   148.11 0.18\nnumarticle_wins_1   10.34  4.72    26.00 0.03\nnumbook_wins_1      11.66  4.80    28.73 0.03\nhigh_wins_1          8.78  3.87    18.44 0.03\ncolle_wins_1        10.50  4.42    26.23 0.03\ngrad_wins_1         14.09  7.79    71.79 0.03\ndocu_wins_1         11.07  4.15    22.30 0.03\ninarti_wins_1       11.18  5.44    35.99 0.03\nradio_wins_1        11.13  5.36    35.25 0.03\nclass_wins1          7.95  4.16    24.16 0.02\nmedia_wins1          7.06  4.05    20.95 0.02\nknow_cent            5.25  0.46    -0.47 0.03\nneedforclo_cent      3.57 -0.05    -0.24 0.02\nclass_wins1_cent     7.95  4.16    24.16 0.02\nmedia_wins1_cent     7.06  4.05    20.95 0.02\n\n\n##Shapiro-Wilk values\n\ndescriptives(expertise6_ready, vars = vars(class_wins1, media_wins1, know, ih),n=FALSE, missing= FALSE, median=FALSE, sw = TRUE)\n\n\n DESCRIPTIVES\n\n Descriptives                                                                         \n ──────────────────────────────────────────────────────────────────────────────────── \n                         class_wins1     media_wins1     know            ih           \n ──────────────────────────────────────────────────────────────────────────────────── \n   Mean                  2.746646e-18    2.432227e-17    5.261995e-17      5.112305   \n   Standard deviation       0.7210980       0.7266705       0.9977173      2.142995   \n   Minimum                 -0.5754818      -0.4829025       -2.197199      1.000000   \n   Maximum                   7.378558        6.574853        3.050287      9.000000   \n   Shapiro-Wilk W           0.5389428       0.5381616       0.9721739     0.9707327   \n   Shapiro-Wilk p          < .0000001      < .0000001      < .0000001    < .0000001   \n ────────────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/Expertise6_Data_From_Zero_2022-01-16/index.html#hlm-models",
    "href": "posts/Expertise6_Data_From_Zero_2022-01-16/index.html#hlm-models",
    "title": "Expertise6 Data Analysis from the very beginning",
    "section": "HLM Models",
    "text": "HLM Models\n\nModel objective expertise with grad courses, books and magazines\n\n## Model objective expertise ----\n\nModel_obj<-lmer(ih ~class_wins1_cent*needforclo_cent+class_wins1_cent+needforclo_cent+\n                (1|Score_Type)+(1|ID), data=expertise6_ready)\nsummary(Model_obj)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: ih ~ class_wins1_cent * needforclo_cent + class_wins1_cent +  \n    needforclo_cent + (1 | Score_Type) + (1 | ID)\n   Data: expertise6_ready\n\nREML criterion at convergence: 6320.8\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.93455 -0.65040  0.02653  0.66826  2.56442 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.7734   0.8794  \n Score_Type (Intercept) 0.7745   0.8801  \n Residual               3.1236   1.7674  \nNumber of obs: 1528, groups:  ID, 191; Score_Type, 8\n\nFixed effects:\n                                   Estimate Std. Error         df t value\n(Intercept)                         5.10303    0.32079    7.58400  15.907\nclass_wins1_cent                   -0.07206    0.06840 1503.58449  -1.053\nneedforclo_cent                     0.18572    0.11261  189.81958   1.649\nclass_wins1_cent:needforclo_cent    0.02288    0.08782 1510.75809   0.261\n                                 Pr(>|t|)    \n(Intercept)                      4.24e-07 ***\nclass_wins1_cent                    0.292    \nneedforclo_cent                     0.101    \nclass_wins1_cent:needforclo_cent    0.794    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cls_1_ ndfrc_\nclss_wns1_c  0.000              \nnedfrcl_cnt  0.000  0.013       \nclss_wn1_:_  0.004  0.093 -0.050\n\n\n\n\nModel objective expertise with high school and college courses\n\nModel_obj2<-lmer(ih ~media_wins1_cent * needforclo_cent +media_wins1_cent + needforclo_cent+(1|Score_Type)+(1|ID), data=expertise6_ready)\nsummary(Model_obj2)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: ih ~ media_wins1_cent * needforclo_cent + media_wins1_cent +  \n    needforclo_cent + (1 | Score_Type) + (1 | ID)\n   Data: expertise6_ready\n\nREML criterion at convergence: 6312\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.95382 -0.66151  0.02337  0.66064  2.55803 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.7553   0.8691  \n Score_Type (Intercept) 0.7773   0.8817  \n Residual               3.1108   1.7638  \nNumber of obs: 1528, groups:  ID, 191; Score_Type, 8\n\nFixed effects:\n                                   Estimate Std. Error         df t value\n(Intercept)                         5.10190    0.32118    7.56820  15.885\nmedia_wins1_cent                   -0.22040    0.07087 1498.82315  -3.110\nneedforclo_cent                     0.18599    0.11163  188.65641   1.666\nmedia_wins1_cent:needforclo_cent   -0.06861    0.08735 1466.83328  -0.785\n                                 Pr(>|t|)    \n(Intercept)                      4.38e-07 ***\nmedia_wins1_cent                  0.00191 ** \nneedforclo_cent                   0.09735 .  \nmedia_wins1_cent:needforclo_cent  0.43236    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) md_w1_ ndfrc_\nmd_wns1_cnt  0.000              \nnedfrcl_cnt  0.000  0.017       \nmd_wns1_c:_  0.004  0.087 -0.044\n\n\n\n\nModel of perceived expertise and need for cognition\n\nModel_perc<-lmer(ih ~know_cent*needforclo_cent+ know_cent + needforclo_cent +\n                (1|Score_Type)+(1|ID), data=expertise6_ready)\nsummary(Model_perc)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: ih ~ know_cent * needforclo_cent + know_cent + needforclo_cent +  \n    (1 | Score_Type) + (1 | ID)\n   Data: expertise6_ready\n\nREML criterion at convergence: 6321.4\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.92364 -0.65771  0.01587  0.66300  2.56876 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.7989   0.8938  \n Score_Type (Intercept) 0.7760   0.8809  \n Residual               3.1140   1.7646  \nNumber of obs: 1528, groups:  ID, 191; Score_Type, 8\n\nFixed effects:\n                            Estimate Std. Error         df t value Pr(>|t|)    \n(Intercept)                  5.10150    0.32130    7.60464  15.878 4.19e-07 ***\nknow_cent                   -0.06434    0.05369 1451.39265  -1.198    0.231    \nneedforclo_cent              0.18403    0.11385  185.25861   1.616    0.108    \nknow_cent:needforclo_cent   -0.02139    0.07491 1397.20099  -0.286    0.775    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) knw_cn ndfrc_\nknow_cent    0.001              \nnedfrcl_cnt  0.000  0.046       \nknw_cnt:nd_  0.012  0.063 -0.040\n\n\n\n\nModel of objective expertise, perceived expertise and need for cognition without interaction\n\nModel.1.1<-lmer(ih ~class_wins1_cent + media_wins1_cent + know_cent +needforclo_cent +(1|Score_Type)+(1|ID),   \n              data=expertise6_ready)\nsummary(Model.1.1)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: \nih ~ class_wins1_cent + media_wins1_cent + know_cent + needforclo_cent +  \n    (1 | Score_Type) + (1 | ID)\n   Data: expertise6_ready\n\nREML criterion at convergence: 6316.6\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.94278 -0.66321  0.01385  0.66298  2.54206 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.7506   0.8664  \n Score_Type (Intercept) 0.7758   0.8808  \n Residual               3.1160   1.7652  \nNumber of obs: 1528, groups:  ID, 191; Score_Type, 8\n\nFixed effects:\n                   Estimate Std. Error         df t value Pr(>|t|)    \n(Intercept)       5.103e+00  3.209e-01  7.566e+00  15.905 4.36e-07 ***\nclass_wins1_cent  2.461e-02  7.825e-02  1.493e+03   0.314  0.75321    \nmedia_wins1_cent -2.259e-01  8.177e-02  1.508e+03  -2.762  0.00581 ** \nknow_cent        -2.644e-03  5.943e-02  1.411e+03  -0.044  0.96453    \nneedforclo_cent   1.823e-01  1.114e-01  1.840e+02   1.635  0.10367    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cls_1_ md_w1_ knw_cn\nclss_wns1_c  0.000                     \nmd_wns1_cnt  0.000 -0.357              \nknow_cent    0.001 -0.239 -0.259       \nnedfrcl_cnt  0.000 -0.001  0.003  0.044\n\n\n\n\nModel of objective expertise, perceived expertise and need for cognition with interaction\n\nModel.2.1<-lmer(ih ~class_wins1_cent + media_wins1_cent  + know_cent+ needforclo_cent +\n                  class_wins1_cent*needforclo_cent + media_wins1_cent*needforclo_cent +\n                  know_cent*needforclo_cent+(1|Score_Type)+(1|ID),data=expertise6_ready)\nsummary(Model.2.1)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: \nih ~ class_wins1_cent + media_wins1_cent + know_cent + needforclo_cent +  \n    class_wins1_cent * needforclo_cent + media_wins1_cent * needforclo_cent +  \n    know_cent * needforclo_cent + (1 | Score_Type) + (1 | ID)\n   Data: expertise6_ready\n\nREML criterion at convergence: 6324.1\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.94235 -0.65807  0.01588  0.66131  2.55946 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.7574   0.8703  \n Score_Type (Intercept) 0.7741   0.8798  \n Residual               3.1172   1.7656  \nNumber of obs: 1528, groups:  ID, 191; Score_Type, 8\n\nFixed effects:\n                                   Estimate Std. Error         df t value\n(Intercept)                       5.102e+00  3.206e-01  7.573e+00  15.914\nclass_wins1_cent                  2.937e-02  7.858e-02  1.490e+03   0.374\nmedia_wins1_cent                 -2.337e-01  8.223e-02  1.499e+03  -2.842\nknow_cent                        -1.025e-03  5.960e-02  1.407e+03  -0.017\nneedforclo_cent                   1.836e-01  1.120e-01  1.834e+02   1.640\nclass_wins1_cent:needforclo_cent  9.434e-02  1.101e-01  1.439e+03   0.857\nmedia_wins1_cent:needforclo_cent -1.188e-01  1.108e-01  1.512e+03  -1.073\nknow_cent:needforclo_cent        -8.282e-03  8.415e-02  1.402e+03  -0.098\n                                 Pr(>|t|)    \n(Intercept)                      4.29e-07 ***\nclass_wins1_cent                  0.70864    \nmedia_wins1_cent                  0.00455 ** \nknow_cent                         0.98628    \nneedforclo_cent                   0.10278    \nclass_wins1_cent:needforclo_cent  0.39167    \nmedia_wins1_cent:needforclo_cent  0.28364    \nknow_cent:needforclo_cent         0.92162    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cls_1_ md_w1_ knw_cn ndfrc_ c_1_:_ m_1_:_\nclss_wns1_c  0.001                                          \nmd_wns1_cnt -0.001 -0.359                                   \nknow_cent    0.001 -0.234 -0.263                            \nnedfrcl_cnt  0.000 -0.005  0.002  0.043                     \nclss_wn1_:_ -0.001  0.075 -0.043  0.023 -0.024              \nmd_wns1_c:_  0.000 -0.044  0.099 -0.044 -0.014 -0.495       \nknw_cnt:nd_  0.011  0.023 -0.038  0.051 -0.018 -0.213 -0.249\n\n\n#Plot of the model 1.1\n\nModel.1.1 <- lmer(ih ~class_wins1_cent + media_wins1_cent + know_cent +needforclo_cent +(1|Score_Type)+(1|ID),   \n              data=expertise6_ready)\nsummary(Model.1.1)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: \nih ~ class_wins1_cent + media_wins1_cent + know_cent + needforclo_cent +  \n    (1 | Score_Type) + (1 | ID)\n   Data: expertise6_ready\n\nREML criterion at convergence: 6316.6\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.94278 -0.66321  0.01385  0.66298  2.54206 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.7506   0.8664  \n Score_Type (Intercept) 0.7758   0.8808  \n Residual               3.1160   1.7652  \nNumber of obs: 1528, groups:  ID, 191; Score_Type, 8\n\nFixed effects:\n                   Estimate Std. Error         df t value Pr(>|t|)    \n(Intercept)       5.103e+00  3.209e-01  7.566e+00  15.905 4.36e-07 ***\nclass_wins1_cent  2.461e-02  7.825e-02  1.493e+03   0.314  0.75321    \nmedia_wins1_cent -2.259e-01  8.177e-02  1.508e+03  -2.762  0.00581 ** \nknow_cent        -2.644e-03  5.943e-02  1.411e+03  -0.044  0.96453    \nneedforclo_cent   1.823e-01  1.114e-01  1.840e+02   1.635  0.10367    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cls_1_ md_w1_ knw_cn\nclss_wns1_c  0.000                     \nmd_wns1_cnt  0.000 -0.357              \nknow_cent    0.001 -0.239 -0.259       \nnedfrcl_cnt  0.000 -0.001  0.003  0.044\n\n# save fitted data\nef.1.media <- effect(term = \"media_wins1_cent\",\n                                   mod = Model.1.1)\nef.1.media.data <- as.data.frame(ef.1.media) #convert the effects list to a data frame\nef.1.media.data #print effects data frame\n\n  media_wins1_cent      fit        se    lower    upper\n1             -0.5 5.215930 0.3234555 4.581465 5.850395\n2              1.0 4.877152 0.3310815 4.227728 5.526575\n3              3.0 4.425447 0.4038177 3.633349 5.217545\n4              5.0 3.973743 0.5196216 2.954493 4.992992\n5              7.0 3.522038 0.6560706 2.235141 4.808936\n\nexpert_media_graph <- expertise6_ready %>%\n  group_by(ID) %>%\n  summarise(ih = mean(ih),\n            media_wins1_cent = media_wins1)\nexpert_media_graph <- as.data.frame(expert_media_graph)\n\nexpert_media_graph$media_wins1_cent<-expert_media_graph$media_wins1_cent-min(expert_media_graph$media_wins1_cent)\n\n\nfig.1.expert6_w1 <- ggplot(data = ef.1.media.data, aes(x = media_wins1_cent,\n                                                  y = fit)) +\n  geom_point(data=expert_media_graph, aes(x=media_wins1_cent,y=ih),\n             show.legend = FALSE, pch=21, color=\"blueviolet\", alpha=.25, size=3) +\n  geom_line(aes(x=media_wins1_cent), color = \"darkred\", size = 1.2)  +\n  geom_ribbon(aes(ymin = fit-se, ymax = fit+se), alpha = 0.50, fill = \"gray70\") +\n  labs(title = \"Expertise and Inherence Bias\", x = \"Expertise (books,articles)\",\n       y = \"Inherence bias\") +\n  scale_x_continuous(limits = c(0, 8), breaks = seq(0, 8, 2), expand = c(0, 0)) +\n  scale_y_continuous(limits = c(0, 10), breaks = seq(0, 10, 2), expand = c(0, 0)) +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n        axis.title = element_text(size = 14, face = \"bold\"),\n        axis.text = element_text(size = 12),\n        legend.title = element_blank(),\n        panel.grid = element_blank(),\n        panel.border = element_blank(),\n        plot.background=element_rect(fill = \"gray96\"),\n        panel.background = element_rect(fill = \"gray96\"))\nfig.1.expert6_w1\n\n\n\nggsave(paste0(\"figure_expertise6+media_w1\",\n              format(Sys.time(), \"%Y-%m-%d\")\n              ,\".eps\"), width = 20, height = 20, units = \"cm\") ###Save the Figure 3C"
  },
  {
    "objectID": "posts/Expertise3_Data_From_Zero_2022-01-04/index.html#take-the-means-of-winsorized-variables-to-create-our-new-categories-media-and-class",
    "href": "posts/Expertise3_Data_From_Zero_2022-01-04/index.html#take-the-means-of-winsorized-variables-to-create-our-new-categories-media-and-class",
    "title": "Expertise3 Data Analysis from the very beginning",
    "section": "Take the means of winsorized variables to create our new categories: media and class",
    "text": "Take the means of winsorized variables to create our new categories: media and class\n\nexpertise3_wins_average <- expertise3_wins_average %>% mutate(class_wins1 = mapply(function(x, y, z) {\n  ifelse(is.na(x), (y + z)/2, \n         ifelse(is.na(y), (x + z)/2, \n                ifelse(is.na(z), (x + y)/2, (x + y + z)/3)))}, \n  grad_w1, colle_w1, high_w1))\n\nexpertise3_wins_average <- expertise3_wins_average %>% mutate(media_wins1 = \n                                                          mapply(function(x, y) {\n  ifelse(is.na(x), (y), \n         ifelse(is.na(y), (x),\n                  (x + y)/2))}, \n                      numarticle_w1,numbook_w1))\n\n##Shapiro-Wilk values\n\ndescriptives(expertise3_wins_average, vars = vars(class_wins1, media_wins1, know, ih), n=FALSE, missing= FALSE, median=FALSE, sw = TRUE)\n\n\n DESCRIPTIVES\n\n Descriptives                                                                         \n ──────────────────────────────────────────────────────────────────────────────────── \n                         class_wins1     media_wins1     know            ih           \n ──────────────────────────────────────────────────────────────────────────────────── \n   Mean                  1.297120e-17    2.043312e-17    4.689314e-17      4.869579   \n   Standard deviation       0.7425085       0.8782155       0.9977963      1.891089   \n   Minimum                 -0.5346050      -0.4793892       -2.659207      1.000000   \n   Maximum                   7.340605        9.991529        2.956309      9.000000   \n   Shapiro-Wilk W           0.4621646       0.5037120       0.9918312     0.9818945   \n   Shapiro-Wilk p          < .0000001      < .0000001      < .0000001    < .0000001   \n ──────────────────────────────────────────────────────────────────────────────────── \n\n\n\n# Center variables: \n\nexpertise3_wins_average$know_cent <- scale(expertise3_wins_average$know, center = TRUE, scale = FALSE)\n\nexpertise3_wins_average$needforcog_cent <- scale(expertise3_wins_average$needforcog, center = TRUE, scale = FALSE)\n\nexpertise3_wins_average$class_wins1_cent <- scale(expertise3_wins_average$class_wins1, center = TRUE, scale = FALSE)\n\nexpertise3_wins_average$media_wins1_cent <- scale(expertise3_wins_average$media_wins1, center = TRUE, scale = FALSE)"
  },
  {
    "objectID": "posts/Expertise3_Data_From_Zero_2022-01-04/index.html#hlm-models-2",
    "href": "posts/Expertise3_Data_From_Zero_2022-01-04/index.html#hlm-models-2",
    "title": "Expertise3 Data Analysis from the very beginning",
    "section": "HLM Models",
    "text": "HLM Models\n\nModel of objective expertise, perceived expertise and need for cognition without interaction\n\nModel.1.1<-lmer(ih ~class_wins1_cent + media_wins1_cent + know_cent +needforcog_cent +(1|Score_Type)+(1|ID),   \n              data=expertise3_wins_average)\nsummary(Model.1.1)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: \nih ~ class_wins1_cent + media_wins1_cent + know_cent + needforcog_cent +  \n    (1 | Score_Type) + (1 | ID)\n   Data: expertise3_wins_average\n\nREML criterion at convergence: 6294.1\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-3.02716 -0.65887 -0.02207  0.61398  2.83610 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.488    0.6985  \n Score_Type (Intercept) 0.359    0.5991  \n Residual               2.676    1.6359  \nNumber of obs: 1590, groups:  ID, 199; Score_Type, 8\n\nFixed effects:\n                   Estimate Std. Error         df t value Pr(>|t|)    \n(Intercept)         4.87054    0.22137    7.75433  22.002 2.87e-08 ***\nclass_wins1_cent    0.10399    0.06558 1565.35319   1.586  0.11302    \nmedia_wins1_cent   -0.17748    0.06032 1323.81512  -2.942  0.00331 ** \nknow_cent           0.11515    0.05079 1330.40508   2.267  0.02353 *  \nneedforcog_cent    -0.20245    0.05022  196.37030  -4.032 7.92e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cls_1_ md_w1_ knw_cn\nclss_wns1_c  0.000                     \nmd_wns1_cnt  0.000 -0.302              \nknow_cent    0.000 -0.109 -0.245       \nnedfrcg_cnt  0.000 -0.027 -0.059  0.000\n\n\n#Plot of the model 1.1\n\nModel.1.1 <- lmer(ih ~class_wins1_cent + media_wins1_cent + know_cent +needforcog_cent +(1|Score_Type)+(1|ID),   \n              data=expertise3_wins_average)\nsummary(Model.1.1)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: \nih ~ class_wins1_cent + media_wins1_cent + know_cent + needforcog_cent +  \n    (1 | Score_Type) + (1 | ID)\n   Data: expertise3_wins_average\n\nREML criterion at convergence: 6294.1\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-3.02716 -0.65887 -0.02207  0.61398  2.83610 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.488    0.6985  \n Score_Type (Intercept) 0.359    0.5991  \n Residual               2.676    1.6359  \nNumber of obs: 1590, groups:  ID, 199; Score_Type, 8\n\nFixed effects:\n                   Estimate Std. Error         df t value Pr(>|t|)    \n(Intercept)         4.87054    0.22137    7.75433  22.002 2.87e-08 ***\nclass_wins1_cent    0.10399    0.06558 1565.35319   1.586  0.11302    \nmedia_wins1_cent   -0.17748    0.06032 1323.81512  -2.942  0.00331 ** \nknow_cent           0.11515    0.05079 1330.40508   2.267  0.02353 *  \nneedforcog_cent    -0.20245    0.05022  196.37030  -4.032 7.92e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cls_1_ md_w1_ knw_cn\nclss_wns1_c  0.000                     \nmd_wns1_cnt  0.000 -0.302              \nknow_cent    0.000 -0.109 -0.245       \nnedfrcg_cnt  0.000 -0.027 -0.059  0.000\n\n# save fitted data\nef.1.media <- effect(term = \"media_wins1_cent\",\n                                   mod = Model.1.1)\nef.1.media.data <- as.data.frame(ef.1.media) #convert the effects list to a data frame\nef.1.media.data #print effects data frame\n\n  media_wins1_cent      fit        se    lower    upper\n1             -0.5 4.958820 0.2234149 4.520600 5.397039\n2              2.0 4.515125 0.2521131 4.020614 5.009635\n3              5.0 3.982690 0.3741251 3.248858 4.716523\n4              7.0 3.627734 0.4767515 2.692605 4.562864\n5             10.0 3.095300 0.6425344 1.834994 4.355607\n\nexpert_media_graph <- expertise3_wins_average %>%\n  group_by(ID) %>%\n  summarise(ih = mean(ih),\n            media_wins1_cent = media_wins1)\nexpert_media_graph <- as.data.frame(expert_media_graph)\n\nexpert_media_graph$media_wins1_cent<-expert_media_graph$media_wins1_cent-min(expert_media_graph$media_wins1_cent)\n\n\nfig.1.expert_w1 <- ggplot(data = ef.1.media.data, aes(x = media_wins1_cent,\n                                                  y = fit)) +\n  geom_point(data=expert_media_graph, aes(x=media_wins1_cent,y=ih),\n             show.legend = FALSE, pch=21, color=\"blueviolet\", alpha=.25, size=3) +\n  geom_line(aes(x=media_wins1_cent), color = \"darkred\", size = 1.2)  +\n  geom_ribbon(aes(ymin = fit-se, ymax = fit+se), alpha = 0.50, fill = \"gray70\") +\n  labs(title = \"Expertise and Inherence Bias\", x = \"Expertise (books,articles)\",\n       y = \"Inherence bias\") +\n  scale_x_continuous(limits = c(0, 8), breaks = seq(0, 8, 2), expand = c(0, 0)) +\n  scale_y_continuous(limits = c(0, 8), breaks = seq(0, 8, 2), expand = c(0, 0)) +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n        axis.title = element_text(size = 14, face = \"bold\"),\n        axis.text = element_text(size = 12),\n        legend.title = element_blank(),\n        panel.grid = element_blank(),\n        panel.border = element_blank(),\n        plot.background=element_rect(fill = \"gray96\"),\n        panel.background = element_rect(fill = \"gray96\"))\nfig.1.expert_w1\n\n\n\nggsave(paste0(\"figure_expertise+media_w1\",\n              format(Sys.time(), \"%Y-%m-%d\")\n              ,\".eps\"), width = 20, height = 20, units = \"cm\") ###Save the Figure 3C\n\n#Winsorize at 2.5% the raw variables for our new expertise variables\n\n# winsorize the variables (at 1%)\nexpertise3_wins_average_w25 <- expertise3_wins_fact%>%\n  mutate(numarticle_w1=Winsorize(numarticle, probs = c(0,0.975)), \n         numbook_w1=Winsorize(numbook, probs = c(0,0.975)),\n         high_w1=Winsorize(high, probs = c(0,0.975)),\n         colle_w1=Winsorize(colle, probs = c(0,0.975)),\n         grad_w1=Winsorize(grad,na.rm=TRUE, probs = c(0,0.975)))\n\n\n# Standardize the Selected variables  ----\nvars_to_standardize <- c(\"know\", \"numarticle_w1\",\"numbook_w1\", \"high_w1\",\"colle_w1\", \"grad_w1\")\n\n\nexpertise3_wins_average_w25<-expertise3_wins_average_w25 %>% \n  group_by(Score_Type) %>% \n  mutate_at(vars(vars_to_standardize), scale)"
  },
  {
    "objectID": "posts/Expertise3_Data_From_Zero_2022-01-04/index.html#take-the-means-of-winsorized-variables-to-create-our-new-categories-media-and-class-1",
    "href": "posts/Expertise3_Data_From_Zero_2022-01-04/index.html#take-the-means-of-winsorized-variables-to-create-our-new-categories-media-and-class-1",
    "title": "Expertise3 Data Analysis from the very beginning",
    "section": "Take the means of winsorized variables to create our new categories: media and class",
    "text": "Take the means of winsorized variables to create our new categories: media and class\n\nexpertise3_wins_average_w25 <- expertise3_wins_average_w25 %>% mutate(class_wins1 = mapply(function(x, y, z) {\n  ifelse(is.na(x), (y + z)/2, \n         ifelse(is.na(y), (x + z)/2, \n                ifelse(is.na(z), (x + y)/2, (x + y + z)/3)))}, \n  grad_w1, colle_w1, high_w1))\n\nexpertise3_wins_average_w25 <- expertise3_wins_average_w25 %>% mutate(media_wins1 = \n                                                          mapply(function(x, y) {\n  ifelse(is.na(x), (y), \n         ifelse(is.na(y), (x),\n                  (x + y)/2))}, \n                      numarticle_w1,numbook_w1))\n\n##Shapiro-Wilk values\n\ndescriptives(expertise3_wins_average_w25, vars = vars(class_wins1, media_wins1, know, ih), n=FALSE, missing= FALSE, median=FALSE, sw = TRUE)\n\n\n DESCRIPTIVES\n\n Descriptives                                                                          \n ───────────────────────────────────────────────────────────────────────────────────── \n                         class_wins1     media_wins1      know            ih           \n ───────────────────────────────────────────────────────────────────────────────────── \n   Mean                  1.471464e-17    -5.021109e-18    4.689314e-17      4.869579   \n   Standard deviation       0.8508659        0.8782263       0.9977963      1.891089   \n   Minimum                 -0.7824349       -0.5626174       -2.659207      1.000000   \n   Maximum                   6.440938         7.384871        2.956309      9.000000   \n   Shapiro-Wilk W           0.5612456        0.6047509       0.9918312     0.9818945   \n   Shapiro-Wilk p          < .0000001       < .0000001      < .0000001    < .0000001   \n ───────────────────────────────────────────────────────────────────────────────────── \n\n\n\n# Center variables: \n\nexpertise3_wins_average_w25$know_cent <- scale(expertise3_wins_average_w25$know, center = TRUE, scale = FALSE)\n\nexpertise3_wins_average_w25$needforcog_cent <- scale(expertise3_wins_average_w25$needforcog, center = TRUE, scale = FALSE)\n\nexpertise3_wins_average_w25$class_wins1_cent <- scale(expertise3_wins_average_w25$class_wins1, center = TRUE, scale = FALSE)\n\nexpertise3_wins_average_w25$media_wins1_cent <- scale(expertise3_wins_average_w25$media_wins1, center = TRUE, scale = FALSE)"
  },
  {
    "objectID": "posts/Expertise3_Data_From_Zero_2022-01-04/index.html#hlm-models-3",
    "href": "posts/Expertise3_Data_From_Zero_2022-01-04/index.html#hlm-models-3",
    "title": "Expertise3 Data Analysis from the very beginning",
    "section": "HLM Models",
    "text": "HLM Models\n\nModel of objective expertise, perceived expertise and need for cognition\n\nModel.1.1<-lmer(ih ~class_wins1_cent + media_wins1_cent + know_cent +needforcog_cent +(1|Score_Type)+(1|ID),   \n              data=expertise3_wins_average_w25)\nsummary(Model.1.1)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: \nih ~ class_wins1_cent + media_wins1_cent + know_cent + needforcog_cent +  \n    (1 | Score_Type) + (1 | ID)\n   Data: expertise3_wins_average_w25\n\nREML criterion at convergence: 6294.1\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-3.02153 -0.64676 -0.02857  0.61140  2.81613 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.4918   0.7013  \n Score_Type (Intercept) 0.3590   0.5991  \n Residual               2.6740   1.6353  \nNumber of obs: 1590, groups:  ID, 199; Score_Type, 8\n\nFixed effects:\n                   Estimate Std. Error         df t value Pr(>|t|)    \n(Intercept)         4.87054    0.22142    7.76025  21.997 2.85e-08 ***\nclass_wins1_cent    0.06547    0.05651 1577.81243   1.159  0.24683    \nmedia_wins1_cent   -0.18164    0.05939 1432.18838  -3.058  0.00227 ** \nknow_cent           0.12442    0.05113 1321.38198   2.433  0.01509 *  \nneedforcog_cent    -0.20102    0.05030  195.97212  -3.997 9.09e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cls_1_ md_w1_ knw_cn\nclss_wns1_c  0.000                     \nmd_wns1_cnt  0.000 -0.278              \nknow_cent    0.000 -0.107 -0.269       \nnedfrcg_cnt  0.000 -0.017 -0.059 -0.001\n\n\n#Plot of the model 1.1 with 2.5 winsorize\n\nModel.1.1 <- lmer(ih ~class_wins1_cent + media_wins1_cent + know_cent +needforcog_cent +(1|Score_Type)+(1|ID),   \n              data=expertise3_wins_average_w25)\nsummary(Model.1.1)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: \nih ~ class_wins1_cent + media_wins1_cent + know_cent + needforcog_cent +  \n    (1 | Score_Type) + (1 | ID)\n   Data: expertise3_wins_average_w25\n\nREML criterion at convergence: 6294.1\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-3.02153 -0.64676 -0.02857  0.61140  2.81613 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.4918   0.7013  \n Score_Type (Intercept) 0.3590   0.5991  \n Residual               2.6740   1.6353  \nNumber of obs: 1590, groups:  ID, 199; Score_Type, 8\n\nFixed effects:\n                   Estimate Std. Error         df t value Pr(>|t|)    \n(Intercept)         4.87054    0.22142    7.76025  21.997 2.85e-08 ***\nclass_wins1_cent    0.06547    0.05651 1577.81243   1.159  0.24683    \nmedia_wins1_cent   -0.18164    0.05939 1432.18838  -3.058  0.00227 ** \nknow_cent           0.12442    0.05113 1321.38198   2.433  0.01509 *  \nneedforcog_cent    -0.20102    0.05030  195.97212  -3.997 9.09e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cls_1_ md_w1_ knw_cn\nclss_wns1_c  0.000                     \nmd_wns1_cnt  0.000 -0.278              \nknow_cent    0.000 -0.107 -0.269       \nnedfrcg_cnt  0.000 -0.017 -0.059 -0.001\n\n# save fitted data\nef.1.media <- effect(term = \"media_wins1_cent\",\n                                   mod = Model.1.1)\nef.1.media.data <- as.data.frame(ef.1.media) #convert the effects list to a data frame\nef.1.media.data #print effects data frame\n\n  media_wins1_cent      fit        se    lower    upper\n1             -0.6 4.979056 0.2242627 4.539173 5.418939\n2              1.0 4.688432 0.2292460 4.238775 5.138090\n3              3.0 4.325153 0.2842137 3.767679 4.882627\n4              5.0 3.961873 0.3704346 3.235280 4.688467\n5              7.0 3.598593 0.4710488 2.674649 4.522538\n\nexpert_media_graph <- expertise3_wins_average_w25 %>%\n  group_by(ID) %>%\n  summarise(ih = mean(ih),\n            media_wins1_cent = media_wins1)\nexpert_media_graph <- as.data.frame(expert_media_graph)\n\nexpert_media_graph$media_wins1_cent<-expert_media_graph$media_wins1_cent-min(expert_media_graph$media_wins1_cent)\n\n\nfig.1.expert_w25 <- ggplot(data = ef.1.media.data, aes(x = media_wins1_cent,\n                                                  y = fit)) +\n  geom_point(data=expert_media_graph, aes(x=media_wins1_cent,y=ih),\n             show.legend = FALSE, pch=21, color=\"blueviolet\", alpha=.25, size=3) +\n  geom_line(aes(x=media_wins1_cent), color = \"darkred\", size = 1.2)  +\n  geom_ribbon(aes(ymin = fit-se, ymax = fit+se), alpha = 0.50, fill = \"gray70\") +\n  labs(title = \"Expertise and Inherence Bias\", x = \"Expertise (books,articles)\",\n       y = \"Inherence bias\") +\n  scale_x_continuous(limits = c(0, 10), breaks = seq(0, 8, 2), expand = c(0, 0)) +\n  scale_y_continuous(limits = c(0, 8), breaks = seq(0, 8, 2), expand = c(0, 0)) +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n        axis.title = element_text(size = 14, face = \"bold\"),\n        axis.text = element_text(size = 12),\n        legend.title = element_blank(),\n        panel.grid = element_blank(),\n        panel.border = element_blank(),\n        plot.background=element_rect(fill = \"gray96\"),\n        panel.background = element_rect(fill = \"gray96\"))\nfig.1.expert_w25\n\n\n\nggsave(paste0(\"figure_expertise+media_w25\",\n              format(Sys.time(), \"%Y-%m-%d\")\n              ,\".eps\"), width = 20, height = 20, units = \"cm\") ###Save the Figure 3C\n\n#Sqrt variables\n##Square root of the raw variables for our new expertise variables\n\n# take the square root of the variables\nexpertise3_sqrt <- expertise3_wins_fact%>%\n  mutate(numarticle_sqrt=sqrt(numarticle),\n         numbook_sqrt=sqrt(numbook),\n         high_sqrt=sqrt(high),\n         colle_sqrt=sqrt(colle),\n         grad_sqrt=sqrt(grad))"
  },
  {
    "objectID": "posts/Expertise3_Data_From_Zero_2022-01-04/index.html#standardize-and-average-of-our-variables-to-create-our-new-categories-media-and-class",
    "href": "posts/Expertise3_Data_From_Zero_2022-01-04/index.html#standardize-and-average-of-our-variables-to-create-our-new-categories-media-and-class",
    "title": "Expertise3 Data Analysis from the very beginning",
    "section": "Standardize and average of our variables to create our new categories: media and class",
    "text": "Standardize and average of our variables to create our new categories: media and class\n\n# Standardize the Selected variables  ----\nvars_to_standardize <- c(\"know\", \"numarticle_sqrt\",\"numbook_sqrt\", \"high_sqrt\",\"colle_sqrt\", \"grad_sqrt\")\n\n\nexpertise3_sqrt<-expertise3_sqrt %>% \n  group_by(Score_Type) %>% \n  mutate_at(vars(vars_to_standardize), scale)\n\n\nexpertise3_sqrt <- expertise3_sqrt %>% mutate(class_wins1 = mapply(function(x, y, z) {\n  ifelse(is.na(x), (y + z)/2, \n         ifelse(is.na(y), (x + z)/2, \n                ifelse(is.na(z), (x + y)/2, (x + y + z)/3)))}, \n  grad_sqrt, colle_sqrt, high_sqrt))\n\nexpertise3_sqrt <- expertise3_sqrt %>% mutate(media_wins1 = \n                                                          mapply(function(x, y) {\n  ifelse(is.na(x), (y), \n         ifelse(is.na(y), (x),\n                  (x + y)/2))}, \n                      numarticle_sqrt,numbook_sqrt))\n\n##Histograms of square root variables\n\nhist(expertise3_sqrt$class_wins1)\n\n\n\n\n\nhist(expertise3_sqrt$media_wins1)\n\n\n\n\n\nhist(expertise3_sqrt$know)\n\n\n\n\n\nhist(expertise3_sqrt$ih)\n\n\n\n\n##Shapiro-Wilk values\n\ndescriptives(expertise3_sqrt, vars = vars(class_wins1, media_wins1, know, ih), n=FALSE, missing= FALSE, median=FALSE, sw = TRUE)\n\n\n DESCRIPTIVES\n\n Descriptives                                                                          \n ───────────────────────────────────────────────────────────────────────────────────── \n                         class_wins1     media_wins1      know            ih           \n ───────────────────────────────────────────────────────────────────────────────────── \n   Mean                  3.005692e-17    -1.588623e-16    4.689314e-17      4.869579   \n   Standard deviation       0.7488095        0.8749660       0.9977963      1.891089   \n   Minimum                 -0.6141819       -0.7077785       -2.659207      1.000000   \n   Maximum                   7.141527         8.588626        2.956309      9.000000   \n   Shapiro-Wilk W           0.5201987        0.6946538       0.9918312     0.9818945   \n   Shapiro-Wilk p          < .0000001       < .0000001      < .0000001    < .0000001   \n ───────────────────────────────────────────────────────────────────────────────────── \n\n\n##Cente the variables\n\n# Center variables: \n\nexpertise3_sqrt$know_cent <- scale(expertise3_sqrt$know, center = TRUE, scale = FALSE)\n\nexpertise3_sqrt$needforcog_cent <- scale(expertise3_sqrt$needforcog, center = TRUE, scale = FALSE)\n\nexpertise3_sqrt$class_wins1_cent <- scale(expertise3_sqrt$class_wins1, center = TRUE, scale = FALSE)\n\nexpertise3_sqrt$media_wins1_cent <- scale(expertise3_sqrt$media_wins1, center = TRUE, scale = FALSE)"
  },
  {
    "objectID": "posts/Expertise3_Data_From_Zero_2022-01-04/index.html#hlm-models-4",
    "href": "posts/Expertise3_Data_From_Zero_2022-01-04/index.html#hlm-models-4",
    "title": "Expertise3 Data Analysis from the very beginning",
    "section": "HLM Models",
    "text": "HLM Models\n\nModel of objective expertise, perceived expertise and need for cognition\n\nModel.1.1<-lmer(ih ~class_wins1_cent + media_wins1_cent + know_cent +needforcog_cent +(1|Score_Type)+(1|ID),   \n              data=expertise3_sqrt)\nsummary(Model.1.1)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: \nih ~ class_wins1_cent + media_wins1_cent + know_cent + needforcog_cent +  \n    (1 | Score_Type) + (1 | ID)\n   Data: expertise3_sqrt\n\nREML criterion at convergence: 6295.8\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.98887 -0.65579 -0.02521  0.61279  2.84589 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.4937   0.7026  \n Score_Type (Intercept) 0.3589   0.5991  \n Residual               2.6771   1.6362  \nNumber of obs: 1590, groups:  ID, 199; Score_Type, 8\n\nFixed effects:\n                   Estimate Std. Error         df t value Pr(>|t|)    \n(Intercept)         4.87049    0.22143    7.76331  21.996 2.84e-08 ***\nclass_wins1_cent    0.08276    0.06617 1571.06950   1.251  0.21118    \nmedia_wins1_cent   -0.17106    0.06382 1220.55148  -2.680  0.00745 ** \nknow_cent           0.12201    0.05160 1311.37413   2.364  0.01820 *  \nneedforcog_cent    -0.20105    0.05044  196.73295  -3.986 9.47e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cls_1_ md_w1_ knw_cn\nclss_wns1_c  0.000                     \nmd_wns1_cnt  0.000 -0.339              \nknow_cent    0.000 -0.086 -0.291       \nnedfrcg_cnt  0.000 -0.026 -0.069  0.008\n\n\n#Cube root variables\n##Cube root of the raw variables for our new expertise variables\n\n# cube root of the variables \nexpertise3_cube <- expertise3_wins_fact%>%\n  mutate(numarticle_cube=(numarticle^(1/3)),\n         numbook_cube=(numbook^(1/3)),\n         high_cube=(high^(1/3)),\n         colle_cube=(colle^(1/3)),\n         grad_cube=(grad^(1/3)))"
  },
  {
    "objectID": "posts/Expertise3_Data_From_Zero_2022-01-04/index.html#standardize-and-average-of-variables-to-create-our-new-categories-media-and-class",
    "href": "posts/Expertise3_Data_From_Zero_2022-01-04/index.html#standardize-and-average-of-variables-to-create-our-new-categories-media-and-class",
    "title": "Expertise3 Data Analysis from the very beginning",
    "section": "Standardize and average of variables to create our new categories: media and class",
    "text": "Standardize and average of variables to create our new categories: media and class\n\n# Standardize the Selected variables  ----\nvars_to_standardize <- c(\"know\", \"numarticle_cube\",\"numbook_cube\", \"high_cube\",\"colle_cube\", \"grad_cube\")\n\n\nexpertise3_cube<-expertise3_cube %>% \n  group_by(Score_Type) %>% \n  mutate_at(vars(vars_to_standardize), scale)\n\n\nexpertise3_cube <- expertise3_cube %>% mutate(class_wins1 = mapply(function(x, y, z) {\n  ifelse(is.na(x), (y + z)/2, \n         ifelse(is.na(y), (x + z)/2, \n                ifelse(is.na(z), (x + y)/2, (x + y + z)/3)))}, \n  grad_cube, colle_cube, high_cube))\n\nexpertise3_cube <- expertise3_cube %>% mutate(media_wins1 = \n                                                          mapply(function(x, y) {\n  ifelse(is.na(x), (y), \n         ifelse(is.na(y), (x),\n                  (x + y)/2))}, \n                       numarticle_cube,numbook_cube))\n\n##Histograms of square root variables\n\nhist(expertise3_cube$class_wins1)\n\n\n\n\n\nhist(expertise3_cube$media_wins1)\n\n\n\n\n\nhist(expertise3_cube$know)\n\n\n\n\n\nhist(expertise3_cube$ih)\n\n\n\n\n##Shapiro-Wilk values\n\ndescriptives(expertise3_cube, vars = vars(class_wins1, media_wins1, know, ih), n=FALSE, missing= FALSE, median=FALSE,sw = TRUE)\n\n\n DESCRIPTIVES\n\n Descriptives                                                                          \n ───────────────────────────────────────────────────────────────────────────────────── \n                         class_wins1      media_wins1     know            ih           \n ───────────────────────────────────────────────────────────────────────────────────── \n   Mean                  -6.380993e-18    3.138193e-17    4.689314e-17      4.869579   \n   Standard deviation        0.7507681       0.8707762       0.9977963      1.891089   \n   Minimum                  -0.6582257      -0.8354621       -2.659207      1.000000   \n   Maximum                    7.213367        6.302444        2.956309      9.000000   \n   Shapiro-Wilk W            0.5488866       0.8207145       0.9918312     0.9818945   \n   Shapiro-Wilk p           < .0000001      < .0000001      < .0000001    < .0000001   \n ───────────────────────────────────────────────────────────────────────────────────── \n\n\n##Center variables\n\n# Center variables: \n\nexpertise3_cube$know_cent <- scale(expertise3_cube$know, center = TRUE, scale = FALSE)\n\nexpertise3_cube$needforcog_cent <- scale(expertise3_cube$needforcog, center = TRUE, scale = FALSE)\n\nexpertise3_cube$class_wins1_cent <- scale(expertise3_cube$class_wins1, center = TRUE, scale = FALSE)\n\nexpertise3_cube$media_wins1_cent <- scale(expertise3_cube$media_wins1, center = TRUE, scale = FALSE)"
  },
  {
    "objectID": "posts/Expertise3_Data_From_Zero_2022-01-04/index.html#hlm-models-5",
    "href": "posts/Expertise3_Data_From_Zero_2022-01-04/index.html#hlm-models-5",
    "title": "Expertise3 Data Analysis from the very beginning",
    "section": "HLM Models",
    "text": "HLM Models\n\nModel of objective expertise, perceived expertise and need for cognition\n\nModel.1.1<-lmer(ih ~class_wins1_cent + media_wins1_cent + know_cent +needforcog_cent +(1|Score_Type)+(1|ID),   \n              data=expertise3_cube)\nsummary(Model.1.1)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: \nih ~ class_wins1_cent + media_wins1_cent + know_cent + needforcog_cent +  \n    (1 | Score_Type) + (1 | ID)\n   Data: expertise3_cube\n\nREML criterion at convergence: 6297.3\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.9804 -0.6563 -0.0198  0.6097  2.8588 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.4951   0.7037  \n Score_Type (Intercept) 0.3589   0.5991  \n Residual               2.6794   1.6369  \nNumber of obs: 1590, groups:  ID, 199; Score_Type, 8\n\nFixed effects:\n                   Estimate Std. Error         df t value Pr(>|t|)    \n(Intercept)         4.87047    0.22144    7.76566  21.994 2.83e-08 ***\nclass_wins1_cent    0.07247    0.06693 1573.64949   1.083   0.2791    \nmedia_wins1_cent   -0.15383    0.06450 1326.49676  -2.385   0.0172 *  \nknow_cent           0.12180    0.05201 1299.59345   2.342   0.0193 *  \nneedforcog_cent    -0.20136    0.05050  196.56908  -3.987 9.42e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cls_1_ md_w1_ knw_cn\nclss_wns1_c  0.000                     \nmd_wns1_cnt  0.000 -0.359              \nknow_cent    0.000 -0.069 -0.313       \nnedfrcg_cnt  0.000 -0.022 -0.073  0.010"
  },
  {
    "objectID": "posts/Expertise6_Data_From_Zero_2022-01-16/index.html#winsorize-the-variables-at-2.5",
    "href": "posts/Expertise6_Data_From_Zero_2022-01-16/index.html#winsorize-the-variables-at-2.5",
    "title": "Expertise6 Data Analysis from the very beginning",
    "section": "Winsorize the variables at 2.5%",
    "text": "Winsorize the variables at 2.5%\n\n# winsorize the variables (at 2.5%)\nexpertise6_readyw25 <- expertise6_fact%>%\n  mutate(numarticle_wins_1=Winsorize(numarticle, probs = c(0,0.975)), \n         numbook_wins_1=Winsorize(numbook, probs = c(0,0.975)),\n         high_wins_1=Winsorize(high, probs = c(0,0.975)),\n         colle_wins_1=Winsorize(colle, probs = c(0,0.975)),\n         grad_wins_1=Winsorize(grad,na.rm=TRUE, probs = c(0,0.975)),\n         docu_wins_1=Winsorize(numdocu,na.rm=TRUE, probs = c(0,0.975)),\n         inarti_wins_1=Winsorize(numinarti,na.rm=TRUE, probs = c(0,0.975)),\n         radio_wins_1=Winsorize(numradio,na.rm=TRUE, probs = c(0,0.975)))\n\n#check descriptives\ndescriptives(dat=expertise6_readyw25, vars(docu_wins_1,inarti_wins_1, radio_wins_1 ), median=F, n=F, missing=T, sd=T, skew =T)\n\n\n DESCRIPTIVES\n\n Descriptives                                                            \n ─────────────────────────────────────────────────────────────────────── \n                          docu_wins_1    inarti_wins_1    radio_wins_1   \n ─────────────────────────────────────────────────────────────────────── \n   Missing                          0                0               0   \n   Mean                      1.908203         7.897135       0.6673177   \n   Standard deviation        4.235883         19.75722        1.995598   \n   Minimum                   0.000000         0.000000        0.000000   \n   Maximum                   20.00000         100.0000        10.00000   \n   Skewness                  3.115780         3.750920        3.538529   \n   Std. error skewness     0.06243908       0.06243908      0.06243908   \n ─────────────────────────────────────────────────────────────────────── \n\ndescriptives(dat=expertise6_readyw25, vars(high_wins_1, colle_wins_1), median=F, n=F, missing=T, sd=T)\n\n\n DESCRIPTIVES\n\n Descriptives                                          \n ───────────────────────────────────────────────────── \n                         high_wins_1    colle_wins_1   \n ───────────────────────────────────────────────────── \n   Missing                         0               0   \n   Mean                    0.3079427       0.5032552   \n   Standard deviation      0.8547027        1.128830   \n   Minimum                  0.000000        0.000000   \n   Maximum                  4.000000        5.000000   \n ───────────────────────────────────────────────────── \n\ndescriptives(dat=expertise6_readyw25, vars(numarticle_wins_1, numbook_wins_1, grad_wins_1), median=F, n=F, missing=T, sd=T)\n\n\n DESCRIPTIVES\n\n Descriptives                                                                 \n ──────────────────────────────────────────────────────────────────────────── \n                         numarticle_wins_1    numbook_wins_1    grad_wins_1   \n ──────────────────────────────────────────────────────────────────────────── \n   Missing                               0                 0              0   \n   Mean                           4.841797          1.345052     0.02929688   \n   Standard deviation             10.50945          3.036276      0.1686923   \n   Minimum                        0.000000          0.000000       0.000000   \n   Maximum                        50.00000          15.00000       1.000000   \n ──────────────────────────────────────────────────────────────────────────── \n\n\n\n# Standardize the Selected variables  ----\nvars_to_standardize <- c(\"know\", \"docu_wins_1\",\"inarti_wins_1\", \"radio_wins_1\",\"high_wins_1\", \"colle_wins_1\",\n                         \"numarticle_wins_1\", \"numbook_wins_1\", \"grad_wins_1\")\n\n\n\nexpertise6_readyw25<-expertise6_readyw25 %>% \n  group_by(Score_Type) %>% \n  mutate_at(vars(vars_to_standardize), scale)\n\n\nsum(is.na(expertise6_readyw25$grad_wins_1))\n\n[1] 384"
  },
  {
    "objectID": "posts/Expertise6_Data_From_Zero_2022-01-16/index.html#take-the-means-of-winsorized-variables-to-create-our-new-categories-media-and-class-1",
    "href": "posts/Expertise6_Data_From_Zero_2022-01-16/index.html#take-the-means-of-winsorized-variables-to-create-our-new-categories-media-and-class-1",
    "title": "Expertise6 Data Analysis from the very beginning",
    "section": "Take the means of winsorized variables to create our new categories: media and class",
    "text": "Take the means of winsorized variables to create our new categories: media and class\n\nexpertise6_readyw25 <- expertise6_readyw25 %>% replace_na(list(colle_wins_1 = NA, high_wins_1 = NA, grad_wins_1 = NA, docu_wins_1= NA,inarti_wins_1 = NA, radio_wins_1 = NA, numarticle_wins_1 = NA, numbook_wins_1 = NA))\n\n\nexpertise6_readyw25 <- expertise6_readyw25 %>% mutate(class_wins1 = mapply(function(x, y, z) {\n  ifelse(is.na(x), (y + z)/2, \n         ifelse(is.na(y), (x + z)/2, \n                ifelse(is.na(z), (x + y)/2, (x + y + z)/3)))}, \n  grad_wins_1, colle_wins_1, high_wins_1))\n\nexpertise6_readyw25 <- expertise6_readyw25 %>% mutate(media_wins1 = mapply(function(x, y, z, t, r) {\n  ifelse(is.na(x), (y + z + t + r)/4, \n         ifelse(is.na(y), (x + z + t + r)/4,\n                ifelse(is.na(z), (x + y + t + r)/4, \n                       ifelse(is.na(t),(x + y + z + r)/4, \n                              ifelse(is.na(r),(x + y + z + t)/4,\n                                     (x + y + z + t + r)/5)))))}, \n  docu_wins_1,inarti_wins_1, radio_wins_1, numbook_wins_1, numarticle_wins_1))"
  },
  {
    "objectID": "posts/Expertise6_Data_From_Zero_2022-01-16/index.html#center-the-variables-1",
    "href": "posts/Expertise6_Data_From_Zero_2022-01-16/index.html#center-the-variables-1",
    "title": "Expertise6 Data Analysis from the very beginning",
    "section": "Center the variables",
    "text": "Center the variables\n\nexpertise6_readyw25$know_cent <- scale(expertise6_readyw25$know, center = TRUE, scale = FALSE)\n\nexpertise6_readyw25$needforclo_cent <- scale(expertise6_readyw25$needforclo, center = TRUE, scale = FALSE)\n\nexpertise6_readyw25$class_wins1_cent <- scale(expertise6_readyw25$class_wins1, center = TRUE, scale = FALSE)\n\nexpertise6_readyw25$media_wins1_cent <- scale(expertise6_readyw25$media_wins1, center = TRUE, scale = FALSE)\n\n##Shapiro-Wilk values\n\ndescriptives(expertise6_readyw25, vars = vars(class_wins1, media_wins1, know, ih),n=FALSE, missing= FALSE, median=FALSE, sw = TRUE)\n\n\n DESCRIPTIVES\n\n Descriptives                                                                           \n ────────────────────────────────────────────────────────────────────────────────────── \n                         class_wins1      media_wins1      know            ih           \n ────────────────────────────────────────────────────────────────────────────────────── \n   Mean                  -1.966020e-17    -3.545341e-17    5.261995e-17      5.112305   \n   Standard deviation        0.7133831        0.7273781       0.9977173      2.142995   \n   Minimum                  -0.6311378       -0.5679514       -2.197199      1.000000   \n   Maximum                    5.997473         5.102448        3.050287      9.000000   \n   Shapiro-Wilk W            0.5950088        0.6332209       0.9721739     0.9707327   \n   Shapiro-Wilk p           < .0000001       < .0000001      < .0000001    < .0000001   \n ──────────────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/Expertise6_Data_From_Zero_2022-01-16/index.html#hlm-models-1",
    "href": "posts/Expertise6_Data_From_Zero_2022-01-16/index.html#hlm-models-1",
    "title": "Expertise6 Data Analysis from the very beginning",
    "section": "HLM Models",
    "text": "HLM Models\n\nModel of objective expertise, perceived expertise and need for cognition without interaction\n\nModel.1.1<-lmer(ih ~class_wins1_cent + media_wins1_cent + know_cent +needforclo_cent +(1|Score_Type)+(1|ID),   \n              data=expertise6_readyw25)\nsummary(Model.1.1)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: \nih ~ class_wins1_cent + media_wins1_cent + know_cent + needforclo_cent +  \n    (1 | Score_Type) + (1 | ID)\n   Data: expertise6_readyw25\n\nREML criterion at convergence: 6314\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.95570 -0.66262  0.01905  0.66183  2.53467 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.7398   0.8601  \n Score_Type (Intercept) 0.7757   0.8808  \n Residual               3.1141   1.7647  \nNumber of obs: 1528, groups:  ID, 191; Score_Type, 8\n\nFixed effects:\n                   Estimate Std. Error         df t value Pr(>|t|)    \n(Intercept)         5.10310    0.32075    7.55730  15.910 4.39e-07 ***\nclass_wins1_cent    0.02293    0.07879 1496.51118   0.291  0.77104    \nmedia_wins1_cent   -0.26334    0.08251 1499.89932  -3.192  0.00144 ** \nknow_cent           0.01554    0.06023 1401.19389   0.258  0.79639    \nneedforclo_cent     0.18163    0.11090  183.85178   1.638  0.10319    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cls_1_ md_w1_ knw_cn\nclss_wns1_c  0.000                     \nmd_wns1_cnt -0.001 -0.335              \nknow_cent    0.001 -0.235 -0.300       \nnedfrcl_cnt  0.000  0.001  0.004  0.042\n\n\n#Plot of the model 1.1\n\nModel.1.1 <- lmer(ih ~class_wins1_cent + media_wins1_cent + know_cent +needforclo_cent +(1|Score_Type)+(1|ID),   \n              data=expertise6_readyw25)\nsummary(Model.1.1)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: \nih ~ class_wins1_cent + media_wins1_cent + know_cent + needforclo_cent +  \n    (1 | Score_Type) + (1 | ID)\n   Data: expertise6_readyw25\n\nREML criterion at convergence: 6314\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.95570 -0.66262  0.01905  0.66183  2.53467 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.7398   0.8601  \n Score_Type (Intercept) 0.7757   0.8808  \n Residual               3.1141   1.7647  \nNumber of obs: 1528, groups:  ID, 191; Score_Type, 8\n\nFixed effects:\n                   Estimate Std. Error         df t value Pr(>|t|)    \n(Intercept)         5.10310    0.32075    7.55730  15.910 4.39e-07 ***\nclass_wins1_cent    0.02293    0.07879 1496.51118   0.291  0.77104    \nmedia_wins1_cent   -0.26334    0.08251 1499.89932  -3.192  0.00144 ** \nknow_cent           0.01554    0.06023 1401.19389   0.258  0.79639    \nneedforclo_cent     0.18163    0.11090  183.85178   1.638  0.10319    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cls_1_ md_w1_ knw_cn\nclss_wns1_c  0.000                     \nmd_wns1_cnt -0.001 -0.335              \nknow_cent    0.001 -0.235 -0.300       \nnedfrcl_cnt  0.000  0.001  0.004  0.042\n\n# save fitted data\nef.1.media <- effect(term = \"media_wins1_cent\",\n                                   mod = Model.1.1)\nef.1.media.data <- as.data.frame(ef.1.media) #convert the effects list to a data frame\nef.1.media.data #print effects data frame\n\n  media_wins1_cent      fit        se    lower    upper\n1             -0.6 5.261047 0.3245589 4.624418 5.897677\n2              0.8 4.892372 0.3274497 4.250072 5.534673\n3              2.0 4.576366 0.3606657 3.868912 5.283820\n4              4.0 4.049687 0.4601571 3.147079 4.952296\n5              5.0 3.786348 0.5224948 2.761463 4.811234\n\nexpert_media_graph <- expertise6_readyw25 %>%\n  group_by(ID) %>%\n  summarise(ih = mean(ih),\n            media_wins1_cent = media_wins1)\nexpert_media_graph <- as.data.frame(expert_media_graph)\n\nexpert_media_graph$media_wins1_cent<-expert_media_graph$media_wins1_cent-min(expert_media_graph$media_wins1_cent)\n\n\nfig.1.expert6_w1 <- ggplot(data = ef.1.media.data, aes(x = media_wins1_cent,\n                                                  y = fit)) +\n  geom_point(data=expert_media_graph, aes(x=media_wins1_cent,y=ih),\n             show.legend = FALSE, pch=21, color=\"blueviolet\", alpha=.25, size=3) +\n  geom_line(aes(x=media_wins1_cent), color = \"darkred\", size = 1.2)  +\n  geom_ribbon(aes(ymin = fit-se, ymax = fit+se), alpha = 0.50, fill = \"gray70\") +\n  labs(title = \"Expertise and Inherence Bias\", x = \"Expertise (books,articles)\",\n       y = \"Inherence bias\") +\n  scale_x_continuous(limits = c(0, 6), breaks = seq(0, 6, 2), expand = c(0, 0)) +\n  scale_y_continuous(limits = c(0, 8), breaks = seq(0, 8, 2), expand = c(0, 0)) +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n        axis.title = element_text(size = 14, face = \"bold\"),\n        axis.text = element_text(size = 12),\n        legend.title = element_blank(),\n        panel.grid = element_blank(),\n        panel.border = element_blank(),\n        plot.background=element_rect(fill = \"gray96\"),\n        panel.background = element_rect(fill = \"gray96\"))\nfig.1.expert6_w1\n\n\n\nggsave(paste0(\"figure_expertise6+media_w25\",\n              format(Sys.time(), \"%Y-%m-%d\")\n              ,\".eps\"), width = 20, height = 20, units = \"cm\") ###Save the Figure\n\n#Sqrt variables\n##Square root of the raw variables for our new expertise variables\n\n# take the square root of the variables\nexpertise6_sqrt <- expertise6_fact%>%\n  mutate(numarticle_sqrt=sqrt(numarticle),\n         numbook_sqrt=sqrt(numbook),\n         high_sqrt=sqrt(high),\n         colle_sqrt=sqrt(colle),\n         grad_sqrt=sqrt(grad),\n         docu_sqrt=sqrt(numdocu),\n         inarti_sqrt=sqrt(numinarti),\n         radio_sqrt=sqrt(numradio))\n\n\n# Standardize the Selected variables  ----\nvars_to_standardize <- c(\"know\", \"docu_sqrt\",\"inarti_sqrt\", \"radio_sqrt\",\"high_sqrt\", \"colle_sqrt\",\n                         \"numarticle_sqrt\", \"numbook_sqrt\", \"grad_sqrt\")\n\n\n\nexpertise6_sqrt<-expertise6_sqrt %>% \n  group_by(Score_Type) %>% \n  mutate_at(vars(vars_to_standardize), scale)"
  },
  {
    "objectID": "posts/Expertise6_Data_From_Zero_2022-01-16/index.html#take-the-means-of-winsorized-variables-to-create-our-new-categories-media-and-class-2",
    "href": "posts/Expertise6_Data_From_Zero_2022-01-16/index.html#take-the-means-of-winsorized-variables-to-create-our-new-categories-media-and-class-2",
    "title": "Expertise6 Data Analysis from the very beginning",
    "section": "Take the means of winsorized variables to create our new categories: media and class",
    "text": "Take the means of winsorized variables to create our new categories: media and class\n\nexpertise6_sqrt <- expertise6_sqrt %>% mutate(class_wins1 = mapply(function(x, y, z) {\n  ifelse(is.na(x), (y + z)/2, \n         ifelse(is.na(y), (x + z)/2, \n                ifelse(is.na(z), (x + y)/2, (x + y + z)/3)))}, \n  grad_sqrt, colle_sqrt, high_sqrt))\n\nexpertise6_sqrt <- expertise6_sqrt %>% mutate(media_wins1 = mapply(function(x, y, z, t, r) {\n  ifelse(is.na(x), (y + z + t + r)/4, \n         ifelse(is.na(y), (x + z + t + r)/4,\n                ifelse(is.na(z), (x + y + t + r)/4, \n                       ifelse(is.na(t),(x + y + z + r)/4, \n                              ifelse(is.na(r),(x + y + z + t)/4,\n                                     (x + y + z + t + r)/5)))))}, \n  docu_sqrt,inarti_sqrt, radio_sqrt, numbook_sqrt, numarticle_sqrt))"
  },
  {
    "objectID": "posts/Expertise6_Data_From_Zero_2022-01-16/index.html#center-the-variables-2",
    "href": "posts/Expertise6_Data_From_Zero_2022-01-16/index.html#center-the-variables-2",
    "title": "Expertise6 Data Analysis from the very beginning",
    "section": "Center the variables",
    "text": "Center the variables\n\nexpertise6_sqrt$know_cent <- scale(expertise6_sqrt$know, center = TRUE, scale = FALSE)\n\nexpertise6_sqrt$needforclo_cent <- scale(expertise6_sqrt$needforclo, center = TRUE, scale = FALSE)\n\nexpertise6_sqrt$class_wins1_cent <- scale(expertise6_sqrt$class_wins1, center = TRUE, scale = FALSE)\n\nexpertise6_sqrt$media_wins1_cent <- scale(expertise6_sqrt$media_wins1, center = TRUE, scale = FALSE)"
  },
  {
    "objectID": "posts/Expertise6_Data_From_Zero_2022-01-16/index.html#hlm-models-2",
    "href": "posts/Expertise6_Data_From_Zero_2022-01-16/index.html#hlm-models-2",
    "title": "Expertise6 Data Analysis from the very beginning",
    "section": "HLM Models",
    "text": "HLM Models\n\nModel of objective expertise, perceived expertise and need for cognition without interaction\n\nModel.1.1<-lmer(ih ~class_wins1_cent + media_wins1_cent + know_cent +needforclo_cent +(1|Score_Type)+(1|ID),   \n              data=expertise6_sqrt)\nsummary(Model.1.1)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: \nih ~ class_wins1_cent + media_wins1_cent + know_cent + needforclo_cent +  \n    (1 | Score_Type) + (1 | ID)\n   Data: expertise6_sqrt\n\nREML criterion at convergence: 6310.1\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.94094 -0.67254  0.00759  0.67196  2.53215 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.7285   0.8535  \n Score_Type (Intercept) 0.7760   0.8809  \n Residual               3.1093   1.7633  \nNumber of obs: 1528, groups:  ID, 191; Score_Type, 8\n\nFixed effects:\n                   Estimate Std. Error         df t value Pr(>|t|)    \n(Intercept)       5.103e+00  3.207e-01  7.548e+00  15.912 4.44e-07 ***\nclass_wins1_cent  1.329e-01  7.130e-02  1.496e+03   1.864 0.062523 .  \nmedia_wins1_cent -3.284e-01  8.534e-02  1.475e+03  -3.849 0.000124 ***\nknow_cent         8.013e-03  5.948e-02  1.392e+03   0.135 0.892865    \nneedforclo_cent   1.785e-01  1.104e-01  1.834e+02   1.617 0.107519    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cls_1_ md_w1_ knw_cn\nclss_wns1_c  0.000                     \nmd_wns1_cnt -0.001 -0.374              \nknow_cent    0.001 -0.157 -0.329       \nnedfrcl_cnt  0.000 -0.026  0.008  0.050\n\n\n#Cube root variables\n##Cube root of the raw variables for our new expertise variables\n\n# cube root of the variables \nexpertise6_cube <- expertise6_fact%>%\n  mutate(numarticle_cube=(numarticle^(1/3)),\n         numbook_cube=(numbook^(1/3)),\n         high_cube=(high^(1/3)),\n         colle_cube=(colle^(1/3)),\n         grad_cube=(grad^(1/3)),\n         docu_cube=(numdocu^(1/3)),\n         inarti_cube=(numinarti^(1/3)),\n         radio_cube=(numradio^(1/3)))\n\n\n# Standardize the Selected variables  ----\nvars_to_standardize <- c(\"know\", \"docu_cube\",\"inarti_cube\", \"radio_cube\",\"high_cube\", \"colle_cube\",\n                         \"numarticle_cube\", \"numbook_cube\", \"grad_cube\")\n\n\n\nexpertise6_cube<-expertise6_cube %>% \n  group_by(Score_Type) %>% \n  mutate_at(vars(vars_to_standardize), scale)"
  },
  {
    "objectID": "posts/Expertise6_Data_From_Zero_2022-01-16/index.html#take-the-means-of-winsorized-variables-to-create-our-new-categories-media-and-class-3",
    "href": "posts/Expertise6_Data_From_Zero_2022-01-16/index.html#take-the-means-of-winsorized-variables-to-create-our-new-categories-media-and-class-3",
    "title": "Expertise6 Data Analysis from the very beginning",
    "section": "Take the means of winsorized variables to create our new categories: media and class",
    "text": "Take the means of winsorized variables to create our new categories: media and class\n\nexpertise6_cube <- expertise6_cube %>% mutate(class_wins1 = mapply(function(x, y, z) {\n  ifelse(is.na(x), (y + z)/2, \n         ifelse(is.na(y), (x + z)/2, \n                ifelse(is.na(z), (x + y)/2, (x + y + z)/3)))}, \n  grad_cube, colle_cube, high_cube))\n\nexpertise6_cube <- expertise6_cube %>% mutate(media_wins1 = mapply(function(x, y, z, t, r) {\n  ifelse(is.na(x), (y + z + t + r)/4, \n         ifelse(is.na(y), (x + z + t + r)/4,\n                ifelse(is.na(z), (x + y + t + r)/4, \n                       ifelse(is.na(t),(x + y + z + r)/4, \n                              ifelse(is.na(r),(x + y + z + t)/4,\n                                     (x + y + z + t + r)/5)))))}, \n  docu_cube,inarti_cube, radio_cube, numbook_cube, numarticle_cube))\n\n##Histograms of square root variables\n\nhist(expertise6_sqrt$class_wins1)\n\n\n\n\n\nhist(expertise6_sqrt$media_wins1)\n\n\n\n\n\nhist(expertise6_sqrt$know)\n\n\n\n\n\nhist(expertise6_sqrt$ih)\n\n\n\n\n##Shapiro-Wilk values\n\ndescriptives(expertise6_sqrt, vars = vars(class_wins1, media_wins1, know, ih), n=FALSE, missing= FALSE, median=FALSE, sw = TRUE)\n\n\n DESCRIPTIVES\n\n Descriptives                                                                         \n ──────────────────────────────────────────────────────────────────────────────────── \n                         class_wins1     media_wins1     know            ih           \n ──────────────────────────────────────────────────────────────────────────────────── \n   Mean                  1.420305e-17    2.533419e-17    5.261995e-17      5.112305   \n   Standard deviation       0.7850757       0.7272822       0.9977173      2.142995   \n   Minimum                 -0.3681922      -0.6669302       -2.197199      1.000000   \n   Maximum                   12.85515        6.200443        3.050287      9.000000   \n   Shapiro-Wilk W           0.4392085       0.7072822       0.9721739     0.9707327   \n   Shapiro-Wilk p          < .0000001      < .0000001      < .0000001    < .0000001   \n ────────────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/Expertise6_Data_From_Zero_2022-01-16/index.html#center-the-variables-3",
    "href": "posts/Expertise6_Data_From_Zero_2022-01-16/index.html#center-the-variables-3",
    "title": "Expertise6 Data Analysis from the very beginning",
    "section": "Center the variables",
    "text": "Center the variables\n\nexpertise6_cube$know_cent <- scale(expertise6_cube$know, center = TRUE, scale = FALSE)\n\nexpertise6_cube$needforclo_cent <- scale(expertise6_cube$needforclo, center = TRUE, scale = FALSE)\n\nexpertise6_cube$class_wins1_cent <- scale(expertise6_cube$class_wins1, center = TRUE, scale = FALSE)\n\nexpertise6_cube$media_wins1_cent <- scale(expertise6_cube$media_wins1, center = TRUE, scale = FALSE)\n\n##Histograms of cube root variables\n\nhist(expertise6_cube$class_wins1)\n\n\n\n\n\nhist(expertise6_cube$media_wins1)\n\n\n\n\n\nhist(expertise6_cube$know)\n\n\n\n\n\nhist(expertise6_cube$ih)\n\n\n\n\n##Shapiro-Wilk values\n\ndescriptives(expertise6_cube, vars = vars(class_wins1, media_wins1, know, ih),n=FALSE, missing= FALSE, median=FALSE, sw = TRUE)\n\n\n DESCRIPTIVES\n\n Descriptives                                                                         \n ──────────────────────────────────────────────────────────────────────────────────── \n                         class_wins1     media_wins1     know            ih           \n ──────────────────────────────────────────────────────────────────────────────────── \n   Mean                  4.452457e-17    1.832663e-16    5.261995e-17      5.112305   \n   Standard deviation       0.7589839       0.7322954       0.9977173      2.142995   \n   Minimum                 -0.5470988      -0.8251761       -2.197199      1.000000   \n   Maximum                   10.38594        4.701118        3.050287      9.000000   \n   Shapiro-Wilk W           0.5526782       0.8353051       0.9721739     0.9707327   \n   Shapiro-Wilk p          < .0000001      < .0000001      < .0000001    < .0000001   \n ────────────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/Expertise6_Data_From_Zero_2022-01-16/index.html#hlm-models-3",
    "href": "posts/Expertise6_Data_From_Zero_2022-01-16/index.html#hlm-models-3",
    "title": "Expertise6 Data Analysis from the very beginning",
    "section": "HLM Models",
    "text": "HLM Models\n\nModel of objective expertise, perceived expertise and need for cognition without interaction\n\nModel.1.1<-lmer(ih ~class_wins1_cent + media_wins1_cent + know_cent +needforclo_cent +(1|Score_Type)+(1|ID),   \n              data=expertise6_cube)\nsummary(Model.1.1)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: \nih ~ class_wins1_cent + media_wins1_cent + know_cent + needforclo_cent +  \n    (1 | Score_Type) + (1 | ID)\n   Data: expertise6_cube\n\nREML criterion at convergence: 6308.2\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.93913 -0.66733  0.00914  0.65985  2.52937 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.7172   0.8469  \n Score_Type (Intercept) 0.7758   0.8808  \n Residual               3.1096   1.7634  \nNumber of obs: 1528, groups:  ID, 191; Score_Type, 8\n\nFixed effects:\n                   Estimate Std. Error         df t value Pr(>|t|)    \n(Intercept)         5.10324    0.32057    7.54005  15.919 4.48e-07 ***\nclass_wins1_cent    0.12867    0.07402 1503.91694   1.738   0.0824 .  \nmedia_wins1_cent   -0.35420    0.08629 1418.15202  -4.105 4.28e-05 ***\nknow_cent           0.02167    0.06034 1373.20152   0.359   0.7195    \nneedforclo_cent     0.17650    0.10979  182.95923   1.608   0.1096    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cls_1_ md_w1_ knw_cn\nclss_wns1_c  0.000                     \nmd_wns1_cnt  0.000 -0.358              \nknow_cent    0.001 -0.177 -0.346       \nnedfrcl_cnt  0.000 -0.022  0.014  0.046"
  },
  {
    "objectID": "posts/Grad Application Presentation/GradSchoolApplications2.html#overwiev",
    "href": "posts/Grad Application Presentation/GradSchoolApplications2.html#overwiev",
    "title": "Grad School Applications",
    "section": "Overwiev",
    "text": "Overwiev\n\nAims and offers of graduate programs in psychology\n\nResearch Specific Areas\nApplication Specific Areas\n\nExpectations of the programs\nBenefits of the students\nApplication process\n\n\nWe’re gonna talk about grad school applications in psychology. This date is not an optimal date for this maybe because senior students or masters students who are applying for the next year probably has been done with their applications and for junior students, it may be early, but getting familiar with the doctoral and masters programs in our areas is a good starting point to prepare your future, if you wanna stay this area. My preparation for this presentations is based on introducing these graduate programs in psychology, mentioning about the expectations, responsibilities of students and the benefits that the students get and also how can you start for application process with critical dates, critical documents, tips and resources."
  },
  {
    "objectID": "posts/Grad Application Presentation/GradSchoolApplications2.html#aims-reasons-and-expectations",
    "href": "posts/Grad Application Presentation/GradSchoolApplications2.html#aims-reasons-and-expectations",
    "title": "Grad School Applications",
    "section": "Aims, reasons and expectations",
    "text": "Aims, reasons and expectations\n\nThe general aim: Preparing students for careers in academia, research, or clinical practice.\nWhy do you apply?\nWhat can be the expectations in the programs?"
  },
  {
    "objectID": "posts/Grad Application Presentation/GradSchoolApplications2.html#research-specific-areas",
    "href": "posts/Grad Application Presentation/GradSchoolApplications2.html#research-specific-areas",
    "title": "Grad School Applications",
    "section": "Research Specific Areas",
    "text": "Research Specific Areas\n\nCoursework are based on conceptual knowledge, statistics and research methodology.\nResearch: Students are expected to conduct original research in psychology, typically resulting in a dissertation or thesis. This involves developing research questions, designing studies, collecting and analyzing data, and reporting findings.\nTeaching: PhD students usually have TA duties for graduate and undergraduate courses\n\n\n\nphd students have also teaching duties like being a teaching assistant for grad and undergrad courses. Beside, phd students may also have opportunities to teach undergraduate courses in psychology."
  },
  {
    "objectID": "posts/Grad Application Presentation/GradSchoolApplications2.html#application-focused-areas",
    "href": "posts/Grad Application Presentation/GradSchoolApplications2.html#application-focused-areas",
    "title": "Grad School Applications",
    "section": "Application Focused Areas",
    "text": "Application Focused Areas\n\nPsyD (Doctor of Psychology) program is a professional doctoral degree that typically emphasizes clinical training over research.\nThey involves less research and more coursework and clinical training than PhD programs.\nGraduates of PsyD programs are typically prepared for careers in clinical practice, such as working as licensed psychologists in private practice, hospitals, or other mental health settings."
  },
  {
    "objectID": "posts/Grad Application Presentation/GradSchoolApplications2.html#psyd-or-phd",
    "href": "posts/Grad Application Presentation/GradSchoolApplications2.html#psyd-or-phd",
    "title": "Grad School Applications",
    "section": "PsyD or PhD?",
    "text": "PsyD or PhD?"
  },
  {
    "objectID": "posts/Grad Application Presentation/GradSchoolApplications2.html#sub-areas-in-psychology",
    "href": "posts/Grad Application Presentation/GradSchoolApplications2.html#sub-areas-in-psychology",
    "title": "Grad School Applications",
    "section": "Sub areas in psychology",
    "text": "Sub areas in psychology\n\nDevelopmental\nSocial\nCognitive\nClinical\nNeurosicence\n\n\nMaybe we can talk about sub areas in psychology, these are not mutually exclusive areas as you know, but it may help us to decide what our main focus and what kind of tradition that its contain For example, developmental psych focused on social-emotional and cognitive development as a lifelong process, social psychology focused on the topics like attitudes, the self, self-regulation, social motivation, emotion, stereotyping and cognitive psychology focused on the topics like attention, memory, training and learning, emotion.\nClinical areas contain different topics like intervention and prevention methods across the life course for such phenomena as aggression and antisocial behavior, depression, personality disorders, eating disorders and obesity, substance abuse, HIVAIDS, cardiovascular disease, and chronic pain.\nFor neuroscience, the topics can be like social and motivated behavior; social and emotional interplay at the neural circuit and behavioral systems level, midbrain dopaminergic reward systems; cognitive development and aging; neural basis of vision and audition.\nThese areas of course can be combined or named differently at different universities, like cognitive and behavioral sciences or clinical neuroscience or developmental psychopathology, etc. Finding an appropriate program for yourself should be based on your interests, your skills, your resources, so you should decide an optimal way to find best programs for yourself."
  },
  {
    "objectID": "posts/Grad Application Presentation/GradSchoolApplications2.html#funding-opportunities",
    "href": "posts/Grad Application Presentation/GradSchoolApplications2.html#funding-opportunities",
    "title": "Grad School Applications",
    "section": "Funding Opportunities",
    "text": "Funding Opportunities\n\nScholarships/Fellowships by the programs\nStipends based on research grants"
  },
  {
    "objectID": "posts/Grad Application Presentation/GradSchoolApplications2.html#timeline",
    "href": "posts/Grad Application Presentation/GradSchoolApplications2.html#timeline",
    "title": "Grad School Applications",
    "section": "Timeline",
    "text": "Timeline\n\nNov 15/Dec 1: Deadline of applications in US/Canada\nJan/Feb: Interviews\nFeb/March: Offers\nApril 1/April 15: Deadline for decisions"
  },
  {
    "objectID": "posts/Grad Application Presentation/GradSchoolApplications2.html#personal-timeline-example-for-a-student",
    "href": "posts/Grad Application Presentation/GradSchoolApplications2.html#personal-timeline-example-for-a-student",
    "title": "Grad School Applications",
    "section": "Personal timeline example for a student",
    "text": "Personal timeline example for a student\n\nSeptember 15: Deciding to apply PhD programs\nSeptember/October: Consulting your own professors/supervisors and asking for reference letters\nSeptember/October/November: SoP, Personal history, CV\nOctober/November: Contact professors that you want to apply\nOctober/November: Standardized tests (GRE, Toefl, Ielts, etc.)\nNovember/December: Applications"
  },
  {
    "objectID": "posts/Grad Application Presentation/GradSchoolApplications2.html#exams-toeflielts-gre",
    "href": "posts/Grad Application Presentation/GradSchoolApplications2.html#exams-toeflielts-gre",
    "title": "Grad School Applications",
    "section": "Exams: Toefl/Ielts-Gre",
    "text": "Exams: Toefl/Ielts-Gre\n\nCheck the websiteArea-Specific Gre\n\n\n\n\nGre can be optional (if you’re international, toefl may be also), check the website!\n\n\n\n\n\n\n\n\n\n\n\nGre has two main components, quantitative and verbal parts. Also, there may be area specific gre requirements like psychology area test."
  },
  {
    "objectID": "posts/Grad Application Presentation/GradSchoolApplications2.html#searching-for-advisor-and-programs",
    "href": "posts/Grad Application Presentation/GradSchoolApplications2.html#searching-for-advisor-and-programs",
    "title": "Grad School Applications",
    "section": "Searching for advisor and programs",
    "text": "Searching for advisor and programs\n\nAfter deciding the area and the topics that you interested in, the period of lab search may start.\nWhat can be the sources? Your own professors, the articles that you read, supervisors/friends in PhD programs\nLooking for lab/department website"
  },
  {
    "objectID": "posts/Grad Application Presentation/GradSchoolApplications2.html#looking-for-the-lab-website",
    "href": "posts/Grad Application Presentation/GradSchoolApplications2.html#looking-for-the-lab-website",
    "title": "Grad School Applications",
    "section": "Looking for the lab website",
    "text": "Looking for the lab website"
  },
  {
    "objectID": "posts/Grad Application Presentation/GradSchoolApplications2.html#why-should-you-email-the-professors",
    "href": "posts/Grad Application Presentation/GradSchoolApplications2.html#why-should-you-email-the-professors",
    "title": "Grad School Applications",
    "section": "Why should you email the professors",
    "text": "Why should you email the professors\n\nTo learn availability in the lab\nTo be “on the radar”\nLearn useful information (like collaboration between faculty members)"
  },
  {
    "objectID": "posts/Grad Application Presentation/GradSchoolApplications2.html#how-to-email-the-faculty",
    "href": "posts/Grad Application Presentation/GradSchoolApplications2.html#how-to-email-the-faculty",
    "title": "Grad School Applications",
    "section": "How to email the faculty",
    "text": "How to email the faculty\n\nBrief and informative\nPersonal details (your current education status, the lab experience, interest,etc.)\nA well-prepared CV"
  },
  {
    "objectID": "posts/Grad Application Presentation/GradSchoolApplications2.html#statement-of-purpose",
    "href": "posts/Grad Application Presentation/GradSchoolApplications2.html#statement-of-purpose",
    "title": "Grad School Applications",
    "section": "Statement of Purpose",
    "text": "Statement of Purpose\n\nWhy you interest in this area\nYour experience\nThe reasons behind your focus on research and teaching\nYour future projections after grad school and plans about grad school\nThe contributions of the program and the advisor"
  },
  {
    "objectID": "posts/Grad Application Presentation/GradSchoolApplications2.html#application-tips",
    "href": "posts/Grad Application Presentation/GradSchoolApplications2.html#application-tips",
    "title": "Grad School Applications",
    "section": "Application tips",
    "text": "Application tips\n\nWhere can you follow academic opportunities?\n\nAcademic twitter/mastodon"
  },
  {
    "objectID": "posts/Grad Application Presentation/GradSchoolApplications2.html#section",
    "href": "posts/Grad Application Presentation/GradSchoolApplications2.html#section",
    "title": "Grad School Applications",
    "section": "",
    "text": "Lab websites, researchgate, findaphd.com, scholarshipdb.com\nMail lists of societies"
  },
  {
    "objectID": "posts/Grad Application Presentation/GradSchoolApplications2.html#application-tips-1",
    "href": "posts/Grad Application Presentation/GradSchoolApplications2.html#application-tips-1",
    "title": "Grad School Applications",
    "section": "Application tips",
    "text": "Application tips\n\nQuantifiable outputs\n\nPoster/conference presentations\nHonors thesis\nUndergraduate journals, school clubs\nAwards, gpa"
  },
  {
    "objectID": "posts/Grad Application Presentation/GradSchoolApplications2.html#section-1",
    "href": "posts/Grad Application Presentation/GradSchoolApplications2.html#section-1",
    "title": "Grad School Applications",
    "section": "",
    "text": "Research Experience\n\nWhat did you do?\nHow did you do?\nWhat did you learn?\nWhat was your contribution?\nResearch related skills (statistics, research methods)\nConceptual understanding\n\n\n\nA good advisor is aware of phd is a place to contribute to these skills, focus on specific research topics and clarify what you learn before, but it’s alway good to show your enthusiasm and openness to take feedback to contribute yourself"
  },
  {
    "objectID": "posts/Grad Application Presentation/GradSchoolApplications2.html#application-tips-2",
    "href": "posts/Grad Application Presentation/GradSchoolApplications2.html#application-tips-2",
    "title": "Grad School Applications",
    "section": "Application tips",
    "text": "Application tips\n\nHow many programs should someone apply?\nApplication fee waivers!"
  },
  {
    "objectID": "posts/Grad Application Presentation/GradSchoolApplications2.html#useful-links",
    "href": "posts/Grad Application Presentation/GradSchoolApplications2.html#useful-links",
    "title": "Grad School Applications",
    "section": "Useful Links",
    "text": "Useful Links\n\nWhy and how to email faculty prior to applying to graduate school\nCognitive Development Society Mail List\nNYU Application Resources"
  }
]