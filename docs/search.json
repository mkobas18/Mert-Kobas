[
  {
    "objectID": "posts/Thomas et al. 2013/index.html",
    "href": "posts/Thomas et al. 2013/index.html",
    "title": "Thomas et al. (2014) Memory Constraints on Hypothesis Generation and Decision Making",
    "section": "",
    "text": "The focus of this article is hypothesis generation and memory relation. In everyday life, we use hypothesis to produce explanations after we test it. The general idea here is how memory as a cognitive structure constrains hypothesis generation and related decision-making processes.\nThree primary processes can explain this idea:\n\nRetrieval (Long term memory, retrieving memory traces)\nMaintenance (Working memory, sustaining these traces in the active thinking process)\nJudgement (Decision after test these hypotheses)\n\n\n\n\nThomas et al. (2013), page 265, Figure 1\n\n\nOther important term here is hypothesis-guided search. I interpreted this as a top-down evidence looking stage between hypothesis generation and decision making. Medical diagnoses are examples for this.\nComment 1: However, the first hypothesis must be bottom-up or based on the testimony here. So, a medical doctor may see their patient with holding their arm, so the first hypothesis must be that the arm was broken. After this, X-ray, etc. can be used to prove this hypothesis and make a decision.\n\n\nThey mention how experts and other people generate only a small number of these hypotheses (probably due to WM restrictions, they mentioned in their other article: People with high WM produce 4 hypothesis in a time window, but low WM produce around 2.)\nMost likely hypotheses will be produced first (broken arm). New hypotheses are produced, if they better matches to the data (after X-ray screening, it may be only muscle ache).\nQuestion 1: They gave an example here about a mental illness: “Entertaining the hypothesis that the kitchen faucet is communicating with you when the dripping is perfectly consistent with its just being leaky”. Is this a failure of data-hypothesis match or interpreting data based on prior beliefs? (if this would be another context like believing spirituality) If you believe that non-living things can have spirit/can talk/etc., then you may find this more likely (more match with data hypothesis). So beyond data and hypothesis, do our beliefs play a role to distort our probability judgement process?\nAnother interesting point here is related to experience. Experts often recall specific hypothesis (broken bones) because those are activated more often. The article said that these hypotheses have more a priori likelihood. I interpreted this as these hypotheses were seen as more likely in the beginning. So if there are two hypotheses that come to my mind (broken arm vs. shoulder problem in the example of painful arm), I would prioritize broken arm because of my previous experiences.\nComment 2: Does it mean that experts produce some heuristics for explanations (holding arms mean broken bones without any alternatives -alternatives come after testing-)?\nOr expertise still provides more memory space (less cognitive load), so does it mean for more accurate hypothesis generation and testing process? (holding arms may mean broken bone because I see this many times, but alternative hypotheses can be possible but less likely -alternatives bring with the main hypothesis-)\nSo, do experts produce explanation both with inherent and extrinsic features (because they can use their cognitive resources flexibly to produce hypotheses with inherent and extrinsic features) just like the nonspeed condition group in Hussak & Cimpian (2018)? What if under time limitation and LTM restriction?\n\n\n\nThey introduce us here another term: Subadditivity. It means that the sum of the probabilities of sub-arguments in a hypothesis is more than the probability of hypothesis itself. The probabilities here reflect the evaluation of an individual. The probability of broken bones (25%), muscle problems (20%) and carpal tunnel (15%) > the probability of arm problem (50%)\n\n\n\nWith only one hypothesis, it becomes including confirmation search strategy. For example, If a person thought that somebody is married already, they search engagement ring of the other person.\nWith two or more hypothesis, it becomes diagnostic search. For example, if my computer cannot open any Youtube videos, I may think that it is because of my web browser or internet connection or Youtube servers, so I check my connection, I probably try another browser and search internet about servers, so I try to search diagnostically.\nIn the social cases, diagnostic search may be impossible sometimes? like if my partner is angry, I think that it maybe due to work or something that I say in the morning or an irrelevant thing, but checking all of these diagnostically is only possible to ask my partner, but this may make them more angry, so it is impossible?\n\n\n\nTime pressure led to generation of fewer hypotheses.\nQuestion 2a: So time pressure both leads to produce inherent biases in explanation and also less alternative explanations. Are those related somehow (people start with inherent explanations then they add extrinsic ones with alternative hypotheses), or separate processes (less alternative explanations + including mostly inherent features)?\nQuestion 2b: In the education system, we use time pressure to measure expertise (at least the assumption is this) like college entrance exams or language exams. People relate the mistakes in those exams usually with attentional issues (or didn’t have the knowledge) rather than memory problems. It reminds me Horne et al. (2019)’s article here. Time pressure may be a distractor due to related stress, so both hypothesis generation and generated explanations -including mostly inherent biases- were affected by attentional proccesses (focusing effects may become more effective).\nComment 3:Related to question 2b, they also mention here about primacy bias and recency bias. For the explanations, it may mean that people have difficulties the focus on middle part of the data (or an event). As an example, a person may be near the seaside and sees another person going to the sea. When the person after saw that person B as drowning, they explained it as person B doesn’t know swimming (inherent bias) in the first look and try to save them. The recent event is drowning here, however, it may be also waves of sea (extrinsic one). Since there is a time pressure (person b is dying), person A may not care the process of the event.\nAnother important point here is with slow presentation of data, people have recency bias but with fast rate of presentation, primacy bias emerged. Complexity of task or increased working memory capacity leads to increase in primacy bias during hypothesis generation.\nThe last point: People produce good hypotheses to explain the data, but they have also systematic biases (?? can inherent bias be an example?) in beliefs and information search due to memory limitations.\nQuestion 3 (bonus question, theoretical): Does it mean that cognitive biases in explanation generation process are the production of memory constrains? Or do they place in cognitive architecture -due to evoluationary processes, etc.-? Or both of them?"
  },
  {
    "objectID": "posts/Expertise_Data_From_Zero/index.html",
    "href": "posts/Expertise_Data_From_Zero/index.html",
    "title": "Expertise Data Analysis Trials From Zero",
    "section": "",
    "text": "This post includes the trial analyses of an example data related to expertise.\nImport necessary packages and expertise data\n\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(purrr)\n\nExpertise3 <- read_csv(\"~/Desktop/Expertise3_December 23, 2022_10.53.csv\")\n\nCreates a new dataframe called expertise3_clean, which is a copy of the original dataframe called Expertise3 and removes the second row of the dataframe\n\nexpertise3_clean<-Expertise3%>% \n  slice(-2) \n\nexpertise3_clean <- expertise3_clean %>%\n  select(-StartDate, -EndDate, -Status, -IPAddress, -Progress,-ResponseId,-RecordedDate,\n         -RecipientLastName, -RecipientFirstName,-RecipientEmail, -ExternalReference, \n         -LocationLatitude,-LocationLongitude, -DistributionChannel, -UserLanguage)\n\nCreate a variable called column_names and assign it the names of the columns in the dataframe and change the column names\n\ncolumn_names <- names(expertise3_clean)\ncolnames(expertise3_clean) <- c('duration', 'finished', 'stih_lang','r_ih_lang','stih_school','r_ih_school','stih_cards','r_ih_cards','stih_breakfast','r_ih_breakfast','stih_weddings','r_ih_weddings','stih_teeth','r_ih_teeth','stih_traffic','r_ih_traffic','stih_tv','r_ih_tv', 'know_lang','know_school','know_cards','know_breakfast','know_weddings','know_teeth','know_traffic','know_tv', 'course_lang','high_lang', 'colle_lang', 'grad_lang',\n'book_lang','numbook_lang','article_lang','numarticle_lang','course_school','high_school', 'colle_school', 'grad_school','book_school','numbook_school','article_school','numarticle_school','course_cards','high_cards', 'colle_cards', 'grad_cards',\n 'book_cards','numbook_cards','article_cards','numarticle_cards','course_breakfast','high_breakfast', 'colle_breakfast', 'grad_breakfast','book_breakfast','numbook_breakfast','article_breakfast','numarticle_breakfast','course_weddings','high_weddings', 'colle_weddings', 'grad_weddings','book_weddings','numbook_weddings','article_weddings','numarticle_weddings','course_teeth','high_teeth', 'colle_teeth', 'grad_teeth', 'book_teeth','numbook_teeth','article_teeth','numarticle_teeth','course_traffic','high_traffic', 'colle_traffic', 'grad_traffic','book_traffic','numbook_traffic','article_traffic','numarticle_traffic','course_tv','high_tv', 'colle_tv', 'grad_tv','book_tv','numbook_tv','article_tv','numarticle_tv','needforcog1','needforcog2','needforcog3','needforcog4','needforcog5','needforcog6','needforcog7','needforcog8','needforcog9','needforcog10','needforcog11','needforcog12','needforcog13','needforcog14','needforcog15','needforcog16','needforcog17','needforcog18','otherways','sex','birthdate','education','income','religion','identity','age','political_atti','english_level','proceure_confu','whatwestudied','moretothisstudy','additional_thoughts','attention')\n\nThe code below shows the survey items:\n\n#selects all columns except the ones specified\nrow_values <- expertise3_clean %>%\n  select(-duration,-finished)%>%\n  #selects only the first row\n  filter(row_number() == 1)\n\n#unlist the row_values\nrow_values <- unlist(row_values)\n\nmy_list <- map(row_values, ~paste0(.))\n\nlibrary(stringr)\nmy_list <- str_replace(my_list, \"(?<! )\\\\n(?! )\", \"\")\nmy_list <- str_replace(my_list, \"[^\\\\s]*\\\\\\\\n[^\\\\s]*\", \"\")\nlist_string <- paste0(\"* \", paste(my_list, collapse = \"\\n* \"))\n#Show the survey items\ncat(list_string)\n\n* Human languages are structured to best convey ourthoughts and feelings. Words and their meanings likely form ideal matches.\n* There are absolutely no good reasons whywe use specific words to represent our thoughts. Any combination of sounds\ncould in principle refer to any idea.\n* The fact that elementary school stops at 5th grade is probably ideal for children's learning. This is likely the best way to organize K-12 schooling.\n* Middle school (grades 6-8) is separate from elementary school (grades K-5) largely because of decisions made by educators a long time ago. This may not be the most optimal way of organizing early education.\n* It’s not a coincidence thatwe send people cards on holidays. This tradition seems\nparticularly fitting.\n* The fact that we send people cards on holidays is only a convention. A different way of sending warm wishes could've been implemented just as easily.\n* There are good reasons why orange juiceis typically consumed for breakfast. There are features about it that make it\nparticularly suited for this meal (for example, its refreshing taste).\n* The current popularity of orange juice for breakfast reflectsin part  marketing campaigns that promoted\ndrinking orange juice in the morning. However, had history taken a different\nturn, orange juice could just as easily have been more popular for lunch or\ndinner.\n* It seems right to use white for wedding dresses. Othercolors, such as red and blue, have features that make them less suited for\nwedding dresses.\n* Even though white is thetraditional color for wedding dresses, this could have easily been different.\nWhen you really think about it, there is no reason why other, brighter, colors couldn’t\nbe used for wedding dresses.\n* It seems ideal that toothpaste istypically flavored with mint. Mint is inherently more refreshing than any other\nflavor that currently exists.\n* When you think about it, toothpaste could have easily beenflavored with something other than mint, such as cinnamon. Many pleasing flavors would work just as\nwell.\n* Traffic lights, with threedifferent colored lights signaling three speeds, seem like the most efficient\nand effective way to direct traffic. Another process likely would not work as\nwell.\n* The current design of traffic lights,with three different colors reflecting three different speeds, is entirely due\nto historical factors. This is probably not the most efficient or effective way\nto manage traffic.\n* Black seems like a good choice for the color of televisions. Other colors just would not work as well.\n* Theonly reason why most TVs are black is\nhistorical happenstance. TVs could practically be a\nvariety of colors.\n* How much do you know about language and linguistics? - Please use the slider to select your answer choice.\n* How much do you know about school and education systems? - Please use the slider to select your answer choice.\n* How much do you know about holiday customs and traditions? - Please use the slider to select your answer choice.\n* How much do you know about breakfast foods? - Please use the slider to select your answer choice.\n* How much do you know about weddings and wedding traditions? - Please use the slider to select your answer choice.\n* How much do you know about teeth and oral hygiene? - Please use the slider to select your answer choice.\n* How much do you know about transportation science and traffic signal systems? - Please use the slider to select your answer choice.\n* How much do you know about the manufacturing of consumer electronics (TVs, MP3 players, camcorders, etc.)? - Please use the slider to select your answer choice.\n* Have you ever taken a class that discussed language and linguistics?\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - High School\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - College\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - Graduate School\n* Have you ever read any books on language and linguistics?\n* If you answered \"yes\" above, please approximate the number of books you have read on this topic:\n* Have you ever read any magazine, newspaper, or academic articles on language and linguistics?\n* If you answered \"yes\" above, please approximate the number of articles you have read on this topic:\n* Have you ever taken a class that discussed school and education systems?\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - High School\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - College\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - Graduate School\n* Have you ever read any books on school and education systems?\n* If you answered \"yes\" above, please approximate the number of books you have read on this topic:\n* Have you ever read any magazine, newspaper, or academic articles on school and education systems?\n* If you answered \"yes\" above, please approximate the number of articles you have read on this topic:\n* Have you ever taken a class that discussed holiday customs and traditions?\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - High School\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - College\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - Graduate School\n* Have you ever read any books on holiday customs and traditions?\n* If you answered \"yes\" above, please approximate the number of books you have read on this topic:\n* Have you ever read any magazine, newspaper, or academic articles on holiday customs and traditions?\n* If you answered \"yes\" above, please approximate the number of articles you have read on this topic:\n* Have you ever taken a class that discussed breakfast foods?\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - High School\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - College\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - Graduate School\n* Have you ever read any books on breakfast foods?\n* If you answered \"yes\" above, please approximate the number of books you have read on this topic:\n* Have you ever read any magazine, newspaper, or academic articles on breakfast foods?\n* If you answered \"yes\" above, please approximate the number of articles you have read on this topic:\n* Have you ever taken a class that discussed weddings and wedding traditions?\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - High School\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - College\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - Graduate School\n* Have you ever read any books on weddings and wedding traditions?\n* If you answered \"yes\" above, please approximate the number of books you have read on this topic:\n* Have you ever read any magazine, newspaper, or academic articles on weddings and wedding traditions?\n* If you answered \"yes\" above, please approximate the number of articles you have read on this topic:\n* Have you ever taken a class that discussed teeth and oral hygiene?\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - High School\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - College\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - Graduate School\n* Have you ever read any books on teeth and oral hygiene?\n* If you answered \"yes\" above, please approximate the number of books you have read on this topic:\n* Have you ever read any magazine, newspaper, or academic articles on teeth and oral hygiene?\n* If you answered \"yes\" above, please approximate the number of articles you have read on this topic:\n* Have you ever taken a class that discussed transportation science and traffic signal systems?\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - High School\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - College\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - Graduate School\n* Have you ever read any books on transportation science and traffic signal systems?\n* If you answered \"yes\" above, please approximate the number of books you have read on this topic:\n* Have you ever read any magazine, newspaper, or academic articles on transportation science and traffic signal systems?\n* If you answered \"yes\" above, please approximate the number of articles you have read on this topic:\n* Have you ever taken a class that discussed the manufacturing of consumer electronics (TVs, MP3 players, camcorders, etc.)?\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - High School\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - College\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - Graduate School\n* Have you ever read any books on the manufacturing of consumer electronics (TVs, MP3 players, camcorders, etc.)?\n* If you answered \"yes\" above, please approximate the number of books you have read on this topic:\n* Have you ever read any magazine, newspaper, or academic articles on the manufacturing of consumer electronics (TVs, MP3 players, camcorders, etc.)?\n* If you answered \"yes\" above, please approximate the number of articles you have read on this topic:\n* I would prefer complex to simple problems.\n* I like to have the responsibility of handling a situation that requires a lot of thinking.\n* Thinking is not my idea of fun.\n* I would rather do something that requires little thought than something that is sure to challenge my thinking abilities.\n* I try to anticipate and avoid situations where there is a likely chance I will have to think in depth about something.\n* I find satisfaction in deliberating hard and long for hours.\n* I only think as hard as I have to.\n* I prefer to think about small, daily projects rather than long-term ones.\n* I like tasks that require little thought once I’ve learned them.\n* The idea of relying on thought to make my way to the top appeals to me.\n* I really enjoy a task that involves coming up with new solutions to problems.\n* Learning new ways to think doesn’t excite me very much.\n* I prefer my life to be filled with puzzles that I must solve.\n* The notion of thinking abstractly appeals to me.\n* I would prefer a task that is intellectual, difficult, and important to one that is somewhat important but does not require much thought.\n* I feel relief rather than satisfaction after completing a task that required a lot of mental effort.\n* It’s enough for me that something gets the job done; I don’t care about why or how it works.\n* I usually end up deliberating about issues even when they do not affect me personally.\n* In this survey you were asked about your knowledge in a variety of subject areas. \nAside from the ways you may have learned about these topics that were mentioned in the survey (i.e., reading or  a college class), how else may you have learned about these topics? Please name other ways that you have learned about the topics mentioned in the survey you just took.\n* Are you male or female?\n* Q60 - What is your date of birth? (mm/dd/yyyy)\n* What is the highest level of education you have completed?\n* Q64 - What is your yearly household income?\n* Q64 - What is your religious affiliation?\n* Q64 - What is your racial or ethnic identity?\n* Q64 - What is your age in years?\n* How would you describe your political attitudes? Please select one of the points on the scale below.\n* Please rate your overall ability in the English language:\n* 1. Did you find any aspect of the procedure odd or confusing?\n* 2. What did you think we were studying?\n* 3. Do you think that there may have been more to this study than meets the eye? If so, what do you think this might have been?\n* 4. Do you have any additional thoughts or comments about the study?\n* Thank you for completing this survey! We just have one last question for you. You will not be penalized for your answer to this question. Since you completed the whole survey, you will receive payment no matter what answer you give here.\n     \n\n    It's very important to the quality and scientific aims of our study that participants pay attention (i.e., read the survey carefully, consider the response options, and avoid distractions).\n\n    \n\n    Were you paying attention while completing this survey?\n\n\nData preparation for further analyses\n\n#Attention check and deletion of cases that didn't attend or finish the study\nexpertise3_new<-expertise3_clean%>%\n  filter(attention==1&finished==1)\n\n#Exclude the variables that will not be used in the later analyses\nexpertise3_new<-expertise3_new%>%\n  select(-finished,-otherways,-birthdate,-proceure_confu,-whatwestudied,-moretothisstudy,-additional_thoughts,-attention)\n\n#adds a column to the dataframe, with the name \"id\"\nexpertise3_new<-cbind(ID = 1:nrow(expertise3_new), expertise3_new)\n\n# change the data type of the variables to numeric\nexpertise3_new <- expertise3_new %>%\n  mutate_at(vars(stih_lang, r_ih_lang, stih_school, r_ih_school, stih_cards, r_ih_cards, stih_breakfast, r_ih_breakfast, \n                 stih_weddings, r_ih_weddings, stih_teeth, r_ih_teeth, stih_traffic, r_ih_traffic, stih_tv, r_ih_tv), as.numeric)\n\n# Create a list of variable names\nvariables <- c(\"stih_lang\", \"r_ih_lang\", \"stih_school\", \"r_ih_school\", \"stih_cards\", \"r_ih_cards\", \"stih_breakfast\", \"r_ih_breakfast\", \"stih_weddings\", \"r_ih_weddings\", \"stih_teeth\", \"r_ih_teeth\", \"stih_traffic\", \"r_ih_traffic\", \"stih_tv\", \"r_ih_tv\")\n\nCheck the correlations between inherence (the variables starting with st) and reverse inherence (the variables starting with r) scores to check whether it’s appropriate for averaging\n\n# Initialize an empty data frame to store the correlation coefficients\ncorrelations <- data.frame(variable1 = character(), variable2 = character(), correlation = numeric(), p.value = numeric(), conf.int = character())\n\n# Iterate over the pairs of variables\nfor (i in seq(1, length(variables), 2)) {\n  j <- i + 1\n  \n  # Calculate the Pearson correlation coefficient and test the statistical significance\n  correlation_test <- cor.test(expertise3_new[, variables[i]], expertise3_new[, variables[j]], method = \"pearson\")\n  \n  # Add the correlation coefficient, p-value, and confidence interval to the data frame\n  correlations <- rbind(correlations, data.frame(variable1 = variables[i], variable2 = variables[j], correlation = correlation_test$estimate, p.value = correlation_test$p.value, conf.int = paste(correlation_test$conf.int[1], correlation_test$conf.int[2], sep = \" - \")))\n}\n\n# View the correlation coefficients and statistical measures\ncorrelations\n\n          variable1      variable2 correlation      p.value\ncor       stih_lang      r_ih_lang  -0.6068527 6.542313e-22\ncor1    stih_school    r_ih_school  -0.5223201 1.125194e-15\ncor2     stih_cards     r_ih_cards  -0.5433698 4.572854e-17\ncor3 stih_breakfast r_ih_breakfast  -0.3592235 1.317523e-07\ncor4  stih_weddings  r_ih_weddings  -0.5284362 4.539085e-16\ncor5     stih_teeth     r_ih_teeth  -0.4440395 3.226080e-11\ncor6   stih_traffic   r_ih_traffic  -0.4869944 1.514193e-13\ncor7        stih_tv        r_ih_tv  -0.3471023 3.653644e-07\n                                    conf.int\ncor  -0.686956408754957 - -0.512178848971074\ncor1  -0.61552619526787 - -0.414704679541722\ncor2 -0.633457573965096 - -0.438748248188404\ncor3 -0.473241603677093 - -0.233368287331007\ncor4 -0.620746208212983 - -0.421675200702786\ncor5 -0.548226097515688 - -0.326281347959786\ncor6  -0.58521523210812 - -0.374689532050954\ncor7 -0.462424268063511 - -0.220232239305416\n\n\nIt seems that each pairs have negative significant correlation, so we can take the average scores to calculate inherence scores\n\nexpertise3_new <- expertise3_new %>%\n  mutate(ih_lang = (stih_lang + (10 - r_ih_lang)) / 2,\n    ih_school = (stih_school + (10 - r_ih_school)) / 2,\n    ih_cards = (stih_cards + (10 - r_ih_cards)) / 2,\n    ih_breakfast = (stih_breakfast + (10 - r_ih_breakfast)) / 2,\n    ih_weddings = (stih_weddings + (10 - r_ih_weddings)) / 2,\n    ih_teeth = (stih_teeth + (10 - r_ih_teeth)) / 2,\n    ih_traffic = (stih_traffic + (10-r_ih_traffic)) / 2,\n    ih_tv = (stih_tv + (10-r_ih_tv)) / 2 )\n\nCalculate “Need for cognition” scale scores\n\n# change the data type of the variables to numeric\nexpertise3_new <- expertise3_new %>%\n  mutate_at(vars(needforcog1,needforcog2,needforcog3,needforcog4,needforcog5,needforcog6\n                 ,needforcog7,needforcog8,needforcog9,needforcog10,needforcog11,needforcog12,\n                   needforcog13,needforcog14,needforcog15,needforcog16,needforcog17,needforcog18), as.numeric)\n\n#add a new variable called needforcog, which is the sum of all the need for cognition items, the items are summed or extracted according to the normal or reverse items\nexpertise3_new <- expertise3_new %>%\n  mutate(needforcog=(needforcog1+needforcog2-needforcog3-needforcog4-needforcog5+needforcog6\n         -needforcog7-needforcog8-needforcog9+needforcog10+needforcog11-needforcog12+\n         needforcog13+needforcog14+needforcog15+needforcog16-needforcog17+needforcog18))\n\nPrepare the expertise scores and other scores ready for analyses\n\n# Replace all NA values in the expertise columns with 0\nexpertise3_new <- apply(expertise3_new, 2, function(x) ifelse(is.na(x), 0, x))\n# Turn the output to a dataframe\nexpertise3_new <- data.frame(expertise3_new)\n\n# Create new data frame as analyzable \nexpertise3_new<-expertise3_new%>%\n  select(-stih_lang,-r_ih_lang,-stih_school,-r_ih_school,\n         -stih_cards,-r_ih_cards,-stih_breakfast,-r_ih_breakfast,-stih_weddings,-r_ih_weddings,\n         -stih_teeth,-r_ih_teeth,-stih_traffic,-r_ih_traffic,-stih_tv,-r_ih_tv,\n         -course_lang,\n         -book_lang,-article_lang,\n         -course_school,\n         -book_school,-article_school,\n         -course_cards,\n         -book_cards,-article_cards,\n         -course_breakfast,\n         -book_breakfast,-article_breakfast,\n         -course_weddings,\n         -book_weddings,-article_weddings,\n         -course_teeth,\n         -book_teeth,-article_teeth,\n         -course_traffic,\n         -book_traffic,-article_traffic,\n         -course_tv,\n         -book_tv,-article_tv,\n         -needforcog1,-needforcog2,-needforcog3,-needforcog4,-needforcog5,-needforcog6,\n         -needforcog7,-needforcog8,-needforcog9,-needforcog10,-needforcog11,-needforcog12,\n         -needforcog13,-needforcog14,-needforcog15,-needforcog16,-needforcog17,-needforcog18)\n\n\n\n# change the data type of the variables to numeric\nexpertise3_new <- expertise3_new %>%\n  mutate_at(vars('know_lang','know_school','know_cards','know_breakfast',\n                 'know_weddings','know_teeth','know_traffic','know_tv',\n                 'high_lang', 'colle_lang', 'grad_lang',\n                 'numbook_lang','numarticle_lang',\n                 'high_school', 'colle_school', 'grad_school',\n                 'numbook_school','numarticle_school',\n                 'high_cards', 'colle_cards', 'grad_cards',\n                 'numbook_cards','numarticle_cards',\n                 'high_breakfast', 'colle_breakfast', 'grad_breakfast',\n                 'numbook_breakfast','numarticle_breakfast',\n                 'high_weddings', 'colle_weddings', 'grad_weddings',\n                 'numbook_weddings','numarticle_weddings',\n                 'high_teeth', 'colle_teeth', 'grad_teeth',\n                 'numbook_teeth','numarticle_teeth',\n                 'high_traffic', 'colle_traffic', 'grad_traffic',\n                 'numbook_traffic','numarticle_traffic',\n                 'high_tv', 'colle_tv', 'grad_tv',\n                 'numbook_tv','numarticle_tv'), as.numeric)\n\n# Select the variables to standardize\nvars_to_standardize <- c('know_lang','know_school','know_cards','know_breakfast',\n                         'know_weddings','know_teeth','know_traffic','know_tv',\n                         'high_lang', 'colle_lang', 'grad_lang',\n                         'numbook_lang','numarticle_lang',\n                         'high_school', 'colle_school', 'grad_school',\n                         'numbook_school','numarticle_school',\n                         'high_cards', 'colle_cards', 'grad_cards',\n                         'numbook_cards','numarticle_cards',\n                         'high_breakfast', 'colle_breakfast', 'grad_breakfast',\n                         'numbook_breakfast','numarticle_breakfast',\n                         'high_weddings', 'colle_weddings', 'grad_weddings',\n                         'numbook_weddings','numarticle_weddings',\n                         'high_teeth', 'colle_teeth', 'grad_teeth',\n                         'numbook_teeth','numarticle_teeth',\n                         'high_traffic', 'colle_traffic', 'grad_traffic',\n                         'numbook_traffic','numarticle_traffic',\n                         'high_tv', 'colle_tv', 'grad_tv',\n                         'numbook_tv','numarticle_tv')\n\n# Standardize the selected variables\nexpertise3_new[, vars_to_standardize] <- scale(expertise3_new[, vars_to_standardize])\n\n# Now we are ready for analysis\n\nhead(expertise3_new)\n\n   ID duration   know_lang know_school know_cards know_breakfast know_weddings\n1   1      403 -0.92933457 -0.09219094 -1.1258315      0.7943452    -0.1799059\n2   2      473  1.39400185 -0.09219094 -0.7512429     -0.6065023    -0.2230326\n3   3      442 -1.60521425 -2.05124849 -1.8750087     -2.2332930    -1.4305799\n4   4      337  0.04224248 -0.13572556 -0.1893600     -0.6065023     0.2513609\n5   5      376  0.38018232  0.82203591  0.1852286      0.7039680    -0.9130596\n6   6      618  0.04224248 -0.13572556  1.1685236      1.0202884     1.8039217\n  know_teeth know_traffic    know_tv  high_lang colle_lang  grad_lang\n1 -1.5832390   0.40296860  0.1980935 -0.6364644 -0.5394097 -0.1521967\n2 -0.2551931  -0.01538048  1.3258444 -0.6364644 -0.5394097 -0.1521967\n3 -2.2029937  -1.06125318 -0.8168823 -0.6364644 -0.5394097 -0.1521967\n4  0.6744390   0.36113369 -0.1778235 -0.6364644 -0.5394097 -0.1521967\n5  1.1613891  -0.01538048  0.8371523 -0.1776703 -0.5394097 -0.1521967\n6 -0.7864115  -0.72657392  1.9273115 -0.6364644 -0.5394097 -0.1521967\n  numbook_lang numarticle_lang high_school colle_school grad_school\n1  -0.49509859      -0.2633927  -0.2455616   -0.3847762  -0.1141508\n2  -0.49509859      -0.2633927  -0.2455616   -0.3847762  -0.1141508\n3  -0.49509859      -0.2633927  -0.2455616   -0.3847762  -0.1141508\n4  -0.49509859      -0.2633927  -0.2455616   -0.3847762  -0.1141508\n5  -0.07774275      -0.2633927  -0.2455616   -0.3847762  -0.1141508\n6  -0.49509859      -0.2633927  -0.2455616   -0.3847762  -0.1141508\n  numbook_school numarticle_school high_cards colle_cards grad_cards\n1     -0.3351031        -0.2475552 -0.2950077   -0.175091 -0.1028024\n2     -0.3351031        -0.2475552 -0.2950077   -0.175091 -0.1028024\n3     -0.3351031        -0.2475552 -0.2950077   -0.175091 -0.1028024\n4     -0.3351031        -0.2475552 -0.2950077   -0.175091 -0.1028024\n5     -0.3351031        -0.2475552 -0.2950077   -0.175091 -0.1028024\n6     -0.3351031         9.3827397 -0.2950077   -0.175091 -0.1028024\n  numbook_cards numarticle_cards high_breakfast colle_breakfast grad_breakfast\n1    -0.3549311       -0.3570679      0.4510718      -0.2243077     -0.1218696\n2    -0.3549311       -0.3570679     -0.2910141      -0.2243077     -0.1218696\n3    -0.3549311       -0.3570679     -0.2910141      -0.2243077     -0.1218696\n4     0.6479213       -0.1076095     -0.2910141      -0.2243077     -0.1218696\n5    -0.3549311       -0.3570679     -0.2910141      -0.2243077     -0.1218696\n6    -0.3549311       -0.3570679     -0.2910141      -0.2243077     -0.1218696\n  numbook_breakfast numarticle_breakfast high_weddings colle_weddings\n1         -0.235773           -0.2393171    -0.1795269     -0.2260702\n2         -0.235773           -0.2393171    -0.1795269     -0.2260702\n3         -0.235773           -0.2393171    -0.1795269     -0.2260702\n4         -0.235773           -0.2393171    -0.1795269     -0.2260702\n5         -0.235773           -0.2393171    -0.1795269     -0.2260702\n6         -0.235773           -0.2393171    -0.1795269     -0.2260702\n  grad_weddings numbook_weddings numarticle_weddings high_teeth colle_teeth\n1   -0.09411928       -0.3825866          -0.4102006 -0.3312423  -0.2490223\n2   -0.09411928       -0.3825866          -0.4102006 -0.3312423  -0.2490223\n3   -0.09411928       -0.3825866          -0.4102006 -0.3312423  -0.2490223\n4   -0.09411928       -0.3825866          -0.4102006 -0.3312423  -0.2490223\n5   -0.09411928       -0.3825866          -0.4102006 -0.3312423  -0.2490223\n6   -0.09411928       -0.3825866          -0.4102006 -0.3312423  -0.2490223\n   grad_teeth numbook_teeth numarticle_teeth high_traffic colle_traffic\n1 -0.09925954    -0.2779491      -0.16517115   -0.2045128    -0.1362734\n2 -0.09925954    -0.2779491      -0.16517115   -0.2045128    -0.1362734\n3 -0.09925954    -0.2779491      -0.16517115   -0.2045128    -0.1362734\n4 -0.09925954     1.3192794      -0.05229037   -0.2045128    -0.1362734\n5 -0.09925954    -0.2779491      -0.16517115   -0.2045128    -0.1362734\n6 -0.09925954    -0.2779491      -0.16517115   -0.2045128    -0.1362734\n  grad_traffic numbook_traffic numarticle_traffic    high_tv   colle_tv grad_tv\n1    -0.070014       -0.254263         -0.1112561 -0.1611468 -0.1414978     NaN\n2    -0.070014       -0.254263         -0.1112561 -0.1611468 -0.1414978     NaN\n3    -0.070014       -0.254263         -0.1112561 -0.1611468 -0.1414978     NaN\n4    -0.070014       -0.254263         -0.1112561 -0.1611468 -0.1414978     NaN\n5    -0.070014       -0.254263         -0.1112561 -0.1611468 -0.1414978     NaN\n6    -0.070014       -0.254263         -0.1112561 -0.1611468 -0.1414978     NaN\n  numbook_tv numarticle_tv sex education income religion identity age\n1 -0.2473571    -0.1178382   1         3 105000     none    white  26\n2 -0.2473571    -0.1178382   1         5  53000  atheist    white  38\n3 -0.2473571    -0.1178382   2         2 15.000     none    white  22\n4 -0.2473571    -0.1178382   2         4 150000   jewish    white  55\n5 -0.2473571    -0.1178382   1         3  45000     None    White  29\n6 -0.2473571    -0.1178382   1         3  60000 catholic    white  58\n  political_atti english_level ih_lang ih_school ih_cards ih_breakfast\n1              4             4     4.0       4.0      2.5          4.5\n2              2             4     5.0       5.0      5.0          5.0\n3              5             4     6.0       4.5      4.0          5.5\n4              5             4     5.0       2.5      2.0          3.0\n5              2             4     4.5       4.0      3.0          6.0\n6              4             4     5.5       3.5      5.0          3.5\n  ih_weddings ih_teeth ih_traffic ih_tv needforcog\n1         3.5      8.0        8.5   9.0         43\n2         5.0      5.0        5.0   5.0         11\n3         5.5      6.5        6.0   4.5          7\n4         1.0      1.0        5.0   1.0         17\n5         3.5      5.5        5.0   4.0         47\n6         3.5      5.5        4.5   4.0         10\n\n\nTransform the data to the long format for analyses:\n\nlibrary('reshape2')\n\nexpertise3_long <- melt(expertise3_new, id.vars = c(\"ID\",'duration',\"sex\" ,\"education\" ,\"income\",\"religion\",'identity','age','political_atti','english_level','needforcog'),\n          measure.vars = c(\"ih_lang\", \"ih_school\",\"ih_cards\", \"ih_breakfast\", \"ih_weddings\", \n\"ih_teeth\", \"ih_traffic\", \"ih_tv\",'know_lang','know_school','know_cards','know_breakfast',\n'know_weddings','know_teeth','know_traffic','know_tv','high_lang', 'colle_lang', 'grad_lang',\n'numbook_lang','numarticle_lang','high_school', 'colle_school', 'grad_school',\n'numbook_school','numarticle_school','high_cards', 'colle_cards', 'grad_cards',\n'numbook_cards','numarticle_cards', 'high_breakfast', 'colle_breakfast', 'grad_breakfast',\n'numbook_breakfast','numarticle_breakfast','high_weddings', 'colle_weddings', 'grad_weddings','numbook_weddings','numarticle_weddings','high_teeth', 'colle_teeth', 'grad_teeth',\n'numbook_teeth','numarticle_teeth','high_traffic', 'colle_traffic', 'grad_traffic',\n'numbook_traffic','numarticle_traffic','high_tv', 'colle_tv', 'grad_tv','numbook_tv','numarticle_tv'),sep = \"_\", variable.name = \"Category\", value.name = \"Score\")\n\n# Split the Category column into two columns based on the underscore separator\nexpertise3_long <- expertise3_long %>% separate(Category, into = c(\"Category\", \"Score_Type\"), sep = \"_\")\n\n#spread the data from long to wide format\nexpertise3_ready <- expertise3_long %>% spread(Category, Score)\n\n#change the score type to a factor\nexpertise3_ready$Score_Type<-as.factor(expertise3_ready$Score_Type)\n\nexpertise3_ready$ih<-as.numeric(expertise3_ready$ih)\n\n\n#view the data\nhead(expertise3_ready)\n\n   ID duration sex education income religion identity age political_atti\n1   1      403   1         3 105000     none    white  26              4\n2   2      473   1         5  53000  atheist    white  38              2\n3   3      442   2         2 15.000     none    white  22              5\n4   4      337   2         4 150000   jewish    white  55              5\n5   5      376   1         3  45000     None    White  29              2\n6   6      618   1         3  60000 catholic    white  58              4\n  english_level needforcog Score_Type              colle               grad\n1             4         43       lang -0.539409671771391 -0.152196749527831\n2             4         11       lang -0.539409671771391 -0.152196749527831\n3             4          7       lang -0.539409671771391 -0.152196749527831\n4             4         17       lang -0.539409671771391 -0.152196749527831\n5             4         47       lang -0.539409671771391 -0.152196749527831\n6             4         10       lang -0.539409671771391 -0.152196749527831\n                high  ih               know         numarticle\n1 -0.636464435362702 4.0 -0.929334565418051 -0.263392653035181\n2 -0.636464435362702 5.0   1.39400184812708 -0.263392653035181\n3 -0.636464435362702 6.0  -1.60521424935845 -0.263392653035181\n4 -0.636464435362702 5.0 0.0422424802462751 -0.263392653035181\n5 -0.177670284076514 4.5  0.380182322216475 -0.263392653035181\n6 -0.636464435362702 5.5 0.0422424802462751 -0.263392653035181\n              numbook\n1  -0.495098592648002\n2  -0.495098592648002\n3  -0.495098592648002\n4  -0.495098592648002\n5 -0.0777427542174549\n6  -0.495098592648002\n\n\nWe can start to analyze our models with using hlm:\n\n## Start to analyze\n\nlibrary(lme4) \nModel.Null<-lmer(ih ~1+(1|Score_Type)+(1|ID),  \n                 data=expertise3_ready)\nsummary(Model.Null)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: ih ~ 1 + (1 | Score_Type) + (1 | ID)\n   Data: expertise3_ready\n\nREML criterion at convergence: 6485.1\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.99061 -0.64339 -0.00129  0.60758  2.86910 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.5764   0.7592  \n Score_Type (Intercept) 0.3589   0.5991  \n Residual               2.7064   1.6451  \nNumber of obs: 1632, groups:  ID, 204; Score_Type, 8\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)   4.8768     0.2221   21.95\n\n# get pseudo R^2\nperformance::icc(Model.Null)\n\n# Intraclass Correlation Coefficient\n\n    Adjusted ICC: 0.257\n  Unadjusted ICC: 0.257\n\n\n\n# Now add infoavoid as a fixed effect\n# same slope for all sites\nModel.1<-lmer(ih ~needforcog+(1|Score_Type)+(1|ID),   \n              data=expertise3_ready)\nsummary(Model.1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: ih ~ needforcog + (1 | Score_Type) + (1 | ID)\n   Data: expertise3_ready\n\nREML criterion at convergence: 6313.2\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.4258 -0.6418 -0.0155  0.6080  2.8740 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.4809   0.6935  \n Score_Type (Intercept) 0.3589   0.5991  \n Residual               2.7064   1.6451  \nNumber of obs: 1632, groups:  ID, 204; Score_Type, 8\n\nFixed effects:\n              Estimate Std. Error t value\n(Intercept)     6.0313     0.6741   8.947\nneedforcog  1  -0.9063     1.1085  -0.818\nneedforcog  2  -1.0313     1.1085  -0.930\nneedforcog  3  -2.1250     0.9051  -2.348\nneedforcog  4  -1.0729     0.8262  -1.299\nneedforcog  5  -1.0313     1.1085  -0.930\nneedforcog  6  -0.5938     1.1085  -0.536\nneedforcog  7  -2.0313     0.9051  -2.244\nneedforcog  8  -0.7813     0.7838  -0.997\nneedforcog  9  -2.3438     1.1085  -2.114\nneedforcog -1  -0.6563     1.1085  -0.592\nneedforcog -2  -0.6563     1.1085  -0.592\nneedforcog -3  -0.4688     0.9051  -0.518\nneedforcog -4  -0.4063     1.1085  -0.366\nneedforcog -5  -0.9063     1.1085  -0.818\nneedforcog -6  -0.4688     0.9051  -0.518\nneedforcog -7  -1.9063     1.1085  -1.720\nneedforcog -9  -0.2813     1.1085  -0.254\nneedforcog 10  -0.9375     0.7155  -1.310\nneedforcog 11  -0.3259     0.7257  -0.449\nneedforcog 12  -0.9509     0.7257  -1.310\nneedforcog 13  -1.3813     0.7573  -1.824\nneedforcog 14  -1.1146     0.8262  -1.349\nneedforcog 15  -0.6875     0.9051  -0.760\nneedforcog 16  -1.3646     0.8262  -1.652\nneedforcog 17  -1.2634     0.7257  -1.741\nneedforcog 18  -1.1979     0.8262  -1.450\nneedforcog 19  -0.5000     0.7838  -0.638\nneedforcog 20  -1.5313     0.8262  -1.853\nneedforcog 21  -1.2188     0.7573  -1.609\nneedforcog 22  -1.4375     0.7838  -1.834\nneedforcog 23  -1.8438     0.9051  -2.037\nneedforcog 24  -0.9688     0.9051  -1.070\nneedforcog 25  -1.5938     1.1085  -1.438\nneedforcog 26  -1.0313     0.8262  -1.248\nneedforcog 27  -1.4688     1.1085  -1.325\nneedforcog 28  -0.7344     0.7838  -0.937\nneedforcog 29  -0.7604     0.8262  -0.920\nneedforcog 30  -1.5156     0.7838  -1.934\nneedforcog 31  -0.3125     0.9051  -0.345\nneedforcog 32  -0.2396     0.8262  -0.290\nneedforcog 33  -0.9479     0.8262  -1.147\nneedforcog 34  -1.3229     0.7390  -1.790\nneedforcog 35  -0.8938     0.7573  -1.180\nneedforcog 36  -1.1354     0.8262  -1.374\nneedforcog 37  -0.9219     0.7838  -1.176\nneedforcog 38  -2.1563     1.1085  -1.945\nneedforcog 39  -0.9271     0.8262  -1.122\nneedforcog 40  -1.5000     0.9051  -1.657\nneedforcog 41  -0.4063     0.9051  -0.449\nneedforcog 42  -1.3438     1.1085  -1.212\nneedforcog 43  -0.8229     0.8262  -0.996\nneedforcog 44   1.0937     1.1085   0.987\nneedforcog 45  -0.5063     0.7573  -0.669\nneedforcog 46  -2.2031     0.7838  -2.811\nneedforcog 47  -1.2656     0.7838  -1.615\nneedforcog 48  -1.3646     0.8262  -1.652\nneedforcog 50  -1.4688     0.9051  -1.623\nneedforcog 51  -1.6979     0.8262  -2.055\nneedforcog 52  -2.0000     0.7390  -2.706\nneedforcog 53  -1.8125     0.7838  -2.312\nneedforcog 54  -1.6563     1.1085  -1.494\nneedforcog 55  -3.9063     1.1085  -3.524\nneedforcog 56  -2.3438     0.7838  -2.990\nneedforcog 57  -0.7813     0.9051  -0.863\nneedforcog 58  -1.7813     0.9051  -1.968\nneedforcog 59  -0.9063     1.1085  -0.818\nneedforcog 60  -1.3750     0.9051  -1.519\nneedforcog 66  -1.0000     0.9051  -1.105\nneedforcog 68  -0.6563     1.1085  -0.592\nneedforcog 69  -2.4688     1.1085  -2.227\nneedforcog 70  -3.0938     1.1085  -2.791\nneedforcog 74  -3.0313     1.1085  -2.734\nneedforcog 78  -1.9063     1.1085  -1.720\nneedforcog-21   0.4687     1.1085   0.423\nneedforcog-30   0.7812     1.1085   0.705\nneedforcog-34  -1.4063     0.9051  -1.554\nneedforcog-36  -0.7813     1.1085  -0.705\n\n# get pseudo R^2\nperformance::icc(Model.1)\n\n# Intraclass Correlation Coefficient\n\n    Adjusted ICC: 0.237\n  Unadjusted ICC: 0.213\n\n\n\n# compare the two models\n\nanova(Model.Null,Model.1)\n\nrefitting model(s) with ML (instead of REML)\n\n\nData: expertise3_ready\nModels:\nModel.Null: ih ~ 1 + (1 | Score_Type) + (1 | ID)\nModel.1: ih ~ needforcog + (1 | Score_Type) + (1 | ID)\n           npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)   \nModel.Null    4 6491.9 6513.5 -3241.9   6483.9                        \nModel.1      81 6526.6 6963.8 -3182.3   6364.6 119.25 77   0.001448 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "posts/Thomas et al. 2008/index.html",
    "href": "posts/Thomas et al. 2008/index.html",
    "title": "Thomas et al. (2008) Diagnostic Hypothesis Generation and Human Judgment",
    "section": "",
    "text": "The focus of this article is providing a model of human judgment/reasoning (Hygene).\nThe universe of possible states means external events related to data that provides a basis to generate hypothesis.\nHypothesis means mental representation of an external event, so decision making process requires learning some subset of the external events. Which subset is an important question here with how this learning happens (e.g., self-explanation, associations or relational understanding?).\n\n\nThree principles\n\n“Data extracted from the environment serve as memory retrieval cues that prompt the retrieval of diagnostic hypotheses from long-term memory.”\nComment1a: Data works as the probing items to retrieve hypothesis from ltm, so it’s maybe like when I saw a shark tail in the ocean, I can retrieve that there may be a shark in the ocean because that shape of tails is specific to sharks.\n“The number of diagnostic hypotheses that one can actively entertain at any point in time is constrained by both cognitive limitations and task characteristics.”\nComment1b:So heuristics and biases play a role to limit the number of diagnostic hypotheses. For the previous example, I may not think that the tail is a joke and a person is using to scare people (an extrinsic reason).\n“Hypotheses maintained in the focus of attention (i.e., WM) serve as input into a comparison process to derive probability judgments and frame information search.”\nComment1c:Hypotheses in WM initate the probability judgement and h-testing.\n\nHygene consists of three parts:\n\nH Generation: How hypotheses are generated on the basis of data extracted from the environment\n-People generate few H’s without any limitation, fewer H’s under time pressure.\nQ1: We are here again, what is the mechanism behind time pressure? Would it be same thing if we also limit the data usage(watching video only 1 time vs. several times example)?\n-People generate H’s highest in a priori probability. (Like in the shark example, few h’s but those have highest probability (effort for effective usage of cognitive resources?))\n-Number of H’s is constrained by WM limitations.\nH Evaluation: How the hypotheses generated from memory are used to make probability judgments\nH Testing: How the generated hypotheses frame subsequent information search in hypothesis-testing situations\n\n\n\n\nThomas et al. (2008), p.161\n\n\nThey had different simulations to test their models. In the simulation 1, they tested their 1st principle, which is H generation.\nHow they modeled the amount of experience seems important here. They defined this as the number of traces stored in the model’s episodic memory. So, it probably means that experience provides more alternative and plausible hypotheses, which explains their results also (Figure 4, p.165). Based on those, we can argue that the traces of real event are more distorted, experience becomes effective. Experience provides true interpretation of distorted data/observations. It makes sense I guess, but only if the task/observation is a thing that can be grasped easily.\nQ2: This is not an empirical result, but it seems plausible but maybe missing. Besides more alternative H’s, experience also makes faster the processing of H’s evaluation, so that new H’s can be generated fast?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Recent Posts",
    "section": "",
    "text": "Expertise3 Data Analysis from the very beginning\n\n\n\n\n\n\n\nexample data\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nDec 26, 2022\n\n\nMK\n\n\n\n\n\n\n  \n\n\n\n\nThomas et al. (2014) Memory Constraints on Hypothesis Generation and Decision Making\n\n\n\n\n\n\n\narticles\n\n\nnotes\n\n\nnew ideas\n\n\n\n\n\n\n\n\n\n\n\nDec 21, 2022\n\n\nMK\n\n\n\n\n\n\n  \n\n\n\n\nThomas et al. (2008) Diagnostic Hypothesis Generation and Human Judgment\n\n\n\n\n\n\n\narticles\n\n\nnotes\n\n\nnew ideas\n\n\n\n\n\n\n\n\n\n\n\nDec 21, 2022\n\n\nMK\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This blog includes my notes about articles, example data analysis and other things like a journal, so I call this as learning journal."
  }
]