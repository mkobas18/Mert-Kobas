{
  "hash": "e9c1d6f2a7bcf1c6fca4461032a51ba5",
  "result": {
    "markdown": "---\ntitle: \"Expertise3 Data Analysis from the very beginning\"\nauthor: \"MK\"\ndate: \"2023-02-23\"\ncategories: [example data, code, analysis]\nimage: \"newdata.png\"\n---\n\n\nThis post includes the trial analyses of an example data related to expertise.\n\n### Import necessary packages and expertise data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Import library ----\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(purrr)\nlibrary(jmv)\nlibrary(psych)\nlibrary(DescTools)\nlibrary(stats)\nlibrary(factoextra)\nlibrary(effects)\n#Read the csv file ----\nExpertise3 <- read_csv(\"~/Desktop/Expertise3_December 23, 2022_10.53.csv\")\n```\n:::\n\n\nCreates a new dataframe called expertise3_clean, which is a copy of the original dataframe called Expertise3 and removes the second row of the dataframe and create a variable called column_names and assign it the names of the columns in the dataframe and change the column names\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#removes the second row of the dataframe\nexpertise3_clean<-Expertise3%>% \n  slice(-2) \n\n\n#selects all columns except the ones listed \nexpertise3_clean <- expertise3_clean %>%\n  select(-StartDate, -EndDate, -Status, -Progress,-ResponseId,-RecordedDate,-RecipientLastName, -RecipientFirstName,-RecipientEmail, -ExternalReference, -LocationLatitude,-LocationLongitude, -DistributionChannel, -UserLanguage)\n\n#create a variable called column_names and assign it the names of the columns in the dataframe\ncolumn_names <- names(expertise3_clean)\n\n##Change the column names ----\ncolnames(expertise3_clean) <- c('ip','duration', 'finished', 'stih_lang','r_ih_lang','stih_school','r_ih_school',\n                                'stih_cards','r_ih_cards','stih_breakfast','r_ih_breakfast','stih_weddings','r_ih_weddings',\n                                'stih_teeth','r_ih_teeth','stih_traffic','r_ih_traffic','stih_tv','r_ih_tv',\n                                'know_lang','know_school','know_cards','know_breakfast','know_weddings','know_teeth',\n                                'know_traffic','know_tv',\n                                'course_lang','high_lang', 'colle_lang', 'grad_lang',\n                                'book_lang','numbook_lang','article_lang','numarticle_lang',\n                                'course_school','high_school', 'colle_school', 'grad_school',\n                                'book_school','numbook_school','article_school','numarticle_school',\n                                'course_cards','high_cards', 'colle_cards', 'grad_cards',\n                                'book_cards','numbook_cards','article_cards','numarticle_cards',\n                                'course_breakfast','high_breakfast', 'colle_breakfast', 'grad_breakfast',\n                                'book_breakfast','numbook_breakfast','article_breakfast','numarticle_breakfast',\n                                'course_weddings','high_weddings', 'colle_weddings', 'grad_weddings',\n                                'book_weddings','numbook_weddings','article_weddings','numarticle_weddings',\n                                'course_teeth','high_teeth', 'colle_teeth', 'grad_teeth',\n                                'book_teeth','numbook_teeth','article_teeth','numarticle_teeth',\n                                'course_traffic','high_traffic', 'colle_traffic', 'grad_traffic',\n                                'book_traffic','numbook_traffic','article_traffic','numarticle_traffic',\n                                'course_tv','high_tv', 'colle_tv', 'grad_tv',\n                                'book_tv','numbook_tv','article_tv','numarticle_tv',\n                                'needforcog1','needforcog2','needforcog3','needforcog4','needforcog5','needforcog6',\n                                'needforcog7','needforcog8','needforcog9','needforcog10','needforcog11','needforcog12',\n                                'needforcog13','needforcog14','needforcog15','needforcog16','needforcog17','needforcog18',\n                                'otherways','sex','birthdate','education','income','religion','identity','age','political_atti',\n                                'english_level','proceure_confu','whatwestudied','moretothisstudy','additional_thoughts','attention')\n```\n:::\n\n\n### Questionnaire Items\n\nThe code below shows the survey items:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#selects all columns except the ones specified\nrow_values <- expertise3_clean %>%\n  select(-duration,-finished, -ip)%>%\n  #selects only the first row\n  filter(row_number() == 1)\n\n#Items in the questionnaire ----\n\n#unlist the row_values\nrow_values <- unlist(row_values)\n\nmy_list <- map(row_values, ~paste0(.))\n\nlibrary(stringr)\nmy_list <- str_replace(my_list, \"(?<! )\\\\n(?! )\", \"\")\nmy_list <- str_replace(my_list, \"[^\\\\s]*\\\\\\\\n[^\\\\s]*\", \"\")\nlist_string <- paste0(\"* \", paste(my_list, collapse = \"\\n* \"))\n\n##Show the survey items ----\ncat(list_string)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n* Human languages are structured to best convey ourthoughts and feelings. Words and their meanings likely form ideal matches.\n* There are absolutely no good reasons whywe use specific words to represent our thoughts. Any combination of sounds\ncould in principle refer to any idea.\n* The fact that elementary school stops at 5th grade is probably ideal for children's learning. This is likely the best way to organize K-12 schooling.\n* Middle school (grades 6-8) is separate from elementary school (grades K-5) largely because of decisions made by educators a long time ago. This may not be the most optimal way of organizing early education.\n* It’s not a coincidence thatwe send people cards on holidays. This tradition seems\nparticularly fitting.\n* The fact that we send people cards on holidays is only a convention. A different way of sending warm wishes could've been implemented just as easily.\n* There are good reasons why orange juiceis typically consumed for breakfast. There are features about it that make it\nparticularly suited for this meal (for example, its refreshing taste).\n* The current popularity of orange juice for breakfast reflectsin part  marketing campaigns that promoted\ndrinking orange juice in the morning. However, had history taken a different\nturn, orange juice could just as easily have been more popular for lunch or\ndinner.\n* It seems right to use white for wedding dresses. Othercolors, such as red and blue, have features that make them less suited for\nwedding dresses.\n* Even though white is thetraditional color for wedding dresses, this could have easily been different.\nWhen you really think about it, there is no reason why other, brighter, colors couldn’t\nbe used for wedding dresses.\n* It seems ideal that toothpaste istypically flavored with mint. Mint is inherently more refreshing than any other\nflavor that currently exists.\n* When you think about it, toothpaste could have easily beenflavored with something other than mint, such as cinnamon. Many pleasing flavors would work just as\nwell.\n* Traffic lights, with threedifferent colored lights signaling three speeds, seem like the most efficient\nand effective way to direct traffic. Another process likely would not work as\nwell.\n* The current design of traffic lights,with three different colors reflecting three different speeds, is entirely due\nto historical factors. This is probably not the most efficient or effective way\nto manage traffic.\n* Black seems like a good choice for the color of televisions. Other colors just would not work as well.\n* Theonly reason why most TVs are black is\nhistorical happenstance. TVs could practically be a\nvariety of colors.\n* How much do you know about language and linguistics? - Please use the slider to select your answer choice.\n* How much do you know about school and education systems? - Please use the slider to select your answer choice.\n* How much do you know about holiday customs and traditions? - Please use the slider to select your answer choice.\n* How much do you know about breakfast foods? - Please use the slider to select your answer choice.\n* How much do you know about weddings and wedding traditions? - Please use the slider to select your answer choice.\n* How much do you know about teeth and oral hygiene? - Please use the slider to select your answer choice.\n* How much do you know about transportation science and traffic signal systems? - Please use the slider to select your answer choice.\n* How much do you know about the manufacturing of consumer electronics (TVs, MP3 players, camcorders, etc.)? - Please use the slider to select your answer choice.\n* Have you ever taken a class that discussed language and linguistics?\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - High School\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - College\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - Graduate School\n* Have you ever read any books on language and linguistics?\n* If you answered \"yes\" above, please approximate the number of books you have read on this topic:\n* Have you ever read any magazine, newspaper, or academic articles on language and linguistics?\n* If you answered \"yes\" above, please approximate the number of articles you have read on this topic:\n* Have you ever taken a class that discussed school and education systems?\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - High School\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - College\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - Graduate School\n* Have you ever read any books on school and education systems?\n* If you answered \"yes\" above, please approximate the number of books you have read on this topic:\n* Have you ever read any magazine, newspaper, or academic articles on school and education systems?\n* If you answered \"yes\" above, please approximate the number of articles you have read on this topic:\n* Have you ever taken a class that discussed holiday customs and traditions?\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - High School\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - College\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - Graduate School\n* Have you ever read any books on holiday customs and traditions?\n* If you answered \"yes\" above, please approximate the number of books you have read on this topic:\n* Have you ever read any magazine, newspaper, or academic articles on holiday customs and traditions?\n* If you answered \"yes\" above, please approximate the number of articles you have read on this topic:\n* Have you ever taken a class that discussed breakfast foods?\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - High School\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - College\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - Graduate School\n* Have you ever read any books on breakfast foods?\n* If you answered \"yes\" above, please approximate the number of books you have read on this topic:\n* Have you ever read any magazine, newspaper, or academic articles on breakfast foods?\n* If you answered \"yes\" above, please approximate the number of articles you have read on this topic:\n* Have you ever taken a class that discussed weddings and wedding traditions?\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - High School\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - College\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - Graduate School\n* Have you ever read any books on weddings and wedding traditions?\n* If you answered \"yes\" above, please approximate the number of books you have read on this topic:\n* Have you ever read any magazine, newspaper, or academic articles on weddings and wedding traditions?\n* If you answered \"yes\" above, please approximate the number of articles you have read on this topic:\n* Have you ever taken a class that discussed teeth and oral hygiene?\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - High School\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - College\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - Graduate School\n* Have you ever read any books on teeth and oral hygiene?\n* If you answered \"yes\" above, please approximate the number of books you have read on this topic:\n* Have you ever read any magazine, newspaper, or academic articles on teeth and oral hygiene?\n* If you answered \"yes\" above, please approximate the number of articles you have read on this topic:\n* Have you ever taken a class that discussed transportation science and traffic signal systems?\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - High School\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - College\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - Graduate School\n* Have you ever read any books on transportation science and traffic signal systems?\n* If you answered \"yes\" above, please approximate the number of books you have read on this topic:\n* Have you ever read any magazine, newspaper, or academic articles on transportation science and traffic signal systems?\n* If you answered \"yes\" above, please approximate the number of articles you have read on this topic:\n* Have you ever taken a class that discussed the manufacturing of consumer electronics (TVs, MP3 players, camcorders, etc.)?\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - High School\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - College\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - Graduate School\n* Have you ever read any books on the manufacturing of consumer electronics (TVs, MP3 players, camcorders, etc.)?\n* If you answered \"yes\" above, please approximate the number of books you have read on this topic:\n* Have you ever read any magazine, newspaper, or academic articles on the manufacturing of consumer electronics (TVs, MP3 players, camcorders, etc.)?\n* If you answered \"yes\" above, please approximate the number of articles you have read on this topic:\n* I would prefer complex to simple problems.\n* I like to have the responsibility of handling a situation that requires a lot of thinking.\n* Thinking is not my idea of fun.\n* I would rather do something that requires little thought than something that is sure to challenge my thinking abilities.\n* I try to anticipate and avoid situations where there is a likely chance I will have to think in depth about something.\n* I find satisfaction in deliberating hard and long for hours.\n* I only think as hard as I have to.\n* I prefer to think about small, daily projects rather than long-term ones.\n* I like tasks that require little thought once I’ve learned them.\n* The idea of relying on thought to make my way to the top appeals to me.\n* I really enjoy a task that involves coming up with new solutions to problems.\n* Learning new ways to think doesn’t excite me very much.\n* I prefer my life to be filled with puzzles that I must solve.\n* The notion of thinking abstractly appeals to me.\n* I would prefer a task that is intellectual, difficult, and important to one that is somewhat important but does not require much thought.\n* I feel relief rather than satisfaction after completing a task that required a lot of mental effort.\n* It’s enough for me that something gets the job done; I don’t care about why or how it works.\n* I usually end up deliberating about issues even when they do not affect me personally.\n* In this survey you were asked about your knowledge in a variety of subject areas. \nAside from the ways you may have learned about these topics that were mentioned in the survey (i.e., reading or  a college class), how else may you have learned about these topics? Please name other ways that you have learned about the topics mentioned in the survey you just took.\n* Are you male or female?\n* Q60 - What is your date of birth? (mm/dd/yyyy)\n* What is the highest level of education you have completed?\n* Q64 - What is your yearly household income?\n* Q64 - What is your religious affiliation?\n* Q64 - What is your racial or ethnic identity?\n* Q64 - What is your age in years?\n* How would you describe your political attitudes? Please select one of the points on the scale below.\n* Please rate your overall ability in the English language:\n* 1. Did you find any aspect of the procedure odd or confusing?\n* 2. What did you think we were studying?\n* 3. Do you think that there may have been more to this study than meets the eye? If so, what do you think this might have been?\n* 4. Do you have any additional thoughts or comments about the study?\n* Thank you for completing this survey! We just have one last question for you. You will not be penalized for your answer to this question. Since you completed the whole survey, you will receive payment no matter what answer you give here.\n\t \n\n\tIt's very important to the quality and scientific aims of our study that participants pay attention (i.e., read the survey carefully, consider the response options, and avoid distractions).\n\n\t\n\n\tWere you paying attention while completing this survey?\n```\n:::\n:::\n\n\n### Exclusion Criterias\n\nData preparation for further analyses\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Attention check and deletion of cases that didn't attend or finish the study ----\nexpertise3_new<-expertise3_clean%>%\n  filter(attention==1&finished==1)\n\n\n##Exclude the participants that joined outside of US ----\n\n#view(expertise3_new)\nexpertise3_new<-expertise3_new%>%\n  filter(ip!=\"37.221.172.194\")\n\nexpertise3_new <- expertise3_new %>%\n  filter(!(ip %in% c(\"77.198.10.26\", \"83.233.218.246\", \"189.172.66.106\", \"190.167.6.137\")))\n\n##selecting the columns that we want to keep ----\nexpertise3_new<-expertise3_new%>%\n  select(-finished,-otherways,-birthdate,-proceure_confu,-whatwestudied,-moretothisstudy,-additional_thoughts,-attention)\n\n#adds a column to the dataframe, with the name \"id\"\nexpertise3_new<-cbind(ID = 1:nrow(expertise3_new), expertise3_new)\n\n# Numeric variables ----\n# Change the data type of the variables to numeric \nexpertise3_new <- expertise3_new %>%\n  mutate_at(vars(stih_lang, r_ih_lang, stih_school, r_ih_school, stih_cards, r_ih_cards, stih_breakfast, r_ih_breakfast, \n                 stih_weddings, r_ih_weddings, stih_teeth, r_ih_teeth, stih_traffic, r_ih_traffic, stih_tv, r_ih_tv), as.numeric)\n```\n:::\n\n\n## Correlation for ih scores\n\nCheck the correlations between inherence (the variables starting with st) and reverse inherence (the variables starting with r) scores to check whether it's appropriate for averaging\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Correlations between ih scores ----\n\n# Create a list of variable names\nvariables <- c(\"stih_lang\", \"r_ih_lang\", \"stih_school\", \"r_ih_school\", \"stih_cards\", \"r_ih_cards\", \"stih_breakfast\", \"r_ih_breakfast\", \"stih_weddings\", \"r_ih_weddings\", \"stih_teeth\", \"r_ih_teeth\", \"stih_traffic\", \"r_ih_traffic\", \"stih_tv\", \"r_ih_tv\")\n\n# Initialize an empty data frame to store the correlation coefficients\ncorrelations <- data.frame(variable1 = character(), variable2 = character(), correlation = numeric(), p.value = numeric(), conf.int = character())\n\n# Iterate over the pairs of variables\nfor (i in seq(1, length(variables), 2)) {\n  j <- i + 1\n  \n  # Calculate the Pearson correlation coefficient and test the statistical significance\n  correlation_test <- cor.test(expertise3_new[, variables[i]], expertise3_new[, variables[j]], method = \"pearson\")\n  \n  # Add the correlation coefficient, p-value, and confidence interval to the data frame\n  correlations <- rbind(correlations, data.frame(variable1 = variables[i], variable2 = variables[j], correlation = correlation_test$estimate, p.value = correlation_test$p.value, conf.int = paste(correlation_test$conf.int[1], correlation_test$conf.int[2], sep = \" - \")))}\n\n## View the correlation coefficients and statistical measures ----\ncorrelations\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          variable1      variable2 correlation      p.value\ncor       stih_lang      r_ih_lang  -0.6112615 8.942654e-22\ncor1    stih_school    r_ih_school  -0.5181904 4.547542e-15\ncor2     stih_cards     r_ih_cards  -0.5395115 2.001074e-16\ncor3 stih_breakfast r_ih_breakfast  -0.3813141 2.751024e-08\ncor4  stih_weddings  r_ih_weddings  -0.5222404 2.554910e-15\ncor5     stih_teeth     r_ih_teeth  -0.4231281 5.283701e-10\ncor6   stih_traffic   r_ih_traffic  -0.4832966 4.822560e-13\ncor7        stih_tv        r_ih_tv  -0.3396313 9.221208e-07\n                                    conf.int\ncor  -0.691555186062648 - -0.516045968176506\ncor1  -0.613091701381407 - -0.40854648901427\ncor2 -0.631233272015308 - -0.432907170777665\ncor3 -0.494193465446722 - -0.255790470305453\ncor4  -0.61654545073014 - -0.413161868080018\ncor5 -0.531226389630016 - -0.301474099514304\ncor6  -0.583183953827757 - -0.36901235203396\ncor7 -0.457126840023376 - -0.210484471909784\n```\n:::\n:::\n\n\n## IH scores calculation\n\nIt seems that each pairs have negative significant correlation, so we can take the average scores to calculate inherence scores\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Average of ih scores  ----\n#It seems that each pairs have negative significant correlation, so we can take the average scores to measure inherence scores\nexpertise3_new <- expertise3_new %>%\n  mutate(ih_lang = (stih_lang + (10 - r_ih_lang))/2,\n         ih_school = (stih_school + (10 - r_ih_school)) / 2,\n         ih_cards = (stih_cards + (10 - r_ih_cards)) / 2,\n         ih_breakfast = (stih_breakfast + (10 - r_ih_breakfast)) / 2,\n         ih_weddings = (stih_weddings + (10 - r_ih_weddings)) / 2,\n         ih_teeth = (stih_teeth + (10 - r_ih_teeth)) / 2,\n         ih_traffic = (stih_traffic + (10-r_ih_traffic)) / 2,\n         ih_tv = (stih_tv + (10-r_ih_tv)) / 2 )\n```\n:::\n\n\n## Need for Cognition scores calculation\n\nCalculate \"Need for cognition\" scale scores\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Need for cognition scale scores  ----\n\n# change the data type of the variables to numeric\nexpertise3_new <- expertise3_new %>%\n  mutate_at(vars(needforcog1,needforcog2,needforcog3,needforcog4,needforcog5,needforcog6\n                 ,needforcog7,needforcog8,needforcog9,needforcog10,needforcog11,needforcog12,\n                 needforcog13,needforcog14,needforcog15,needforcog16,needforcog17,needforcog18), as.numeric)\n\n## Calculate needforcog scores  ----\n#add a new variable called needforcog, which is the sum of all the need for cognition items, the items are weighted according to the scoring key\nexpertise3_new <- expertise3_new %>%\n  group_by(ID)%>%\n  mutate(needforcog=(needforcog1+needforcog2+\n                       (10-needforcog3)+(10-needforcog4)+(10-needforcog5)+needforcog6+\n                       (10-needforcog7)+(10-needforcog8)+(10-needforcog9)+\n                       needforcog10+needforcog11+(10-needforcog12)+\n                       needforcog13+needforcog14+needforcog15+\n                       (10-needforcog16)+(10-needforcog17)+needforcog18)/18)\n```\n:::\n\n\n## Data preparation\n\nPrepare the expertise scores and other scores ready for analyses\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Replace expertise variables' NA values in the expertise columns with 0  ----\nvariables <- c('high_lang', 'colle_lang', 'grad_lang',\n               'numbook_lang','numarticle_lang',\n               'high_school', 'colle_school', 'grad_school',\n               'numbook_school','numarticle_school',\n               'high_cards', 'colle_cards', 'grad_cards',\n               'numbook_cards','numarticle_cards',\n               'high_breakfast', 'colle_breakfast', 'grad_breakfast',\n               'numbook_breakfast','numarticle_breakfast',\n               'high_weddings', 'colle_weddings', 'grad_weddings',\n               'numbook_weddings','numarticle_weddings',\n               'high_teeth', 'colle_teeth', 'grad_teeth',\n               'numbook_teeth','numarticle_teeth',\n               'high_traffic', 'colle_traffic', 'grad_traffic',\n               'numbook_traffic','numarticle_traffic',\n               'high_tv', 'colle_tv', 'grad_tv',\n               'numbook_tv','numarticle_tv')\n\nexpertise3_new[variables] <- lapply(expertise3_new[variables], \n                                    function(x) ifelse(is.na(x), 0, ifelse(x=='no',0,x)))\n\n# Missing values in the dataframe  ----\napply(is.na(expertise3_new), 2, sum)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                  ID                   ip             duration \n                   0                    0                    0 \n           stih_lang            r_ih_lang          stih_school \n                   0                    0                    0 \n         r_ih_school           stih_cards           r_ih_cards \n                   0                    0                    0 \n      stih_breakfast       r_ih_breakfast        stih_weddings \n                   0                    0                    0 \n       r_ih_weddings           stih_teeth           r_ih_teeth \n                   0                    1                    1 \n        stih_traffic         r_ih_traffic              stih_tv \n                   0                    0                    0 \n             r_ih_tv            know_lang          know_school \n                   0                    0                    0 \n          know_cards       know_breakfast        know_weddings \n                   0                    0                    0 \n          know_teeth         know_traffic              know_tv \n                   0                    1                    0 \n         course_lang            high_lang           colle_lang \n                   0                    0                    0 \n           grad_lang            book_lang         numbook_lang \n                   0                    0                    0 \n        article_lang      numarticle_lang        course_school \n                   0                    0                    1 \n         high_school         colle_school          grad_school \n                   0                    0                    0 \n         book_school       numbook_school       article_school \n                   0                    0                    1 \n   numarticle_school         course_cards           high_cards \n                   0                    0                    0 \n         colle_cards           grad_cards           book_cards \n                   0                    0                    0 \n       numbook_cards        article_cards     numarticle_cards \n                   0                    1                    0 \n    course_breakfast       high_breakfast      colle_breakfast \n                   0                    0                    0 \n      grad_breakfast       book_breakfast    numbook_breakfast \n                   0                    0                    0 \n   article_breakfast numarticle_breakfast      course_weddings \n                   1                    0                    0 \n       high_weddings       colle_weddings        grad_weddings \n                   0                    0                    0 \n       book_weddings     numbook_weddings     article_weddings \n                   1                    0                    0 \n numarticle_weddings         course_teeth           high_teeth \n                   0                    0                    0 \n         colle_teeth           grad_teeth           book_teeth \n                   0                    0                    0 \n       numbook_teeth        article_teeth     numarticle_teeth \n                   0                    0                    0 \n      course_traffic         high_traffic        colle_traffic \n                   0                    0                    0 \n        grad_traffic         book_traffic      numbook_traffic \n                   0                    0                    0 \n     article_traffic   numarticle_traffic            course_tv \n                   0                    0                    0 \n             high_tv             colle_tv              grad_tv \n                   0                    0                    0 \n             book_tv           numbook_tv           article_tv \n                   0                    0                    1 \n       numarticle_tv          needforcog1          needforcog2 \n                   0                    0                    0 \n         needforcog3          needforcog4          needforcog5 \n                   0                    0                    0 \n         needforcog6          needforcog7          needforcog8 \n                   0                    0                    0 \n         needforcog9         needforcog10         needforcog11 \n                   0                    0                    0 \n        needforcog12         needforcog13         needforcog14 \n                   0                    0                    0 \n        needforcog15         needforcog16         needforcog17 \n                   0                    0                    0 \n        needforcog18                  sex            education \n                   0                    0                    0 \n              income             religion             identity \n                   2                    2                    0 \n                 age       political_atti        english_level \n                   0                    0                    0 \n             ih_lang            ih_school             ih_cards \n                   0                    0                    0 \n        ih_breakfast          ih_weddings             ih_teeth \n                   0                    0                    1 \n          ih_traffic                ih_tv           needforcog \n                   0                    0                    0 \n```\n:::\n\n```{.r .cell-code}\n# Expertise Ready Df ----\n\n# Create new data frame as analyzable \nexpertise3_new<-expertise3_new%>%\n  select(-stih_lang,-r_ih_lang,-stih_school,-r_ih_school,\n         -stih_cards,-r_ih_cards,-stih_breakfast,-r_ih_breakfast,\n         -stih_weddings,-r_ih_weddings,\n         -stih_teeth,-r_ih_teeth,-stih_traffic,-r_ih_traffic,\n         -stih_tv,-r_ih_tv,\n         -course_lang,\n         -book_lang,-article_lang,\n         -course_school,\n         -book_school,-article_school,\n         -course_cards,\n         -book_cards,-article_cards,\n         -course_breakfast,\n         -book_breakfast,-article_breakfast,\n         -course_weddings,\n         -book_weddings,-article_weddings,\n         -course_teeth,\n         -book_teeth,-article_teeth,\n         -course_traffic,\n         -book_traffic,-article_traffic,\n         -course_tv,\n         -book_tv,-article_tv,\n         -needforcog1,-needforcog2,-needforcog3,-needforcog4,-needforcog5,-needforcog6,\n         -needforcog7,-needforcog8,-needforcog9,-needforcog10,-needforcog11,-needforcog12,\n         -needforcog13,-needforcog14,-needforcog15,-needforcog16,-needforcog17,-needforcog18)\n\n\n# change the data type of the variables to numeric\nexpertise3_new <- expertise3_new %>%\n  mutate_at(vars('know_lang','know_school','know_cards','know_breakfast',\n                 'know_weddings','know_teeth','know_traffic','know_tv',\n                 'high_lang', 'colle_lang', 'grad_lang',\n                 'numbook_lang','numarticle_lang',\n                 'high_school', 'colle_school', 'grad_school',\n                 'numbook_school','numarticle_school',\n                 'high_cards', 'colle_cards', 'grad_cards',\n                 'numbook_cards','numarticle_cards',\n                 'high_breakfast', 'colle_breakfast', 'grad_breakfast',\n                 'numbook_breakfast','numarticle_breakfast',\n                 'high_weddings', 'colle_weddings', 'grad_weddings',\n                 'numbook_weddings','numarticle_weddings',\n                 'high_teeth', 'colle_teeth', 'grad_teeth',\n                 'numbook_teeth','numarticle_teeth',\n                 'high_traffic', 'colle_traffic', 'grad_traffic',\n                 'numbook_traffic','numarticle_traffic',\n                 'high_tv', 'colle_tv', 'grad_tv',\n                 'numbook_tv','numarticle_tv'), as.numeric)\n```\n:::\n\n\n## Long format\n\nLong format of expertise dataset for factor analysis\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary('reshape2')\n\nexpertise3_factor <- melt(expertise3_new, id.vars = c(\"ID\",'duration',\"sex\" ,\"education\" ,\"income\",\"religion\",'identity','age','political_atti','english_level','needforcog'), \n                        measure.vars = c(\"ih_lang\", \"ih_school\",\"ih_cards\", \"ih_breakfast\", \"ih_weddings\", \n                                         \"ih_teeth\", \"ih_traffic\", \"ih_tv\",'know_lang','know_school','know_cards','know_breakfast',\n                                         'know_weddings','know_teeth','know_traffic','know_tv',\n                                         'high_lang', 'colle_lang', 'grad_lang',\n                                         'numbook_lang','numarticle_lang',\n                                         'high_school', 'colle_school', 'grad_school',\n                                         'numbook_school','numarticle_school',\n                                         'high_cards', 'colle_cards', 'grad_cards',\n                                         'numbook_cards','numarticle_cards',\n                                         'high_breakfast', 'colle_breakfast', 'grad_breakfast',\n                                         'numbook_breakfast','numarticle_breakfast',\n                                         'high_weddings', 'colle_weddings', 'grad_weddings',\n                                         'numbook_weddings','numarticle_weddings',\n                                         'high_teeth', 'colle_teeth', 'grad_teeth',\n                                         'numbook_teeth','numarticle_teeth',\n                                         'high_traffic', 'colle_traffic', 'grad_traffic',\n                                         'numbook_traffic','numarticle_traffic',\n                                         'high_tv', 'colle_tv', 'grad_tv',\n                                         'numbook_tv','numarticle_tv'),\n                        sep = \"_\", variable.name = \"Category\", value.name = \"Score\")\n\n# Split the Category column into two columns based on the underscore separator\nexpertise3_factor <- expertise3_factor %>% separate(Category, into = c(\"Category\", \"Score_Type\"), sep = \"_\")\n\n## spread the data from long to wide format  ----\nexpertise3_fact <- expertise3_factor %>% spread(Category, Score)\n\n# change the score type to a factor\nexpertise3_fact$Score_Type<-as.factor(expertise3_fact$Score_Type)\n\n# convert the column ih to numeric\nexpertise3_fact$ih<-as.numeric(expertise3_fact$ih)\n```\n:::\n\n\n###Create another data frame to winsorize expertise variables before factor analysis\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexpertise3_wins_fact<-expertise3_fact\n```\n:::\n\n\n## Factor analysis\n\nFactor analysis for expertise variables with raw scores\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Factor analysis for expertise variables with raw scores ----\n\n# Import packages \n\nlibrary(psych) #PCA/EFA analysis\nlibrary(REdaS) #Produces KMO and Bartletts test\nlibrary(GPArotation)\n\n\n# Create a new dataframe that include only related variables\nfactor_exp<-expertise3_fact%>%\n  select(colle, grad, high, numarticle, numbook)\n\n# Check missing values\napply(is.na(factor_exp), 2, sum)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     colle       grad       high numarticle    numbook \n         0          0          0          0          0 \n```\n:::\n\n```{.r .cell-code}\n# Since grad classes for TV category is missing (nobody takes any class in the sample), listwise deletion is applied here.\nbart_spher(factor_exp, use = \"complete.obs\") ###### produces Bartletts test of spherecity \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\tBartlett's Test of Sphericity\n\nCall: bart_spher(x = factor_exp, use = \"complete.obs\")\n\n     X2 = 1028.145\n     df = 10\np-value < 2.22e-16\n```\n:::\n\n```{.r .cell-code}\nKMO(factor_exp)       ###### Kaiser-Meyer-Olkin measure, which is above .5.\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nKaiser-Meyer-Olkin factor adequacy\nCall: KMO(r = factor_exp)\nOverall MSA =  0.58\nMSA for each item = \n     colle       grad       high numarticle    numbook \n      0.57       0.59       0.56       0.56       0.60 \n```\n:::\n\n```{.r .cell-code}\n# Let's check all the variables\nfa(factor_exp, nfactors = 5, rotate =  \"oblimin\" )  \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFactor Analysis using method =  minres\nCall: fa(r = factor_exp, nfactors = 5, rotate = \"oblimin\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n             MR1   MR2   MR3   MR4 MR5   h2   u2 com\ncolle       0.73  0.01 -0.01  0.07   0 0.56 0.44 1.0\ngrad       -0.05  0.54 -0.05  0.08   0 0.31 0.69 1.1\nhigh        0.73 -0.03  0.00 -0.06   0 0.51 0.49 1.0\nnumarticle -0.05 -0.05  0.56  0.01   0 0.28 0.72 1.0\nnumbook     0.14  0.37  0.38  0.06   0 0.57 0.43 2.3\n\n                       MR1  MR2  MR3  MR4  MR5\nSS loadings           1.11 0.53 0.52 0.08 0.00\nProportion Var        0.22 0.11 0.10 0.02 0.00\nCumulative Var        0.22 0.33 0.43 0.45 0.45\nProportion Explained  0.50 0.24 0.23 0.03 0.00\nCumulative Proportion 0.50 0.73 0.97 1.00 1.00\n\n With factor correlations of \n     MR1  MR2  MR3  MR4 MR5\nMR1 1.00 0.32 0.24 0.15   0\nMR2 0.32 1.00 0.52 0.72   0\nMR3 0.24 0.52 1.00 0.30   0\nMR4 0.15 0.72 0.30 1.00   0\nMR5 0.00 0.00 0.00 0.00   1\n\nMean item complexity =  1.3\nTest of the hypothesis that 5 factors are sufficient.\n\nThe degrees of freedom for the null model are  10  and the objective function was  0.65 with Chi Square of  1028.15\nThe degrees of freedom for the model are -5  and the objective function was  0 \n\nThe root mean square of the residuals (RMSR) is  0 \nThe df corrected root mean square of the residuals is  NA \n\nThe harmonic number of observations is  1592 with the empirical chi square  0  with prob <  NA \nThe total number of observations was  1592  with Likelihood Chi Square =  0  with prob <  NA \n\nTucker Lewis Index of factoring reliability =  1.01\nFit based upon off diagonal values = 1\nMeasures of factor score adequacy             \n                                                   MR1  MR2  MR3   MR4 MR5\nCorrelation of (regression) scores with factors   0.84 0.75 0.72  0.57   0\nMultiple R square of scores with factors          0.71 0.57 0.52  0.32   0\nMinimum correlation of possible factor scores     0.42 0.14 0.04 -0.36  -1\n```\n:::\n\n```{.r .cell-code}\n# So we can reduce it to 2 factors\nfa(factor_exp, nfactors = 2, rotate =  \"oblimin\" )  \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFactor Analysis using method =  minres\nCall: fa(r = factor_exp, nfactors = 2, rotate = \"oblimin\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n             MR1   MR2   h2    u2 com\ncolle       0.06  0.72 0.55 0.455 1.0\ngrad        0.36 -0.01 0.13 0.875 1.0\nhigh       -0.05  0.73 0.51 0.488 1.0\nnumarticle  0.33 -0.06 0.10 0.900 1.1\nnumbook     0.97  0.00 0.95 0.052 1.0\n\n                       MR1  MR2\nSS loadings           1.18 1.05\nProportion Var        0.24 0.21\nCumulative Var        0.24 0.45\nProportion Explained  0.53 0.47\nCumulative Proportion 0.53 1.00\n\n With factor correlations of \n     MR1  MR2\nMR1 1.00 0.35\nMR2 0.35 1.00\n\nMean item complexity =  1\nTest of the hypothesis that 2 factors are sufficient.\n\nThe degrees of freedom for the null model are  10  and the objective function was  0.65 with Chi Square of  1028.15\nThe degrees of freedom for the model are 1  and the objective function was  0 \n\nThe root mean square of the residuals (RMSR) is  0.01 \nThe df corrected root mean square of the residuals is  0.03 \n\nThe harmonic number of observations is  1592 with the empirical chi square  3.66  with prob <  0.056 \nThe total number of observations was  1592  with Likelihood Chi Square =  5.04  with prob <  0.025 \n\nTucker Lewis Index of factoring reliability =  0.96\nRMSEA index =  0.05  and the 90 % confidence intervals are  0.014 0.098\nBIC =  -2.34\nFit based upon off diagonal values = 1\nMeasures of factor score adequacy             \n                                                   MR1  MR2\nCorrelation of (regression) scores with factors   0.97 0.84\nMultiple R square of scores with factors          0.95 0.70\nMinimum correlation of possible factor scores     0.90 0.41\n```\n:::\n\n```{.r .cell-code}\n# Figure for the analysis\n\nM1<-fa(factor_exp, nfactors = 2, rotate =  \"oblimin\" ) ##save the analysis as the object m1\nfa.diagram(M1,main=\"Expertise Variables\")  \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n### Eigenvalues\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Check eigenvalues\n\nfa.parallel(factor_exp)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nParallel analysis suggests that the number of factors =  2  and the number of components =  2 \n```\n:::\n:::\n\n\n### Extracting factor values\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfactor_exp_score <- factanal(factor_exp, factors=2, scores=\"regression\", rotation = \"oblimin\")\n\nhead(factor_exp_score$scores)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        Factor1      Factor2\n[1,] -0.2661121 -0.201277588\n[2,] -0.2661121 -0.201277588\n[3,] -0.2661121 -0.201277588\n[4,] -0.2661121 -0.201277588\n[5,]  0.5989102 -0.002729162\n[6,] -0.2661121 -0.201277588\n```\n:::\n\n```{.r .cell-code}\nfactor_exp_comb <- bind_cols(factor_exp, data.frame(factor_exp_score$scores))\n\nfactor_exp_comb$class<-factor_exp_comb$Factor1\n\nfactor_exp_comb$media_grad<-factor_exp_comb$Factor2\n```\n:::\n\n\n### Histogram and descriptives for factor scores\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndescriptives(dat=factor_exp_comb, vars(Factor1, Factor2),\n             sd=T)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n DESCRIPTIVES\n\n Descriptives                                            \n ─────────────────────────────────────────────────────── \n                         Factor1         Factor2         \n ─────────────────────────────────────────────────────── \n   N                             1592             1592   \n   Missing                          0                0   \n   Mean                  8.761835e-16    -5.523220e-16   \n   Median                  -0.2661121       -0.2012776   \n   Standard deviation        1.025264        0.9950548   \n   Minimum                  -4.893420        -2.856758   \n   Maximum                   11.22177         20.89972   \n ─────────────────────────────────────────────────────── \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(factor_exp_comb$Factor1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(factor_exp_comb$Factor2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Standardize the Selected variables  ----\nvars_to_standardize <- c('know_lang','know_school','know_cards','know_breakfast',\n                         'know_weddings','know_teeth','know_traffic','know_tv',\n                         'high_lang', 'colle_lang', 'grad_lang',\n                         'numbook_lang','numarticle_lang',\n                         'high_school', 'colle_school', 'grad_school',\n                         'numbook_school','numarticle_school',\n                         'high_cards', 'colle_cards', 'grad_cards',\n                         'numbook_cards','numarticle_cards',\n                         'high_breakfast', 'colle_breakfast', 'grad_breakfast',\n                         'numbook_breakfast','numarticle_breakfast',\n                         'high_weddings', 'colle_weddings', 'grad_weddings',\n                         'numbook_weddings','numarticle_weddings',\n                         'high_teeth', 'colle_teeth', 'grad_teeth',\n                         'numbook_teeth','numarticle_teeth',\n                         'high_traffic', 'colle_traffic', 'grad_traffic',\n                         'numbook_traffic','numarticle_traffic',\n                         'high_tv', 'colle_tv', 'grad_tv',\n                         'numbook_tv','numarticle_tv')\n\nexpertise3_new[, vars_to_standardize] <- scale(expertise3_new[, vars_to_standardize])\n```\n:::\n\n\n## Long format (with factor scores)\n\nLong format of expertise dataset\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary('reshape2')\n\nexpertise3_long <- melt(expertise3_new, id.vars = c(\"ID\",'duration',\"sex\" ,\"education\" ,\"income\",\"religion\",'identity','age','political_atti','english_level','needforcog'), \n                        measure.vars = c(\"ih_lang\", \"ih_school\",\"ih_cards\", \"ih_breakfast\", \"ih_weddings\", \n                                         \"ih_teeth\", \"ih_traffic\", \"ih_tv\",'know_lang','know_school','know_cards','know_breakfast',\n                                         'know_weddings','know_teeth','know_traffic','know_tv',\n                                         'high_lang', 'colle_lang', 'grad_lang',\n                                         'numbook_lang','numarticle_lang',\n                                         'high_school', 'colle_school', 'grad_school',\n                                         'numbook_school','numarticle_school',\n                                         'high_cards', 'colle_cards', 'grad_cards',\n                                         'numbook_cards','numarticle_cards',\n                                         'high_breakfast', 'colle_breakfast', 'grad_breakfast',\n                                         'numbook_breakfast','numarticle_breakfast',\n                                         'high_weddings', 'colle_weddings', 'grad_weddings',\n                                         'numbook_weddings','numarticle_weddings',\n                                         'high_teeth', 'colle_teeth', 'grad_teeth',\n                                         'numbook_teeth','numarticle_teeth',\n                                         'high_traffic', 'colle_traffic', 'grad_traffic',\n                                         'numbook_traffic','numarticle_traffic',\n                                         'high_tv', 'colle_tv', 'grad_tv',\n                                         'numbook_tv','numarticle_tv'),\n                        sep = \"_\", variable.name = \"Category\", value.name = \"Score\")\n\n# Split the Category column into two columns based on the underscore separator\nexpertise3_long <- expertise3_long %>% separate(Category, into = c(\"Category\", \"Score_Type\"), sep = \"_\")\n\n## spread the data from long to wide format  ----\nexpertise3_ready <- expertise3_long %>% spread(Category, Score)\n\n# change the score type to a factor\nexpertise3_ready$Score_Type<-as.factor(expertise3_ready$Score_Type)\n\n# convert the column ih to numeric\nexpertise3_ready$ih<-as.numeric(expertise3_ready$ih)\n\n# Combine factor scores with the final data ----\nexpertise3_ready<-bind_cols(expertise3_ready,factor_exp_comb$Factor1,factor_exp_comb$Factor2)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nNew names:\n• `` -> `...20`\n• `` -> `...21`\n```\n:::\n\n```{.r .cell-code}\n#Rename the combined factor variables\nexpertise3_ready<-rename(expertise3_ready, classroom=...20)\nexpertise3_ready<-rename(expertise3_ready, media_grad=...21)\n```\n:::\n\n\n## Models\n\nWe can start to analyze our models with using hlm:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Import packages ----\nlibrary(lme4) \n\nlibrary(lmerTest)\n\n# Center variables: need for cognition and know \n\nexpertise3_ready$know_cent <- scale(expertise3_ready$know, center = TRUE, scale = FALSE)\n\nexpertise3_ready$needforcog_cent <- scale(expertise3_ready$needforcog, center = TRUE, scale = FALSE)\n\nexpertise3_ready$class_cent <- scale(expertise3_ready$classroom, center = TRUE, scale = FALSE)\n\nexpertise3_ready$media_cent <- scale(expertise3_ready$media_grad, center = TRUE, scale = FALSE)\n\n# Check descriptives\n\ndescribe(expertise3_ready)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                vars    n   mean    sd median trimmed   mad   min    max  range\nID                 1 1592 100.00 57.46 100.00  100.00 74.13  1.00 199.00 198.00\nduration*          2 1592  91.63 50.67  92.00   91.91 63.75  1.00 180.00 179.00\nsex*               3 1592   1.51  0.50   2.00    1.52  0.00  1.00   2.00   1.00\neducation*         4 1592   3.42  0.95   3.00    3.40  1.48  1.00   6.00   5.00\nincome*            5 1576  47.16 27.29  46.00   47.15 31.13  1.00  98.00  97.00\nreligion*          6 1576  25.15 15.95  19.00   25.11 19.27  1.00  55.00  54.00\nidentity*          7 1592  22.85  8.81  29.00   24.10  1.48  1.00  31.00  30.00\nage*               8 1592  16.17 10.30  13.00   15.21  8.90  1.00  43.00  42.00\npolitical_atti*    9 1592   3.93  2.15   4.00    3.81  2.97  1.00   9.00   8.00\nenglish_level*    10 1592   1.03  0.22   1.00    1.00  0.00  1.00   3.00   2.00\nneedforcog        11 1592   5.96  1.29   5.89    5.96  1.32  2.11   8.78   6.67\nScore_Type*       12 1592   4.50  2.29   4.50    4.50  2.97  1.00   8.00   7.00\ncolle             13 1592   0.00  1.00  -0.23   -0.23  0.06 -0.54   9.94  10.48\ngrad              14 1393   0.00  1.00  -0.10   -0.11  0.02 -0.15  14.04  14.19\nhigh              15 1592   0.00  1.00  -0.25   -0.22  0.13 -0.64  12.21  12.85\nih                16 1591   4.87  1.89   5.00    4.87  1.48  1.00   9.00   8.00\nknow              17 1591   0.00  1.00   0.01    0.00  1.13 -2.66   2.96   5.62\nnumarticle        18 1592   0.00  1.00  -0.12   -0.14  0.17 -0.41  13.91  14.32\nnumbook           19 1592   0.00  1.00  -0.25   -0.22  0.13 -0.49  11.22  11.71\nclassroom         20 1592   0.00  1.03  -0.27   -0.21  0.00 -4.89  11.22  16.12\nmedia_grad        21 1592   0.00  1.00  -0.20   -0.16  0.00 -2.86  20.90  23.76\nknow_cent         22 1591   0.00  1.00   0.01    0.00  1.13 -2.66   2.96   5.62\nneedforcog_cent   23 1592   0.00  1.29  -0.07    0.00  1.32 -3.85   2.82   6.67\nclass_cent        24 1592   0.00  1.03  -0.27   -0.21  0.00 -4.89  11.22  16.12\nmedia_cent        25 1592   0.00  1.00  -0.20   -0.16  0.00 -2.86  20.90  23.76\n                 skew kurtosis   se\nID               0.00    -1.20 1.44\nduration*       -0.04    -1.15 1.27\nsex*            -0.05    -2.00 0.01\neducation*       0.15    -0.17 0.02\nincome*         -0.01    -1.03 0.69\nreligion*        0.17    -1.38 0.40\nidentity*       -0.84    -0.82 0.22\nage*             0.73    -0.40 0.26\npolitical_atti*  0.36    -0.75 0.05\nenglish_level*   7.84    62.72 0.01\nneedforcog      -0.12    -0.08 0.03\nScore_Type*      0.00    -1.24 0.06\ncolle            5.16    31.98 0.03\ngrad            10.74   122.11 0.03\nhigh             5.23    37.74 0.03\nih               0.01    -0.40 0.05\nknow            -0.05    -0.62 0.03\nnumarticle      10.24   121.66 0.03\nnumbook          5.86    44.27 0.03\nclassroom        4.65    29.82 0.03\nmedia_grad      10.27   159.11 0.02\nknow_cent       -0.05    -0.62 0.03\nneedforcog_cent -0.12    -0.08 0.03\nclass_cent       4.65    29.82 0.03\nmedia_cent      10.27   159.11 0.02\n```\n:::\n:::\n\n\n### Model of objective expertise, perceived expertise and need for cognition without interaction\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Model 1 ----\n\n# Need for cognition and perceived expertise as fixed effects\nModel.1<-lmer(ih ~class_cent + media_cent + know_cent + needforcog_cent +(1|Score_Type)+(1|ID),   \n              data=expertise3_ready)\nsummary(Model.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: ih ~ class_cent + media_cent + know_cent + needforcog_cent +  \n    (1 | Score_Type) + (1 | ID)\n   Data: expertise3_ready\n\nREML criterion at convergence: 6300.3\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-3.03870 -0.66109 -0.02984  0.61030  2.85090 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.4983   0.7059  \n Score_Type (Intercept) 0.3471   0.5891  \n Residual               2.6815   1.6375  \nNumber of obs: 1590, groups:  ID, 199; Score_Type, 8\n\nFixed effects:\n                  Estimate Std. Error         df t value Pr(>|t|)    \n(Intercept)        4.87046    0.21811    7.77813  22.330 2.47e-08 ***\nclass_cent         0.02900    0.04777 1569.58075   0.607   0.5439    \nmedia_cent        -0.07888    0.04758 1576.28023  -1.658   0.0976 .  \nknow_cent          0.10044    0.04962 1366.14963   2.024   0.0431 *  \nneedforcog_cent   -0.20573    0.05048  196.31885  -4.076 6.66e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) clss_c md_cnt knw_cn\nclass_cent   0.000                     \nmedia_cent   0.000  0.288              \nknow_cent    0.000 -0.107 -0.216       \nnedfrcg_cnt  0.000 -0.025 -0.052 -0.012\n```\n:::\n\n```{.r .cell-code}\nconfint(Model.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                       2.5 %     97.5 %\n.sig01           0.587868486  0.8255644\n.sig02           0.350482884  1.0048383\n.sigma           1.576916986  1.6990007\n(Intercept)      4.419584771  5.3213676\nclass_cent      -0.064079579  0.1231931\nmedia_cent      -0.172238855  0.0142051\nknow_cent        0.003084828  0.1980084\nneedforcog_cent -0.304733534 -0.1066888\n```\n:::\n:::\n\n\nSo it seems that perceived knowledge and need for cognition are related to inherence significantly. Interestingly, need for cognition interaction is negatively related to inherence scores.People seek out tasks that challenge their abilities may show less inherent bias contained explanations and if they think they know that area, they agree with inherence bias included sentences more. IPeople may overestimate their abilities and they produce/comprehend the explanations with inherence bias more likely. Not suprisingly, people who are seeking for cognitive activities (need for cognition) may also show more effort for the explanations.\n\n### Model of objective expertise, perceived expertise and need for cognition with interaction\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Model 2 ----\n\nModel.2<-lmer(ih ~class_cent + media_cent + know_cent + needforcog_cent + \n                needforcog*class_cent + needforcog*media_cent + needforcog*know +\n                (1|Score_Type)+(1|ID), data=expertise3_ready)\nsummary(Model.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: ih ~ class_cent + media_cent + know_cent + needforcog_cent +  \n    needforcog * class_cent + needforcog * media_cent + needforcog *  \n    know + (1 | Score_Type) + (1 | ID)\n   Data: expertise3_ready\n\nREML criterion at convergence: 6302.9\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.9030 -0.6610 -0.0184  0.6123  3.1697 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.4977   0.7055  \n Score_Type (Intercept) 0.3477   0.5896  \n Residual               2.6669   1.6331  \nNumber of obs: 1590, groups:  ID, 199; Score_Type, 8\n\nFixed effects:\n                        Estimate Std. Error         df t value Pr(>|t|)    \n(Intercept)              4.86968    0.21831    7.78120  22.306 2.47e-08 ***\nclass_cent               0.35793    0.23812 1538.91170   1.503   0.1330    \nmedia_cent              -0.47494    0.30060 1563.54525  -1.580   0.1143    \nknow_cent                0.70578    0.23208 1375.72760   3.041   0.0024 ** \nneedforcog_cent         -0.21651    0.05055  198.18020  -4.283 2.87e-05 ***\nclass_cent:needforcog   -0.05398    0.03763 1535.11456  -1.435   0.1516    \nmedia_cent:needforcog    0.06650    0.04749 1565.74090   1.400   0.1617    \nneedforcog:know         -0.10285    0.03828 1415.00120  -2.687   0.0073 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) clss_c md_cnt knw_cn ndfrc_ clss_: md_cn:\nclass_cent   0.007                                          \nmedia_cent   0.019  0.211                                   \nknow_cent    0.002 -0.108 -0.238                            \nnedfrcg_cnt -0.001 -0.054 -0.021 -0.052                     \nclss_cnt:nd -0.006 -0.980 -0.186  0.104  0.050              \nmd_cnt:ndfr -0.019 -0.190 -0.987  0.248  0.012  0.174       \nnedfrcg:knw -0.002  0.105  0.250 -0.977  0.050 -0.106 -0.268\nfit warnings:\nfixed-effect model matrix is rank deficient so dropping 2 columns / coefficients\n```\n:::\n\n```{.r .cell-code}\nconfint(Model.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                            2.5 %      97.5 %\n.sig01                 0.58704091  0.82394772\n.sig02                 0.35088142  1.00567836\n.sigma                 1.57112583  1.69276158\n(Intercept)            4.41848217  5.32097565\nclass_cent            -0.10748632  0.82468080\nmedia_cent            -1.06371448  0.11322395\nknow_cent              0.25131285  1.15943906\nneedforcog_cent       -0.31555682 -0.11742931\nclass_cent:needforcog -0.12764196  0.01964830\nmedia_cent:needforcog -0.02645455  0.15950956\nneedforcog:know       -0.17764073 -0.02788099\n```\n:::\n:::\n\n\n## Histograms for Factors\n\n\n::: {.cell}\n\n```{.r .cell-code}\n##Histogram, descriptives and correlation matrix for new factors ----\ndescriptives(dat=factor_exp_comb, vars(Factor1, Factor2),\n             sd=T, skew =T)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n DESCRIPTIVES\n\n Descriptives                                             \n ──────────────────────────────────────────────────────── \n                          Factor1         Factor2         \n ──────────────────────────────────────────────────────── \n   N                              1592             1592   \n   Missing                           0                0   \n   Mean                   8.761835e-16    -5.523220e-16   \n   Median                   -0.2661121       -0.2012776   \n   Standard deviation         1.025264        0.9950548   \n   Minimum                   -4.893420        -2.856758   \n   Maximum                    11.22177         20.89972   \n   Skewness                   4.661496         10.28496   \n   Std. error skewness      0.06133318       0.06133318   \n ──────────────────────────────────────────────────────── \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(factor_exp_comb$Factor1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(factor_exp_comb$Factor2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\n\nSo our descriptive stats and graphs showed there is an extreme skewness due to extreme values in our factors.\n\n## Winsorize the variables at 1%\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# winsorize the variables (at 1%)\nexpertise3_ready <- expertise3_ready%>%\n  mutate(numarticle_wins_1=Winsorize(numarticle, probs = c(0,0.99)), \n         numbook_wins_1=Winsorize(numbook, probs = c(0,0.99)),\n         high_wins_1=Winsorize(high, probs = c(0,0.99)),\n         colle_wins_1=Winsorize(colle, probs = c(0,0.99)),\n         grad_wins_1=Winsorize(grad,na.rm=TRUE, probs = c(0,0.99)),\n         classroom_wins_1=Winsorize(classroom,na.rm=TRUE, probs = c(0,0.99)),\n         media_wins_1=Winsorize(media_grad,na.rm=TRUE, probs = c(0,0.99)))\n\n#check descriptives\ndescriptives(dat=expertise3_ready, vars(classroom_wins_1,media_wins_1), median=F, n=F, missing=F, sd=T, skew =T)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n DESCRIPTIVES\n\n Descriptives                                                \n ─────────────────────────────────────────────────────────── \n                          classroom_wins_1    media_wins_1   \n ─────────────────────────────────────────────────────────── \n   Mean                        -0.01922705     -0.04075412   \n   Standard deviation            0.8935287       0.5996877   \n   Minimum                       -4.893420       -2.856758   \n   Maximum                        5.029690        3.427757   \n   Skewness                       3.471803        3.585548   \n   Std. error skewness          0.06133318      0.06133318   \n ─────────────────────────────────────────────────────────── \n```\n:::\n\n```{.r .cell-code}\ndescriptives(dat=expertise3_ready, vars(high_wins_1, colle_wins_1), median=F, n=F, missing=F, sd=T)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n DESCRIPTIVES\n\n Descriptives                                          \n ───────────────────────────────────────────────────── \n                         high_wins_1    colle_wins_1   \n ───────────────────────────────────────────────────── \n   Mean                  -0.02454199     -0.01854773   \n   Standard deviation      0.8226327       0.8666794   \n   Minimum                -0.6447821      -0.5405821   \n   Maximum                  4.341215        5.110902   \n ───────────────────────────────────────────────────── \n```\n:::\n\n```{.r .cell-code}\ndescriptives(dat=expertise3_ready, vars(numarticle_wins_1, numbook_wins_1, grad_wins_1), median=F, n=F, missing=F, sd=T)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n DESCRIPTIVES\n\n Descriptives                                                                 \n ──────────────────────────────────────────────────────────────────────────── \n                         numarticle_wins_1    numbook_wins_1    grad_wins_1   \n ──────────────────────────────────────────────────────────────────────────── \n   Mean                        -0.05220288       -0.02881345    -0.06591236   \n   Standard deviation            0.4758284         0.7815089      0.3238198   \n   Minimum                      -0.4101997        -0.4928124     -0.1541325   \n   Maximum                        3.159021          4.500323       2.798505   \n ──────────────────────────────────────────────────────────────────────────── \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(expertise3_ready$classroom_wins_1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n:::\n\n\nOur first factor is negatively skewed\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(expertise3_ready$media_wins_1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Center variables: \n\nexpertise3_ready$classroom_wins_1_cent <- scale(expertise3_ready$classroom_wins_1, center = TRUE, scale = FALSE)\n\nexpertise3_ready$media_wins_1_cent <- scale(expertise3_ready$media_wins_1, center = TRUE, scale = FALSE)\n```\n:::\n\n\n## HLM Models\n\nSince expertise can be objective (based on the experience like problem solving in that area or productions, year of education or spending more time about that topic) and subjective (how much people perceived themselves as knowledgeable about that topic), objective expertise or real expertise will be negatively related to inherence scores, which means that real experts, which spend more time to learn that subject through courses or pressed materials -books, magazines, articles-, will not satisfy with heuristic explanations. Experts produced more features in diagnostic categories like to describe depression, they listed more attributes for this category (Murphy & Wright, 1984). Therefore, it can be argued that they will evaluate the cases with both intrinsic and extrinsic factors, so they have less scores for inherency.\n\nBesides this, our expertise variables and engaging in cognitive activities may be interact because expertise sources such as books or magazines also contain products of heuristic thinking. People who have low scores in need for cognition trusted these kind of external sources besides cognitive heuristics:\n\n\"Research relating need for cognition to other individual-differences variables provides evidence that individuals high in need for cognition naturally tend to seek, acquire, think about, and reflect back on information to make sense of stimuli, relationships, and events in their world; individuals low in need for cognition, in contrast, are more likely to rely on others (e.g., experts), cognitive heuristics, or social comparison processes to provide this structure.\" (Cacioppo et al., 1996, p.243)\n\nAs a result of this, even if people who read more books, magazines or take more grad courses, they would probably satisfy with sentences that include inherence bias more or less depend on their need for cognition score, so the assumption is the interaction between need for cognition and this expertise factor will be significantly related to inherence scores.\n\n### Model objective expertise with grad courses, books and magazines\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Model objective expertise ----\n\nModel_obj<-lmer(ih ~media_wins_1_cent*needforcog_cent+media_wins_1_cent+needforcog_cent+\n                (1|Score_Type)+(1|ID), data=expertise3_ready)\nsummary(Model_obj)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: ih ~ media_wins_1_cent * needforcog_cent + media_wins_1_cent +  \n    needforcog_cent + (1 | Score_Type) + (1 | ID)\n   Data: expertise3_ready\n\nREML criterion at convergence: 6298.7\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-3.05748 -0.65994 -0.01794  0.60242  2.91751 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.5217   0.7223  \n Score_Type (Intercept) 0.3420   0.5848  \n Residual               2.6705   1.6342  \nNumber of obs: 1591, groups:  ID, 199; Score_Type, 8\n\nFixed effects:\n                                    Estimate Std. Error         df t value\n(Intercept)                          4.86046    0.21698    7.84928  22.401\nmedia_wins_1_cent                   -0.19370    0.08355 1583.44045  -2.318\nneedforcog_cent                     -0.19923    0.05118  198.62257  -3.893\nmedia_wins_1_cent:needforcog_cent    0.11493    0.06318 1574.07118   1.819\n                                  Pr(>|t|)    \n(Intercept)                       2.14e-08 ***\nmedia_wins_1_cent                 0.020550 *  \nneedforcog_cent                   0.000135 ***\nmedia_wins_1_cent:needforcog_cent 0.069067 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) md__1_ ndfrc_\nmd_wns_1_cn  0.009              \nnedfrcg_cnt  0.000 -0.070       \nmd_wns_1_:_ -0.023 -0.390  0.009\n```\n:::\n:::\n\n\nThe results showed that not the interaction but main effects significantly related to inherence scores. It's maybe because of low power to detect interaction or because people with high need for cognition scores also take these courses and read these books due to their tendencies, so there is not an interaction. Media and grad courses expertise is related to inherence scores negatively, adjusting for need for cognition scores. I think these results may tap to the role of memory limitations in heuristic thinking. If experts can produce more hypotheses with using their long term memory, the recent memory traces would be remembered probably better. Especially grad courses and also books and magazines seem to related this. Furthermore, especially grad courses again, can be more challenging for the related area. For example, a course related to language may be challenging or understanding books required more attention to details and different backgrounds about these topics so these people already have an expanded mental shotgun area/stage. Inherence bias occurs as a result of memory limitations, so having active memory traces (and broad mental shotgun representations) may provide these people to think beyond heuristic thinking with inherence bias.\n\nI would expect the same result with high school and college courses, the relation may be more weak because these courses may not be recent, so let's check the model:\n\n### Model objective expertise with high school and college courses\n\n\n::: {.cell}\n\n```{.r .cell-code}\nModel_obj2<-lmer(ih ~classroom_wins_1_cent * needforcog_cent +classroom_wins_1_cent + needforcog_cent+(1|Score_Type)+(1|ID), data=expertise3_ready)\nsummary(Model_obj2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: \nih ~ classroom_wins_1_cent * needforcog_cent + classroom_wins_1_cent +  \n    needforcog_cent + (1 | Score_Type) + (1 | ID)\n   Data: expertise3_ready\n\nREML criterion at convergence: 6299.3\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-3.07377 -0.66531 -0.01234  0.60541  2.92029 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.5238   0.7237  \n Score_Type (Intercept) 0.3451   0.5875  \n Residual               2.6679   1.6334  \nNumber of obs: 1591, groups:  ID, 199; Score_Type, 8\n\nFixed effects:\n                                        Estimate Std. Error         df t value\n(Intercept)                              4.87082    0.21783    7.82515  22.361\nclassroom_wins_1_cent                    0.05744    0.05285 1560.84930   1.087\nneedforcog_cent                         -0.21340    0.05116  197.59997  -4.171\nclassroom_wins_1_cent:needforcog_cent   -0.10805    0.04312 1540.95311  -2.506\n                                      Pr(>|t|)    \n(Intercept)                           2.26e-08 ***\nclassroom_wins_1_cent                   0.2773    \nneedforcog_cent                       4.53e-05 ***\nclassroom_wins_1_cent:needforcog_cent   0.0123 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cl__1_ ndfrc_\nclssrm_w_1_  0.000              \nnedfrcg_cnt  0.000 -0.009       \nclssr__1_:_ -0.002 -0.039  0.051\n```\n:::\n:::\n\n\nThese results showed that the interaction is significant, so the relation between class and inherence scores is depend on need for cognition scores. However, since the trend of the relation between class and inherence scores is positive, this interaction seems negative, so need for cognition mitigated this relation. Commenting this counter intuitive result may be a better way to understand this interaction:\n\nMain effect of class is not significant (the relation between high school and undergraduate courses and inherence scores). This may be as a result of not challenging nature of these courses or they may not be remembered very well, if these people didn't interest in them later. The positive trend may be a product of perceived expertise, so people who take these courses think themselves as experts but they are not. However if they have tendency to engage in cognitive activities, they still trust cognitive heuristics less, so need for cognition interact with this relation.\n\nWhat about perceived expertise? I think perceived expertise may be misleading for people because especially for daily topics like breakfast, they may think that they know enough but their knowledge is limited (and they produce less diverse facts during mental shotgun stage). However, the need for cognition also moderates this relation. Let's check:\n\n### Model of perceived expertise and need for cognition\n\n\n::: {.cell}\n\n```{.r .cell-code}\nModel_perc<-lmer(ih ~know_cent*needforcog_cent+ know_cent + needforcog_cent +\n                (1|Score_Type)+(1|ID), data=expertise3_ready)\nsummary(Model_perc)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: ih ~ know_cent * needforcog_cent + know_cent + needforcog_cent +  \n    (1 | Score_Type) + (1 | ID)\n   Data: expertise3_ready\n\nREML criterion at convergence: 6293.4\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.94526 -0.66336 -0.01584  0.61110  2.90351 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.4960   0.7043  \n Score_Type (Intercept) 0.3643   0.6036  \n Residual               2.6744   1.6354  \nNumber of obs: 1590, groups:  ID, 199; Score_Type, 8\n\nFixed effects:\n                            Estimate Std. Error         df t value Pr(>|t|)    \n(Intercept)                  4.87459    0.22298    7.75612  21.861 3.01e-08 ***\nknow_cent                    0.08693    0.04832 1357.80267   1.799  0.07227 .  \nneedforcog_cent             -0.21666    0.05038  196.42283  -4.301 2.68e-05 ***\nknow_cent:needforcog_cent   -0.09801    0.03666 1399.78419  -2.674  0.00759 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) knw_cn ndfrc_\nknow_cent    0.000              \nnedfrcg_cnt -0.001 -0.025       \nknw_cnt:nd_ -0.007 -0.012  0.053\n```\n:::\n:::\n\n\nThese results are similar to previous one and the relation between perceived knowledge and inherence scores moderated by need for cognition, which means people may overinterpret their knowledge and tend to think heuristically more due to this, but their tendency to engage in cognitive activities change this relation, if they like to engage in cognitive activities more, they satisfy with inherent explanations less even if they overestimate their knowledge about that area. This result partially support and partially conflict with Pennycook et al. (2017). Partially supported because just like that research, we found that people are unaware of their own inherence bias and this may be one of the reason behind inherence bias. However, they also found that Dunning-Kruger effects influence their need for cognition scores, so they also overestimate their need for cognition scores. Unlike my assumption here (the mitigating factor in engaging cognitive activities), their finding taps that perceived knowledge should be positively related to need for cognition:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncorrMatrix(expertise3_ready,\n           vars = vars(know_cent,needforcog_cent), flag = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n CORRELATION MATRIX\n\n Correlation Matrix                                                 \n ────────────────────────────────────────────────────────────────── \n                                     know_cent    needforcog_cent   \n ────────────────────────────────────────────────────────────────── \n   know_cent          Pearson's r            —                      \n                      p-value                —                      \n                                                                    \n   needforcog_cent    Pearson's r    0.0323268                  —   \n                      p-value        0.1974834                  —   \n ────────────────────────────────────────────────────────────────── \n   Note. * p < .05, ** p < .01, *** p < .001\n```\n:::\n:::\n\n\nHowever our scores showed that they are not related significantly. After all of these let's check our first model 1 and model 2 to adjust other variables:\n\n### Model of objective expertise, perceived expertise and need for cognition without interaction\n\n\n::: {.cell}\n\n```{.r .cell-code}\nModel.1.1<-lmer(ih ~classroom_wins_1_cent + media_wins_1_cent + know_cent +needforcog_cent +(1|Score_Type)+(1|ID),   \n              data=expertise3_ready)\nsummary(Model.1.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: ih ~ classroom_wins_1_cent + media_wins_1_cent + know_cent +  \n    needforcog_cent + (1 | Score_Type) + (1 | ID)\n   Data: expertise3_ready\n\nREML criterion at convergence: 6297.9\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-3.02285 -0.66284 -0.02625  0.61186  2.83745 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.4951   0.7037  \n Score_Type (Intercept) 0.3479   0.5898  \n Residual               2.6810   1.6374  \nNumber of obs: 1590, groups:  ID, 199; Score_Type, 8\n\nFixed effects:\n                        Estimate Std. Error         df t value Pr(>|t|)    \n(Intercept)              4.87045    0.21832    7.76987  22.309 2.52e-08 ***\nclassroom_wins_1_cent    0.01370    0.05555 1569.38189   0.247   0.8053    \nmedia_wins_1_cent       -0.17044    0.08309 1567.81620  -2.051   0.0404 *  \nknow_cent                0.11019    0.05019 1355.48286   2.195   0.0283 *  \nneedforcog_cent         -0.20200    0.05045  197.08317  -4.004 8.82e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cl__1_ md__1_ knw_cn\nclssrm_w_1_  0.000                     \nmd_wns_1_cn  0.000  0.304              \nknow_cent    0.000 -0.128 -0.263       \nnedfrcg_cnt  0.000 -0.028 -0.075 -0.004\n```\n:::\n:::\n\n\nThe same comments can be done also here, our results are not different than single models. This model may be better because it also adjusts perceived expertise and need for cognition, when the objective expertise variables are explained and vice versa. Therefore, our media and grad class related factor is still negatively related to inherence score when it was adjusted for perceived knowledge, class and need for cognition. Let's check the model with interaction:\n\n### Model of objective expertise, perceived expertise and need for cognition with interaction\n\n\n::: {.cell}\n\n```{.r .cell-code}\nModel.2.1<-lmer(ih ~classroom_wins_1_cent + media_wins_1_cent + know_cent+needforcog_cent + classroom_wins_1_cent*needforcog_cent + media_wins_1_cent*needforcog_cent + know_cent*needforcog_cent+(1|Score_Type)+(1|ID),   \n              data=expertise3_ready)\nsummary(Model.2.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: ih ~ classroom_wins_1_cent + media_wins_1_cent + know_cent +  \n    needforcog_cent + classroom_wins_1_cent * needforcog_cent +  \n    media_wins_1_cent * needforcog_cent + know_cent * needforcog_cent +  \n    (1 | Score_Type) + (1 | ID)\n   Data: expertise3_ready\n\nREML criterion at convergence: 6296.3\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.86875 -0.65764 -0.02032  0.60869  2.94463 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.4978   0.7056  \n Score_Type (Intercept) 0.3466   0.5888  \n Residual               2.6592   1.6307  \nNumber of obs: 1590, groups:  ID, 199; Score_Type, 8\n\nFixed effects:\n                                        Estimate Std. Error         df t value\n(Intercept)                            4.865e+00  2.180e-01  7.783e+00  22.316\nclassroom_wins_1_cent                  8.854e-03  5.611e-02  1.570e+03   0.158\nmedia_wins_1_cent                     -1.986e-01  9.123e-02  1.570e+03  -2.177\nknow_cent                              1.021e-01  5.010e-02  1.365e+03   2.039\nneedforcog_cent                       -2.148e-01  5.063e-02  1.993e+02  -4.242\nclassroom_wins_1_cent:needforcog_cent -7.424e-02  4.461e-02  1.547e+03  -1.664\nmedia_wins_1_cent:needforcog_cent      1.319e-01  6.806e-02  1.575e+03   1.938\nknow_cent:needforcog_cent             -1.048e-01  3.866e-02  1.408e+03  -2.712\n                                      Pr(>|t|)    \n(Intercept)                           2.46e-08 ***\nclassroom_wins_1_cent                  0.87462    \nmedia_wins_1_cent                      0.02960 *  \nknow_cent                              0.04166 *  \nneedforcog_cent                       3.38e-05 ***\nclassroom_wins_1_cent:needforcog_cent  0.09626 .  \nmedia_wins_1_cent:needforcog_cent      0.05277 .  \nknow_cent:needforcog_cent              0.00678 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cl__1_ md__1_ knw_cn ndfrc_ c__1_: m__1_:\nclssrm_w_1_  0.004                                          \nmd_wns_1_cn  0.010  0.339                                   \nknow_cent    0.001 -0.121 -0.226                            \nnedfrcg_cnt -0.001 -0.034 -0.083 -0.002                     \nclssr__1_:_ -0.007 -0.093 -0.160 -0.001  0.054              \nmd_wns_1_:_ -0.023 -0.150 -0.394 -0.040  0.010  0.245       \nknw_cnt:nd_  0.000  0.017 -0.005  0.034  0.055 -0.133 -0.288\n```\n:::\n:::\n\n\nThis total model is still similar to the single models unlike the interactions between objective expertise and need for cognition. Maybe we don't have enough power to detect these interactions due to sample size. We can still make the similar comments above I guess.\n\n#Winsorize at 1% the raw variables before factor analysis We can see that there are extreme values for our factor 1 (high+colle) and factor 2 (media+grad). We can deal these values with winsorization:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# winsorize the variables (at 1%)\nexpertise3_wins_fact <- expertise3_wins_fact%>%\n  mutate(numarticle_w1=Winsorize(numarticle, probs = c(0,0.99)), \n         numbook_w1=Winsorize(numbook, probs = c(0,0.99)),\n         high_w1=Winsorize(high, probs = c(0,0.99)),\n         colle_w1=Winsorize(colle, probs = c(0,0.99)),\n         grad_w1=Winsorize(grad,na.rm=TRUE, probs = c(0,0.99)))\n\n#check descriptives\ndescriptives(dat=expertise3_wins_fact, vars(high_w1, colle_w1), median=F, n=F, missing=F, sd=T)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n DESCRIPTIVES\n\n Descriptives                                     \n ──────────────────────────────────────────────── \n                         high_w1      colle_w1    \n ──────────────────────────────────────────────── \n   Mean                  0.3216080    0.3071608   \n   Standard deviation    0.9623120    0.9595196   \n   Minimum                0.000000     0.000000   \n   Maximum                6.000000     6.000000   \n ──────────────────────────────────────────────── \n```\n:::\n\n```{.r .cell-code}\ndescriptives(dat=expertise3_wins_fact, vars(numarticle_w1, numbook_w1, grad_w1), median=F, n=F, missing=F, sd=T)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n DESCRIPTIVES\n\n Descriptives                                                        \n ─────────────────────────────────────────────────────────────────── \n                         numarticle_w1    numbook_w1    grad_w1      \n ─────────────────────────────────────────────────────────────────── \n   Mean                       5.582915      1.098618    0.03203518   \n   Standard deviation         14.21353      3.060414     0.2316370   \n   Minimum                    0.000000      0.000000      0.000000   \n   Maximum                    100.0000      20.00000      2.000000   \n ─────────────────────────────────────────────────────────────────── \n```\n:::\n:::\n\n\nOur minimum and maximum values seems better, let's check histograms:\n\n## Factor analysis with winsorized variables\n\nFactor analysis for expertise variables with raw scores\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Factor analysis for expertise variables with raw scores ----\n\n# Import packages \n\nlibrary(psych) #PCA/EFA analysis\nlibrary(REdaS) #Produces KMO and Bartletts test\nlibrary(GPArotation)\n\n\n# Create a new dataframe that include only related variables\nfactor_exp1<-expertise3_wins_fact%>%\n  select(colle_w1, grad_w1, high_w1, numarticle_w1, numbook_w1)\n\n\n# Since grad classes for TV category is missing (nobody takes any class in the sample), listwise deletion is applied here.\nbart_spher(factor_exp1, use = \"complete.obs\") ###### produces Bartletts test of spherecity \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\tBartlett's Test of Sphericity\n\nCall: bart_spher(x = factor_exp1, use = \"complete.obs\")\n\n     X2 = 1655.614\n     df = 10\np-value < 2.22e-16\n```\n:::\n\n```{.r .cell-code}\n#Check eigenvalues\n\nfa.parallel(factor_exp1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-36-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nParallel analysis suggests that the number of factors =  3  and the number of components =  2 \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# So we can reduce it to 2 factors\nfa(factor_exp1, nfactors = 3, rotate =  \"oblimin\" )  \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFactor Analysis using method =  minres\nCall: fa(r = factor_exp1, nfactors = 3, rotate = \"oblimin\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n                MR1   MR2   MR3   h2   u2 com\ncolle_w1      -0.01  0.72  0.13 0.64 0.36 1.1\ngrad_w1        0.17  0.44 -0.31 0.24 0.76 2.2\nhigh_w1        0.09  0.22  0.60 0.59 0.41 1.3\nnumarticle_w1  0.76 -0.04 -0.10 0.55 0.45 1.0\nnumbook_w1     0.77  0.03  0.12 0.66 0.34 1.1\n\n                       MR1  MR2  MR3\nSS loadings           1.27 0.86 0.55\nProportion Var        0.25 0.17 0.11\nCumulative Var        0.25 0.43 0.54\nProportion Explained  0.47 0.32 0.20\nCumulative Proportion 0.47 0.80 1.00\n\n With factor correlations of \n     MR1  MR2  MR3\nMR1 1.00 0.52 0.09\nMR2 0.52 1.00 0.56\nMR3 0.09 0.56 1.00\n\nMean item complexity =  1.3\nTest of the hypothesis that 3 factors are sufficient.\n\nThe degrees of freedom for the null model are  10  and the objective function was  1.04 with Chi Square of  1655.61\nThe degrees of freedom for the model are -2  and the objective function was  0 \n\nThe root mean square of the residuals (RMSR) is  0 \nThe df corrected root mean square of the residuals is  NA \n\nThe harmonic number of observations is  1592 with the empirical chi square  0  with prob <  NA \nThe total number of observations was  1592  with Likelihood Chi Square =  0  with prob <  NA \n\nTucker Lewis Index of factoring reliability =  1.006\nFit based upon off diagonal values = 1\nMeasures of factor score adequacy             \n                                                   MR1  MR2  MR3\nCorrelation of (regression) scores with factors   0.88 0.85 0.78\nMultiple R square of scores with factors          0.77 0.73 0.60\nMinimum correlation of possible factor scores     0.54 0.45 0.20\n```\n:::\n\n```{.r .cell-code}\n# Figure for the analysis\n\nM2<-fa(factor_exp1, nfactors = 3, rotate =  \"oblimin\" ) ##save the analysis as the object m1\nfa.diagram(M2,main=\"Expert Variables\")  \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-37-1.png){width=672}\n:::\n:::\n\n\nEigenvalues suggest 2 components here\n\n### Extracting components\n\n\n::: {.cell}\n\n```{.r .cell-code}\nPrC_expert<- principal(factor_exp1, scores =TRUE, rotate= \"varimax\", nfactors=2, method=\"regression\")\n\nloadings (PrC_expert)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nLoadings:\n              RC1   RC2  \ncolle_w1      0.241 0.821\ngrad_w1       0.635      \nhigh_w1             0.894\nnumarticle_w1 0.849      \nnumbook_w1    0.760 0.350\n\n                 RC1   RC2\nSS loadings    1.759 1.600\nProportion Var 0.352 0.320\nCumulative Var 0.352 0.672\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#Combining data frames\n\nexpertise3_wins_fact1 <- cbind(expertise3_wins_fact, PrC_expert$scores)\n```\n:::\n\n\nCheck the correlation matrix again, it's the same above:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncorrMatrix(data = expertise3_wins_fact1, vars = vars(colle_w1, grad_w1, high_w1, numarticle_w1, numbook_w1, RC1, RC2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n CORRELATION MATRIX\n\n Correlation Matrix                                                                                                                   \n ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n                                   colle_w1      grad_w1       high_w1       numarticle_w1    numbook_w1    RC1           RC2         \n ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n   colle_w1         Pearson's r             —                                                                                         \n                    p-value                 —                                                                                         \n                                                                                                                                      \n   grad_w1          Pearson's r     0.2469764             —                                                                           \n                    p-value        < .0000001             —                                                                           \n                                                                                                                                      \n   high_w1          Pearson's r     0.5239629     0.0806383             —                                                             \n                    p-value        < .0000001     0.0012811             —                                                             \n                                                                                                                                      \n   numarticle_w1    Pearson's r     0.2062351     0.2759111     0.1050732                —                                            \n                    p-value        < .0000001    < .0000001     0.0000266                —                                            \n                                                                                                                                      \n   numbook_w1       Pearson's r     0.3807299     0.2925613     0.3087127        0.5716079             —                              \n                    p-value        < .0000001    < .0000001    < .0000001       < .0000001             —                              \n                                                                                                                                      \n   RC1              Pearson's r     0.2406258     0.6345799     0.0175533        0.8490179     0.7600788             —                \n                    p-value        < .0000001    < .0000001     0.4840040       < .0000001    < .0000001             —                \n                                                                                                                                      \n   RC2              Pearson's r     0.8206051     0.0651383     0.8941356        0.0297629     0.3496585    -0.0000000            —   \n                    p-value        < .0000001     0.0093297    < .0000001        0.2352812    < .0000001     1.0000000            —   \n ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n```\n:::\n:::\n\n\n## Histograms of these components\n\n\n::: {.cell}\n\n```{.r .cell-code}\n##Histogram, descriptives and correlation matrix for new factors ----\ndescriptives(dat=expertise3_wins_fact1, vars(RC1, RC2),\n             sd=T, skew =T)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n DESCRIPTIVES\n\n Descriptives                                              \n ───────────────────────────────────────────────────────── \n                          RC1              RC2             \n ───────────────────────────────────────────────────────── \n   N                               1592             1592   \n   Missing                            0                0   \n   Mean                   -4.315364e-16    -7.624415e-16   \n   Median                    -0.3404724       -0.3261946   \n   Standard deviation          1.000000         1.000000   \n   Minimum                    -1.478736        -1.747536   \n   Maximum                     9.432424         7.325695   \n   Skewness                    4.769103         3.671222   \n   Std. error skewness       0.06133318       0.06133318   \n ───────────────────────────────────────────────────────── \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(expertise3_wins_fact1$RC1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-42-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(expertise3_wins_fact1$RC2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-43-1.png){width=672}\n:::\n:::\n\n\n## Long format (with component scores)\n\nLong format of expertise dataset\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary('reshape2')\n\nexpertise3_wins_comp <- melt(expertise3_new, id.vars = c(\"ID\",'duration',\"sex\" ,\"education\" ,\"income\",\"religion\",'identity','age','political_atti','english_level','needforcog'), \n                        measure.vars = c(\"ih_lang\", \"ih_school\",\"ih_cards\", \"ih_breakfast\", \"ih_weddings\", \n                                         \"ih_teeth\", \"ih_traffic\", \"ih_tv\",'know_lang','know_school','know_cards','know_breakfast',\n                                         'know_weddings','know_teeth','know_traffic','know_tv',\n                                         'high_lang', 'colle_lang', 'grad_lang',\n                                         'numbook_lang','numarticle_lang',\n                                         'high_school', 'colle_school', 'grad_school',\n                                         'numbook_school','numarticle_school',\n                                         'high_cards', 'colle_cards', 'grad_cards',\n                                         'numbook_cards','numarticle_cards',\n                                         'high_breakfast', 'colle_breakfast', 'grad_breakfast',\n                                         'numbook_breakfast','numarticle_breakfast',\n                                         'high_weddings', 'colle_weddings', 'grad_weddings',\n                                         'numbook_weddings','numarticle_weddings',\n                                         'high_teeth', 'colle_teeth', 'grad_teeth',\n                                         'numbook_teeth','numarticle_teeth',\n                                         'high_traffic', 'colle_traffic', 'grad_traffic',\n                                         'numbook_traffic','numarticle_traffic',\n                                         'high_tv', 'colle_tv', 'grad_tv',\n                                         'numbook_tv','numarticle_tv'),\n                        sep = \"_\", variable.name = \"Category\", value.name = \"Score\")\n\n# Split the Category column into two columns based on the underscore separator\nexpertise3_wins_comp <- expertise3_wins_comp %>% separate(Category, into = c(\"Category\", \"Score_Type\"), sep = \"_\")\n\n## spread the data from long to wide format  ----\nexpertise3_win1_comp <- expertise3_wins_comp %>% spread(Category, Score)\n\n# change the score type to a factor\nexpertise3_win1_comp$Score_Type<-as.factor(expertise3_win1_comp$Score_Type)\n\n# convert the column ih to numeric\nexpertise3_win1_comp$ih<-as.numeric(expertise3_win1_comp$ih)\n\n# Combine factor scores with the final data ----\nexpertise3_win1_comp<-bind_cols(expertise3_win1_comp,expertise3_wins_fact1$RC1,expertise3_wins_fact1$RC2)\n\n#Rename the combined factor variables\nexpertise3_win1_comp<-rename(expertise3_win1_comp, PC1=...20)\nexpertise3_win1_comp<-rename(expertise3_win1_comp, PC2=...21)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Center variables: \n\nexpertise3_win1_comp$know_cent <- scale(expertise3_win1_comp$know, center = TRUE, scale = FALSE)\n\nexpertise3_win1_comp$needforcog_cent <- scale(expertise3_win1_comp$needforcog, center = TRUE, scale = FALSE)\n\nexpertise3_win1_comp$PC1_w1_cent <- scale(expertise3_win1_comp$PC1, center = TRUE, scale = FALSE)\n\nexpertise3_win1_comp$PC2_w2_cent <- scale(expertise3_win1_comp$PC2, center = TRUE, scale = FALSE)\n```\n:::\n\n\n## HLM Models\n\n### Model objective expertise with grad courses, books and magazines\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Model objective expertise ----\n\nModel_obj<-lmer(ih ~PC1_w1_cent*needforcog_cent+PC1_w1_cent+needforcog_cent+\n                (1|Score_Type)+(1|ID), data=expertise3_win1_comp)\nsummary(Model_obj)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: ih ~ PC1_w1_cent * needforcog_cent + PC1_w1_cent + needforcog_cent +  \n    (1 | Score_Type) + (1 | ID)\n   Data: expertise3_win1_comp\n\nREML criterion at convergence: 6305.3\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-3.09646 -0.65141 -0.01162  0.60478  2.89691 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.5242   0.7240  \n Score_Type (Intercept) 0.3478   0.5897  \n Residual               2.6776   1.6363  \nNumber of obs: 1591, groups:  ID, 199; Score_Type, 8\n\nFixed effects:\n                              Estimate Std. Error         df t value Pr(>|t|)\n(Intercept)                    4.86635    0.21867    7.83496  22.255  2.3e-08\nPC1_w1_cent                   -0.07447    0.05599 1532.07904  -1.330 0.183697\nneedforcog_cent               -0.20158    0.05130  198.61611  -3.929 0.000118\nPC1_w1_cent:needforcog_cent    0.02305    0.03721 1578.32515   0.619 0.535820\n                               \n(Intercept)                 ***\nPC1_w1_cent                    \nneedforcog_cent             ***\nPC1_w1_cent:needforcog_cent    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) PC1_w1_ ndfrc_\nPC1_w1_cent  0.012               \nnedfrcg_cnt  0.000 -0.070        \nPC1_w1_cn:_ -0.024 -0.521   0.006\n```\n:::\n:::\n\n\n### Model objective expertise with high school and college courses\n\n\n::: {.cell}\n\n```{.r .cell-code}\nModel_obj2<-lmer(ih ~PC2_w2_cent * needforcog_cent +PC2_w2_cent + needforcog_cent+(1|Score_Type)+(1|ID), data=expertise3_win1_comp)\nsummary(Model_obj2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: ih ~ PC2_w2_cent * needforcog_cent + PC2_w2_cent + needforcog_cent +  \n    (1 | Score_Type) + (1 | ID)\n   Data: expertise3_win1_comp\n\nREML criterion at convergence: 6300.7\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-3.09277 -0.66309 -0.01215  0.60398  2.91818 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.5198   0.7210  \n Score_Type (Intercept) 0.3404   0.5834  \n Residual               2.6715   1.6345  \nNumber of obs: 1591, groups:  ID, 199; Score_Type, 8\n\nFixed effects:\n                              Estimate Std. Error         df t value Pr(>|t|)\n(Intercept)                    4.87325    0.21643    7.83004  22.516 2.12e-08\nPC2_w2_cent                    0.08026    0.04831 1567.99193   1.661   0.0968\nneedforcog_cent               -0.21295    0.05105  197.64379  -4.171 4.54e-05\nPC2_w2_cent:needforcog_cent   -0.07549    0.03797 1558.32191  -1.988   0.0470\n                               \n(Intercept)                 ***\nPC2_w2_cent                 .  \nneedforcog_cent             ***\nPC2_w2_cent:needforcog_cent *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) PC2_w2_ ndfrc_\nPC2_w2_cent  0.001               \nnedfrcg_cnt  0.000 -0.030        \nPC2_w2_cn:_ -0.008 -0.068   0.042\n```\n:::\n:::\n\n\n### Model of perceived expertise and need for cognition\n\n\n::: {.cell}\n\n```{.r .cell-code}\nModel_perc<-lmer(ih ~know_cent*needforcog_cent+ know_cent + needforcog_cent +\n                (1|Score_Type)+(1|ID), data=expertise3_win1_comp)\nsummary(Model_perc)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: ih ~ know_cent * needforcog_cent + know_cent + needforcog_cent +  \n    (1 | Score_Type) + (1 | ID)\n   Data: expertise3_win1_comp\n\nREML criterion at convergence: 6293.4\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.94526 -0.66336 -0.01584  0.61110  2.90351 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.4960   0.7043  \n Score_Type (Intercept) 0.3643   0.6036  \n Residual               2.6744   1.6354  \nNumber of obs: 1590, groups:  ID, 199; Score_Type, 8\n\nFixed effects:\n                            Estimate Std. Error         df t value Pr(>|t|)    \n(Intercept)                  4.87459    0.22298    7.75612  21.861 3.01e-08 ***\nknow_cent                    0.08693    0.04832 1357.80267   1.799  0.07227 .  \nneedforcog_cent             -0.21666    0.05038  196.42283  -4.301 2.68e-05 ***\nknow_cent:needforcog_cent   -0.09801    0.03666 1399.78419  -2.674  0.00759 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) knw_cn ndfrc_\nknow_cent    0.000              \nnedfrcg_cnt -0.001 -0.025       \nknw_cnt:nd_ -0.007 -0.012  0.053\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncorrMatrix(expertise3_win1_comp,\n           vars = vars(know_cent,needforcog_cent), flag = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n CORRELATION MATRIX\n\n Correlation Matrix                                                 \n ────────────────────────────────────────────────────────────────── \n                                     know_cent    needforcog_cent   \n ────────────────────────────────────────────────────────────────── \n   know_cent          Pearson's r            —                      \n                      p-value                —                      \n                                                                    \n   needforcog_cent    Pearson's r    0.0323268                  —   \n                      p-value        0.1974834                  —   \n ────────────────────────────────────────────────────────────────── \n   Note. * p < .05, ** p < .01, *** p < .001\n```\n:::\n:::\n\n\n### Model of objective expertise, perceived expertise and need for cognition without interaction\n\n\n::: {.cell}\n\n```{.r .cell-code}\nModel.1.1<-lmer(ih ~PC1_w1_cent + PC2_w2_cent + know_cent +needforcog_cent +(1|Score_Type)+(1|ID),   \n              data=expertise3_win1_comp)\nsummary(Model.1.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: ih ~ PC1_w1_cent + PC2_w2_cent + know_cent + needforcog_cent +  \n    (1 | Score_Type) + (1 | ID)\n   Data: expertise3_win1_comp\n\nREML criterion at convergence: 6299.9\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-3.04789 -0.66791 -0.03113  0.60644  2.84714 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.4987   0.7062  \n Score_Type (Intercept) 0.3401   0.5832  \n Residual               2.6813   1.6375  \nNumber of obs: 1590, groups:  ID, 199; Score_Type, 8\n\nFixed effects:\n                  Estimate Std. Error         df t value Pr(>|t|)    \n(Intercept)        4.87050    0.21611    7.78797  22.537 2.26e-08 ***\nPC1_w1_cent       -0.07868    0.04938 1492.80341  -1.593   0.1113    \nPC2_w2_cent        0.05708    0.04892 1567.08966   1.167   0.2435    \nknow_cent          0.09657    0.05072 1344.82431   1.904   0.0571 .  \nneedforcog_cent   -0.20471    0.05059  197.51174  -4.047 7.45e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) PC1_1_ PC2_2_ knw_cn\nPC1_w1_cent  0.000                     \nPC2_w2_cent  0.000  0.070              \nknow_cent    0.000 -0.260 -0.167       \nnedfrcg_cnt  0.000 -0.078 -0.030  0.001\n```\n:::\n:::\n\n\n### Model of objective expertise, perceived expertise and need for cognition with interaction\n\n\n::: {.cell}\n\n```{.r .cell-code}\nModel.2.1<-lmer(ih ~PC1_w1_cent + PC2_w2_cent  + know_cent+needforcog_cent + PC1_w1_cent*needforcog_cent + PC2_w2_cent*needforcog_cent + know_cent*needforcog_cent+(1|Score_Type)+(1|ID),   \n              data=expertise3_win1_comp)\nsummary(Model.2.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: ih ~ PC1_w1_cent + PC2_w2_cent + know_cent + needforcog_cent +  \n    PC1_w1_cent * needforcog_cent + PC2_w2_cent * needforcog_cent +  \n    know_cent * needforcog_cent + (1 | Score_Type) + (1 | ID)\n   Data: expertise3_win1_comp\n\nREML criterion at convergence: 6304.6\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.94328 -0.65638 -0.01338  0.60696  2.89992 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.5000   0.7071  \n Score_Type (Intercept) 0.3397   0.5828  \n Residual               2.6691   1.6337  \nNumber of obs: 1590, groups:  ID, 199; Score_Type, 8\n\nFixed effects:\n                              Estimate Std. Error         df t value Pr(>|t|)\n(Intercept)                    4.87079    0.21607    7.79504  22.543 2.23e-08\nPC1_w1_cent                   -0.08004    0.05771 1486.77758  -1.387   0.1657\nPC2_w2_cent                    0.06017    0.04926 1568.51514   1.222   0.2220\nknow_cent                      0.09172    0.05068 1352.14067   1.810   0.0705\nneedforcog_cent               -0.21713    0.05075  199.21963  -4.278 2.92e-05\nPC1_w1_cent:needforcog_cent    0.04532    0.03872 1559.22145   1.170   0.2420\nPC2_w2_cent:needforcog_cent   -0.05653    0.03918 1561.39689  -1.443   0.1492\nknow_cent:needforcog_cent     -0.09392    0.03909 1396.96749  -2.403   0.0164\n                               \n(Intercept)                 ***\nPC1_w1_cent                    \nPC2_w2_cent                    \nknow_cent                   .  \nneedforcog_cent             ***\nPC1_w1_cent:needforcog_cent    \nPC2_w2_cent:needforcog_cent    \nknow_cent:needforcog_cent   *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) PC1_w1_ PC2_w2_ knw_cn ndfrc_ PC1_1_: PC2_2_:\nPC1_w1_cent  0.013                                              \nPC2_w2_cent  0.003  0.125                                       \nknow_cent    0.000 -0.220  -0.165                               \nnedfrcg_cnt -0.001 -0.077  -0.034   0.003                       \nPC1_w1_cn:_ -0.023 -0.483  -0.108  -0.012 -0.011                \nPC2_w2_cn:_ -0.006 -0.102  -0.062  -0.011  0.046 -0.038         \nknw_cnt:nd_  0.000 -0.001   0.001   0.041  0.060 -0.251  -0.163 \n```\n:::\n:::\n\n\n#Winsorize at 1% the raw variables for our new expertise variables\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# winsorize the variables (at 1%)\nexpertise3_wins_average <- expertise3_wins_fact%>%\n  mutate(numarticle_w1=Winsorize(numarticle, probs = c(0,0.99)), \n         numbook_w1=Winsorize(numbook, probs = c(0,0.99)),\n         high_w1=Winsorize(high, probs = c(0,0.99)),\n         colle_w1=Winsorize(colle, probs = c(0,0.99)),\n         grad_w1=Winsorize(grad,na.rm=TRUE, probs = c(0,0.99)))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Standardize the Selected variables  ----\nvars_to_standardize <- c(\"know\", \"numarticle_w1\",\"numbook_w1\", \"high_w1\",\"colle_w1\", \"grad_w1\")\n\n\nexpertise3_wins_average<-expertise3_wins_average %>% \n  group_by(Score_Type) %>% \n  mutate_at(vars(vars_to_standardize), scale)\n```\n:::\n\n\n## Take the means of winsorized variables to create our new categories: media and class\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexpertise3_wins_average <- expertise3_wins_average %>% mutate(class_wins1 = mapply(function(x, y, z) {\n  ifelse(is.na(x), (y + z)/2, \n         ifelse(is.na(y), (x + z)/2, \n                ifelse(is.na(z), (x + y)/2, (x + y + z)/3)))}, \n  grad_w1, colle_w1, high_w1))\n\nexpertise3_wins_average <- expertise3_wins_average %>% mutate(media_wins1 = \n                                                          mapply(function(x, y) {\n  ifelse(is.na(x), (y), \n         ifelse(is.na(y), (x),\n                  (x + y)/2))}, \n                      numarticle_w1,numbook_w1))\n```\n:::\n\n\n##Shapiro-Wilk values\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndescriptives(expertise3_wins_average, vars = vars(class_wins1, media_wins1, know, ih), n=FALSE, missing= FALSE, median=FALSE, sw = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n DESCRIPTIVES\n\n Descriptives                                                                         \n ──────────────────────────────────────────────────────────────────────────────────── \n                         class_wins1     media_wins1     know            ih           \n ──────────────────────────────────────────────────────────────────────────────────── \n   Mean                  1.297120e-17    2.043312e-17    4.689314e-17      4.869579   \n   Standard deviation       0.7425085       0.8782155       0.9977963      1.891089   \n   Minimum                 -0.5346050      -0.4793892       -2.659207      1.000000   \n   Maximum                   7.340605        9.991529        2.956309      9.000000   \n   Shapiro-Wilk W           0.4621646       0.5037120       0.9918312     0.9818945   \n   Shapiro-Wilk p          < .0000001      < .0000001      < .0000001    < .0000001   \n ──────────────────────────────────────────────────────────────────────────────────── \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Center variables: \n\nexpertise3_wins_average$know_cent <- scale(expertise3_wins_average$know, center = TRUE, scale = FALSE)\n\nexpertise3_wins_average$needforcog_cent <- scale(expertise3_wins_average$needforcog, center = TRUE, scale = FALSE)\n\nexpertise3_wins_average$class_wins1_cent <- scale(expertise3_wins_average$class_wins1, center = TRUE, scale = FALSE)\n\nexpertise3_wins_average$media_wins1_cent <- scale(expertise3_wins_average$media_wins1, center = TRUE, scale = FALSE)\n```\n:::\n\n\n## HLM Models\n\n\n### Model of objective expertise, perceived expertise and need for cognition without interaction\n\n\n::: {.cell}\n\n```{.r .cell-code}\nModel.1.1<-lmer(ih ~class_wins1_cent + media_wins1_cent + know_cent +needforcog_cent +(1|Score_Type)+(1|ID),   \n              data=expertise3_wins_average)\nsummary(Model.1.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: \nih ~ class_wins1_cent + media_wins1_cent + know_cent + needforcog_cent +  \n    (1 | Score_Type) + (1 | ID)\n   Data: expertise3_wins_average\n\nREML criterion at convergence: 6294.1\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-3.02716 -0.65887 -0.02207  0.61398  2.83610 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.488    0.6985  \n Score_Type (Intercept) 0.359    0.5991  \n Residual               2.676    1.6359  \nNumber of obs: 1590, groups:  ID, 199; Score_Type, 8\n\nFixed effects:\n                   Estimate Std. Error         df t value Pr(>|t|)    \n(Intercept)         4.87054    0.22137    7.75433  22.002 2.87e-08 ***\nclass_wins1_cent    0.10399    0.06558 1565.35319   1.586  0.11302    \nmedia_wins1_cent   -0.17748    0.06032 1323.81512  -2.942  0.00331 ** \nknow_cent           0.11515    0.05079 1330.40508   2.267  0.02353 *  \nneedforcog_cent    -0.20245    0.05022  196.37030  -4.032 7.92e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cls_1_ md_w1_ knw_cn\nclss_wns1_c  0.000                     \nmd_wns1_cnt  0.000 -0.302              \nknow_cent    0.000 -0.109 -0.245       \nnedfrcg_cnt  0.000 -0.027 -0.059  0.000\n```\n:::\n:::\n\n\n#Plot of the model 1.1\n\n::: {.cell}\n\n```{.r .cell-code}\nModel.1.1 <- lmer(ih ~class_wins1_cent + media_wins1_cent + know_cent +needforcog_cent +(1|Score_Type)+(1|ID),   \n              data=expertise3_wins_average)\nsummary(Model.1.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: \nih ~ class_wins1_cent + media_wins1_cent + know_cent + needforcog_cent +  \n    (1 | Score_Type) + (1 | ID)\n   Data: expertise3_wins_average\n\nREML criterion at convergence: 6294.1\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-3.02716 -0.65887 -0.02207  0.61398  2.83610 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.488    0.6985  \n Score_Type (Intercept) 0.359    0.5991  \n Residual               2.676    1.6359  \nNumber of obs: 1590, groups:  ID, 199; Score_Type, 8\n\nFixed effects:\n                   Estimate Std. Error         df t value Pr(>|t|)    \n(Intercept)         4.87054    0.22137    7.75433  22.002 2.87e-08 ***\nclass_wins1_cent    0.10399    0.06558 1565.35319   1.586  0.11302    \nmedia_wins1_cent   -0.17748    0.06032 1323.81512  -2.942  0.00331 ** \nknow_cent           0.11515    0.05079 1330.40508   2.267  0.02353 *  \nneedforcog_cent    -0.20245    0.05022  196.37030  -4.032 7.92e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cls_1_ md_w1_ knw_cn\nclss_wns1_c  0.000                     \nmd_wns1_cnt  0.000 -0.302              \nknow_cent    0.000 -0.109 -0.245       \nnedfrcg_cnt  0.000 -0.027 -0.059  0.000\n```\n:::\n\n```{.r .cell-code}\n# save fitted data\nef.1.media <- effect(term = \"media_wins1_cent\",\n                                   mod = Model.1.1)\nef.1.media.data <- as.data.frame(ef.1.media) #convert the effects list to a data frame\nef.1.media.data #print effects data frame\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  media_wins1_cent      fit        se    lower    upper\n1             -0.5 4.958820 0.2234149 4.520600 5.397039\n2              2.0 4.515125 0.2521131 4.020614 5.009635\n3              5.0 3.982690 0.3741251 3.248858 4.716523\n4              7.0 3.627734 0.4767515 2.692605 4.562864\n5             10.0 3.095300 0.6425344 1.834994 4.355607\n```\n:::\n\n```{.r .cell-code}\nexpert_media_graph <- expertise3_wins_average %>%\n  group_by(ID) %>%\n  summarise(ih = mean(ih),\n            media_wins1_cent = media_wins1)\nexpert_media_graph <- as.data.frame(expert_media_graph)\n\nexpert_media_graph$media_wins1_cent<-expert_media_graph$media_wins1_cent-min(expert_media_graph$media_wins1_cent)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfig.1.expert_w1 <- ggplot(data = ef.1.media.data, aes(x = media_wins1_cent,\n                                                  y = fit)) +\n  geom_point(data=expert_media_graph, aes(x=media_wins1_cent,y=ih),\n             show.legend = FALSE, pch=21, color=\"blueviolet\", alpha=.25, size=3) +\n  geom_line(aes(x=media_wins1_cent), color = \"darkred\", size = 1.2)  +\n  geom_ribbon(aes(ymin = fit-se, ymax = fit+se), alpha = 0.50, fill = \"gray70\") +\n  labs(title = \"Expertise and Inherence Bias\", x = \"Expertise (books,articles)\",\n       y = \"Inherence bias\") +\n  scale_x_continuous(limits = c(0, 8), breaks = seq(0, 8, 2), expand = c(0, 0)) +\n  scale_y_continuous(limits = c(0, 8), breaks = seq(0, 8, 2), expand = c(0, 0)) +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n        axis.title = element_text(size = 14, face = \"bold\"),\n        axis.text = element_text(size = 12),\n        legend.title = element_blank(),\n        panel.grid = element_blank(),\n        panel.border = element_blank(),\n        plot.background=element_rect(fill = \"gray96\"),\n        panel.background = element_rect(fill = \"gray96\"))\nfig.1.expert_w1\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-59-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggsave(paste0(\"figure_expertise+media_w1\",\n              format(Sys.time(), \"%Y-%m-%d\")\n              ,\".eps\"), width = 20, height = 20, units = \"cm\") ###Save the Figure 3C\n```\n:::\n\n\n\n#Winsorize at 2.5% the raw variables for our new expertise variables\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# winsorize the variables (at 1%)\nexpertise3_wins_average_w25 <- expertise3_wins_fact%>%\n  mutate(numarticle_w1=Winsorize(numarticle, probs = c(0,0.975)), \n         numbook_w1=Winsorize(numbook, probs = c(0,0.975)),\n         high_w1=Winsorize(high, probs = c(0,0.975)),\n         colle_w1=Winsorize(colle, probs = c(0,0.975)),\n         grad_w1=Winsorize(grad,na.rm=TRUE, probs = c(0,0.975)))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Standardize the Selected variables  ----\nvars_to_standardize <- c(\"know\", \"numarticle_w1\",\"numbook_w1\", \"high_w1\",\"colle_w1\", \"grad_w1\")\n\n\nexpertise3_wins_average_w25<-expertise3_wins_average_w25 %>% \n  group_by(Score_Type) %>% \n  mutate_at(vars(vars_to_standardize), scale)\n```\n:::\n\n\n## Take the means of winsorized variables to create our new categories: media and class\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexpertise3_wins_average_w25 <- expertise3_wins_average_w25 %>% mutate(class_wins1 = mapply(function(x, y, z) {\n  ifelse(is.na(x), (y + z)/2, \n         ifelse(is.na(y), (x + z)/2, \n                ifelse(is.na(z), (x + y)/2, (x + y + z)/3)))}, \n  grad_w1, colle_w1, high_w1))\n\nexpertise3_wins_average_w25 <- expertise3_wins_average_w25 %>% mutate(media_wins1 = \n                                                          mapply(function(x, y) {\n  ifelse(is.na(x), (y), \n         ifelse(is.na(y), (x),\n                  (x + y)/2))}, \n                      numarticle_w1,numbook_w1))\n```\n:::\n\n\n##Shapiro-Wilk values\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndescriptives(expertise3_wins_average_w25, vars = vars(class_wins1, media_wins1, know, ih), n=FALSE, missing= FALSE, median=FALSE, sw = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n DESCRIPTIVES\n\n Descriptives                                                                          \n ───────────────────────────────────────────────────────────────────────────────────── \n                         class_wins1     media_wins1      know            ih           \n ───────────────────────────────────────────────────────────────────────────────────── \n   Mean                  1.471464e-17    -5.021109e-18    4.689314e-17      4.869579   \n   Standard deviation       0.8508659        0.8782263       0.9977963      1.891089   \n   Minimum                 -0.7824349       -0.5626174       -2.659207      1.000000   \n   Maximum                   6.440938         7.384871        2.956309      9.000000   \n   Shapiro-Wilk W           0.5612456        0.6047509       0.9918312     0.9818945   \n   Shapiro-Wilk p          < .0000001       < .0000001      < .0000001    < .0000001   \n ───────────────────────────────────────────────────────────────────────────────────── \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Center variables: \n\nexpertise3_wins_average_w25$know_cent <- scale(expertise3_wins_average_w25$know, center = TRUE, scale = FALSE)\n\nexpertise3_wins_average_w25$needforcog_cent <- scale(expertise3_wins_average_w25$needforcog, center = TRUE, scale = FALSE)\n\nexpertise3_wins_average_w25$class_wins1_cent <- scale(expertise3_wins_average_w25$class_wins1, center = TRUE, scale = FALSE)\n\nexpertise3_wins_average_w25$media_wins1_cent <- scale(expertise3_wins_average_w25$media_wins1, center = TRUE, scale = FALSE)\n```\n:::\n\n\n## HLM Models\n\n### Model of objective expertise, perceived expertise and need for cognition\n\n\n::: {.cell}\n\n```{.r .cell-code}\nModel.1.1<-lmer(ih ~class_wins1_cent + media_wins1_cent + know_cent +needforcog_cent +(1|Score_Type)+(1|ID),   \n              data=expertise3_wins_average_w25)\nsummary(Model.1.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: \nih ~ class_wins1_cent + media_wins1_cent + know_cent + needforcog_cent +  \n    (1 | Score_Type) + (1 | ID)\n   Data: expertise3_wins_average_w25\n\nREML criterion at convergence: 6294.1\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-3.02153 -0.64676 -0.02857  0.61140  2.81613 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.4918   0.7013  \n Score_Type (Intercept) 0.3590   0.5991  \n Residual               2.6740   1.6353  \nNumber of obs: 1590, groups:  ID, 199; Score_Type, 8\n\nFixed effects:\n                   Estimate Std. Error         df t value Pr(>|t|)    \n(Intercept)         4.87054    0.22142    7.76025  21.997 2.85e-08 ***\nclass_wins1_cent    0.06547    0.05651 1577.81243   1.159  0.24683    \nmedia_wins1_cent   -0.18164    0.05939 1432.18838  -3.058  0.00227 ** \nknow_cent           0.12442    0.05113 1321.38198   2.433  0.01509 *  \nneedforcog_cent    -0.20102    0.05030  195.97212  -3.997 9.09e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cls_1_ md_w1_ knw_cn\nclss_wns1_c  0.000                     \nmd_wns1_cnt  0.000 -0.278              \nknow_cent    0.000 -0.107 -0.269       \nnedfrcg_cnt  0.000 -0.017 -0.059 -0.001\n```\n:::\n:::\n\n\n\n#Plot of the model 1.1 with 2.5 winsorize\n\n\n::: {.cell}\n\n```{.r .cell-code}\nModel.1.1 <- lmer(ih ~class_wins1_cent + media_wins1_cent + know_cent +needforcog_cent +(1|Score_Type)+(1|ID),   \n              data=expertise3_wins_average_w25)\nsummary(Model.1.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: \nih ~ class_wins1_cent + media_wins1_cent + know_cent + needforcog_cent +  \n    (1 | Score_Type) + (1 | ID)\n   Data: expertise3_wins_average_w25\n\nREML criterion at convergence: 6294.1\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-3.02153 -0.64676 -0.02857  0.61140  2.81613 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.4918   0.7013  \n Score_Type (Intercept) 0.3590   0.5991  \n Residual               2.6740   1.6353  \nNumber of obs: 1590, groups:  ID, 199; Score_Type, 8\n\nFixed effects:\n                   Estimate Std. Error         df t value Pr(>|t|)    \n(Intercept)         4.87054    0.22142    7.76025  21.997 2.85e-08 ***\nclass_wins1_cent    0.06547    0.05651 1577.81243   1.159  0.24683    \nmedia_wins1_cent   -0.18164    0.05939 1432.18838  -3.058  0.00227 ** \nknow_cent           0.12442    0.05113 1321.38198   2.433  0.01509 *  \nneedforcog_cent    -0.20102    0.05030  195.97212  -3.997 9.09e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cls_1_ md_w1_ knw_cn\nclss_wns1_c  0.000                     \nmd_wns1_cnt  0.000 -0.278              \nknow_cent    0.000 -0.107 -0.269       \nnedfrcg_cnt  0.000 -0.017 -0.059 -0.001\n```\n:::\n\n```{.r .cell-code}\n# save fitted data\nef.1.media <- effect(term = \"media_wins1_cent\",\n                                   mod = Model.1.1)\nef.1.media.data <- as.data.frame(ef.1.media) #convert the effects list to a data frame\nef.1.media.data #print effects data frame\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  media_wins1_cent      fit        se    lower    upper\n1             -0.6 4.979056 0.2242627 4.539173 5.418939\n2              1.0 4.688432 0.2292460 4.238775 5.138090\n3              3.0 4.325153 0.2842137 3.767679 4.882627\n4              5.0 3.961873 0.3704346 3.235280 4.688467\n5              7.0 3.598593 0.4710488 2.674649 4.522538\n```\n:::\n\n```{.r .cell-code}\nexpert_media_graph <- expertise3_wins_average_w25 %>%\n  group_by(ID) %>%\n  summarise(ih = mean(ih),\n            media_wins1_cent = media_wins1)\nexpert_media_graph <- as.data.frame(expert_media_graph)\n\nexpert_media_graph$media_wins1_cent<-expert_media_graph$media_wins1_cent-min(expert_media_graph$media_wins1_cent)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfig.1.expert_w25 <- ggplot(data = ef.1.media.data, aes(x = media_wins1_cent,\n                                                  y = fit)) +\n  geom_point(data=expert_media_graph, aes(x=media_wins1_cent,y=ih),\n             show.legend = FALSE, pch=21, color=\"blueviolet\", alpha=.25, size=3) +\n  geom_line(aes(x=media_wins1_cent), color = \"darkred\", size = 1.2)  +\n  geom_ribbon(aes(ymin = fit-se, ymax = fit+se), alpha = 0.50, fill = \"gray70\") +\n  labs(title = \"Expertise and Inherence Bias\", x = \"Expertise (books,articles)\",\n       y = \"Inherence bias\") +\n  scale_x_continuous(limits = c(0, 10), breaks = seq(0, 8, 2), expand = c(0, 0)) +\n  scale_y_continuous(limits = c(0, 8), breaks = seq(0, 8, 2), expand = c(0, 0)) +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n        axis.title = element_text(size = 14, face = \"bold\"),\n        axis.text = element_text(size = 12),\n        legend.title = element_blank(),\n        panel.grid = element_blank(),\n        panel.border = element_blank(),\n        plot.background=element_rect(fill = \"gray96\"),\n        panel.background = element_rect(fill = \"gray96\"))\nfig.1.expert_w25\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-67-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggsave(paste0(\"figure_expertise+media_w25\",\n              format(Sys.time(), \"%Y-%m-%d\")\n              ,\".eps\"), width = 20, height = 20, units = \"cm\") ###Save the Figure 3C\n```\n:::\n\n\n\n#Sqrt variables\n\n##Square root of the raw variables for our new expertise variables\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# take the square root of the variables\nexpertise3_sqrt <- expertise3_wins_fact%>%\n  mutate(numarticle_sqrt=sqrt(numarticle),\n         numbook_sqrt=sqrt(numbook),\n         high_sqrt=sqrt(high),\n         colle_sqrt=sqrt(colle),\n         grad_sqrt=sqrt(grad))\n```\n:::\n\n\n## Standardize and average of our variables to create our new categories: media and class\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Standardize the Selected variables  ----\nvars_to_standardize <- c(\"know\", \"numarticle_sqrt\",\"numbook_sqrt\", \"high_sqrt\",\"colle_sqrt\", \"grad_sqrt\")\n\n\nexpertise3_sqrt<-expertise3_sqrt %>% \n  group_by(Score_Type) %>% \n  mutate_at(vars(vars_to_standardize), scale)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nexpertise3_sqrt <- expertise3_sqrt %>% mutate(class_wins1 = mapply(function(x, y, z) {\n  ifelse(is.na(x), (y + z)/2, \n         ifelse(is.na(y), (x + z)/2, \n                ifelse(is.na(z), (x + y)/2, (x + y + z)/3)))}, \n  grad_sqrt, colle_sqrt, high_sqrt))\n\nexpertise3_sqrt <- expertise3_sqrt %>% mutate(media_wins1 = \n                                                          mapply(function(x, y) {\n  ifelse(is.na(x), (y), \n         ifelse(is.na(y), (x),\n                  (x + y)/2))}, \n                      numarticle_sqrt,numbook_sqrt))\n```\n:::\n\n\n##Histograms of square root variables\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(expertise3_sqrt$class_wins1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-71-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(expertise3_sqrt$media_wins1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-72-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(expertise3_sqrt$know)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-73-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(expertise3_sqrt$ih)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-74-1.png){width=672}\n:::\n:::\n\n\n##Shapiro-Wilk values\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndescriptives(expertise3_sqrt, vars = vars(class_wins1, media_wins1, know, ih), n=FALSE, missing= FALSE, median=FALSE, sw = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n DESCRIPTIVES\n\n Descriptives                                                                          \n ───────────────────────────────────────────────────────────────────────────────────── \n                         class_wins1     media_wins1      know            ih           \n ───────────────────────────────────────────────────────────────────────────────────── \n   Mean                  3.005692e-17    -1.588623e-16    4.689314e-17      4.869579   \n   Standard deviation       0.7488095        0.8749660       0.9977963      1.891089   \n   Minimum                 -0.6141819       -0.7077785       -2.659207      1.000000   \n   Maximum                   7.141527         8.588626        2.956309      9.000000   \n   Shapiro-Wilk W           0.5201987        0.6946538       0.9918312     0.9818945   \n   Shapiro-Wilk p          < .0000001       < .0000001      < .0000001    < .0000001   \n ───────────────────────────────────────────────────────────────────────────────────── \n```\n:::\n:::\n\n\n##Cente the variables\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Center variables: \n\nexpertise3_sqrt$know_cent <- scale(expertise3_sqrt$know, center = TRUE, scale = FALSE)\n\nexpertise3_sqrt$needforcog_cent <- scale(expertise3_sqrt$needforcog, center = TRUE, scale = FALSE)\n\nexpertise3_sqrt$class_wins1_cent <- scale(expertise3_sqrt$class_wins1, center = TRUE, scale = FALSE)\n\nexpertise3_sqrt$media_wins1_cent <- scale(expertise3_sqrt$media_wins1, center = TRUE, scale = FALSE)\n```\n:::\n\n\n## HLM Models\n\n### Model of objective expertise, perceived expertise and need for cognition\n\n\n::: {.cell}\n\n```{.r .cell-code}\nModel.1.1<-lmer(ih ~class_wins1_cent + media_wins1_cent + know_cent +needforcog_cent +(1|Score_Type)+(1|ID),   \n              data=expertise3_sqrt)\nsummary(Model.1.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: \nih ~ class_wins1_cent + media_wins1_cent + know_cent + needforcog_cent +  \n    (1 | Score_Type) + (1 | ID)\n   Data: expertise3_sqrt\n\nREML criterion at convergence: 6295.8\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.98887 -0.65579 -0.02521  0.61279  2.84589 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.4937   0.7026  \n Score_Type (Intercept) 0.3589   0.5991  \n Residual               2.6771   1.6362  \nNumber of obs: 1590, groups:  ID, 199; Score_Type, 8\n\nFixed effects:\n                   Estimate Std. Error         df t value Pr(>|t|)    \n(Intercept)         4.87049    0.22143    7.76331  21.996 2.84e-08 ***\nclass_wins1_cent    0.08276    0.06617 1571.06950   1.251  0.21118    \nmedia_wins1_cent   -0.17106    0.06382 1220.55148  -2.680  0.00745 ** \nknow_cent           0.12201    0.05160 1311.37413   2.364  0.01820 *  \nneedforcog_cent    -0.20105    0.05044  196.73295  -3.986 9.47e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cls_1_ md_w1_ knw_cn\nclss_wns1_c  0.000                     \nmd_wns1_cnt  0.000 -0.339              \nknow_cent    0.000 -0.086 -0.291       \nnedfrcg_cnt  0.000 -0.026 -0.069  0.008\n```\n:::\n:::\n\n\n#Cube root variables\n\n##Cube root of the raw variables for our new expertise variables\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# cube root of the variables \nexpertise3_cube <- expertise3_wins_fact%>%\n  mutate(numarticle_cube=(numarticle^(1/3)),\n         numbook_cube=(numbook^(1/3)),\n         high_cube=(high^(1/3)),\n         colle_cube=(colle^(1/3)),\n         grad_cube=(grad^(1/3)))\n```\n:::\n\n\n## Standardize and average of variables to create our new categories: media and class\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Standardize the Selected variables  ----\nvars_to_standardize <- c(\"know\", \"numarticle_cube\",\"numbook_cube\", \"high_cube\",\"colle_cube\", \"grad_cube\")\n\n\nexpertise3_cube<-expertise3_cube %>% \n  group_by(Score_Type) %>% \n  mutate_at(vars(vars_to_standardize), scale)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nexpertise3_cube <- expertise3_cube %>% mutate(class_wins1 = mapply(function(x, y, z) {\n  ifelse(is.na(x), (y + z)/2, \n         ifelse(is.na(y), (x + z)/2, \n                ifelse(is.na(z), (x + y)/2, (x + y + z)/3)))}, \n  grad_cube, colle_cube, high_cube))\n\nexpertise3_cube <- expertise3_cube %>% mutate(media_wins1 = \n                                                          mapply(function(x, y) {\n  ifelse(is.na(x), (y), \n         ifelse(is.na(y), (x),\n                  (x + y)/2))}, \n                       numarticle_cube,numbook_cube))\n```\n:::\n\n\n##Histograms of square root variables\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(expertise3_cube$class_wins1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-81-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(expertise3_cube$media_wins1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-82-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(expertise3_cube$know)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-83-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(expertise3_cube$ih)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-84-1.png){width=672}\n:::\n:::\n\n\n##Shapiro-Wilk values\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndescriptives(expertise3_cube, vars = vars(class_wins1, media_wins1, know, ih), n=FALSE, missing= FALSE, median=FALSE,sw = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n DESCRIPTIVES\n\n Descriptives                                                                          \n ───────────────────────────────────────────────────────────────────────────────────── \n                         class_wins1      media_wins1     know            ih           \n ───────────────────────────────────────────────────────────────────────────────────── \n   Mean                  -6.380993e-18    3.138193e-17    4.689314e-17      4.869579   \n   Standard deviation        0.7507681       0.8707762       0.9977963      1.891089   \n   Minimum                  -0.6582257      -0.8354621       -2.659207      1.000000   \n   Maximum                    7.213367        6.302444        2.956309      9.000000   \n   Shapiro-Wilk W            0.5488866       0.8207145       0.9918312     0.9818945   \n   Shapiro-Wilk p           < .0000001      < .0000001      < .0000001    < .0000001   \n ───────────────────────────────────────────────────────────────────────────────────── \n```\n:::\n:::\n\n\n##Center variables\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Center variables: \n\nexpertise3_cube$know_cent <- scale(expertise3_cube$know, center = TRUE, scale = FALSE)\n\nexpertise3_cube$needforcog_cent <- scale(expertise3_cube$needforcog, center = TRUE, scale = FALSE)\n\nexpertise3_cube$class_wins1_cent <- scale(expertise3_cube$class_wins1, center = TRUE, scale = FALSE)\n\nexpertise3_cube$media_wins1_cent <- scale(expertise3_cube$media_wins1, center = TRUE, scale = FALSE)\n```\n:::\n\n\n## HLM Models\n\n### Model of objective expertise, perceived expertise and need for cognition\n\n\n::: {.cell}\n\n```{.r .cell-code}\nModel.1.1<-lmer(ih ~class_wins1_cent + media_wins1_cent + know_cent +needforcog_cent +(1|Score_Type)+(1|ID),   \n              data=expertise3_cube)\nsummary(Model.1.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: \nih ~ class_wins1_cent + media_wins1_cent + know_cent + needforcog_cent +  \n    (1 | Score_Type) + (1 | ID)\n   Data: expertise3_cube\n\nREML criterion at convergence: 6297.3\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.9804 -0.6563 -0.0198  0.6097  2.8588 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.4951   0.7037  \n Score_Type (Intercept) 0.3589   0.5991  \n Residual               2.6794   1.6369  \nNumber of obs: 1590, groups:  ID, 199; Score_Type, 8\n\nFixed effects:\n                   Estimate Std. Error         df t value Pr(>|t|)    \n(Intercept)         4.87047    0.22144    7.76566  21.994 2.83e-08 ***\nclass_wins1_cent    0.07247    0.06693 1573.64949   1.083   0.2791    \nmedia_wins1_cent   -0.15383    0.06450 1326.49676  -2.385   0.0172 *  \nknow_cent           0.12180    0.05201 1299.59345   2.342   0.0193 *  \nneedforcog_cent    -0.20136    0.05050  196.56908  -3.987 9.42e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cls_1_ md_w1_ knw_cn\nclss_wns1_c  0.000                     \nmd_wns1_cnt  0.000 -0.359              \nknow_cent    0.000 -0.069 -0.313       \nnedfrcg_cnt  0.000 -0.022 -0.073  0.010\n```\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}