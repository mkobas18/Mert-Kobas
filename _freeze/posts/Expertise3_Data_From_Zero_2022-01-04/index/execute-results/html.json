{
  "hash": "86a320510000afd22c7a75c598eb0c6f",
  "result": {
    "markdown": "---\ntitle: \"Expertise3 Data Analysis from the very beginning\"\nauthor: \"MK\"\ndate: \"2023-01-04\"\ncategories: [example data, code, analysis]\nimage: \"data.png\"\n---\n\n\nThis post includes the trial analyses of an example data related to expertise.\n\nImport necessary packages and expertise data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Import library ----\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(purrr)\n\n#Read the csv file ----\nExpertise3 <- read_csv(\"~/Desktop/Expertise3_December 23, 2022_10.53.csv\")\n```\n:::\n\n\nCreates a new dataframe called expertise3_clean, which is a copy of the original dataframe called Expertise3 and removes the second row of the dataframe and create a variable called column_names and assign it the names of the columns in the dataframe and change the column names\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#removes the second row of the dataframe\nexpertise3_clean<-Expertise3%>% \n  slice(-2) \n\n\n#selects all columns except the ones listed \nexpertise3_clean <- expertise3_clean %>%\n  select(-StartDate, -EndDate, -Status, -Progress,-ResponseId,-RecordedDate,-RecipientLastName, -RecipientFirstName,-RecipientEmail, -ExternalReference, -LocationLatitude,-LocationLongitude, -DistributionChannel, -UserLanguage)\n\n#create a variable called column_names and assign it the names of the columns in the dataframe\ncolumn_names <- names(expertise3_clean)\n\n##Change the column names ----\ncolnames(expertise3_clean) <- c('ip','duration', 'finished', 'stih_lang','r_ih_lang','stih_school','r_ih_school',\n                                'stih_cards','r_ih_cards','stih_breakfast','r_ih_breakfast','stih_weddings','r_ih_weddings',\n                                'stih_teeth','r_ih_teeth','stih_traffic','r_ih_traffic','stih_tv','r_ih_tv',\n                                'know_lang','know_school','know_cards','know_breakfast','know_weddings','know_teeth',\n                                'know_traffic','know_tv',\n                                'course_lang','high_lang', 'colle_lang', 'grad_lang',\n                                'book_lang','numbook_lang','article_lang','numarticle_lang',\n                                'course_school','high_school', 'colle_school', 'grad_school',\n                                'book_school','numbook_school','article_school','numarticle_school',\n                                'course_cards','high_cards', 'colle_cards', 'grad_cards',\n                                'book_cards','numbook_cards','article_cards','numarticle_cards',\n                                'course_breakfast','high_breakfast', 'colle_breakfast', 'grad_breakfast',\n                                'book_breakfast','numbook_breakfast','article_breakfast','numarticle_breakfast',\n                                'course_weddings','high_weddings', 'colle_weddings', 'grad_weddings',\n                                'book_weddings','numbook_weddings','article_weddings','numarticle_weddings',\n                                'course_teeth','high_teeth', 'colle_teeth', 'grad_teeth',\n                                'book_teeth','numbook_teeth','article_teeth','numarticle_teeth',\n                                'course_traffic','high_traffic', 'colle_traffic', 'grad_traffic',\n                                'book_traffic','numbook_traffic','article_traffic','numarticle_traffic',\n                                'course_tv','high_tv', 'colle_tv', 'grad_tv',\n                                'book_tv','numbook_tv','article_tv','numarticle_tv',\n                                'needforcog1','needforcog2','needforcog3','needforcog4','needforcog5','needforcog6',\n                                'needforcog7','needforcog8','needforcog9','needforcog10','needforcog11','needforcog12',\n                                'needforcog13','needforcog14','needforcog15','needforcog16','needforcog17','needforcog18',\n                                'otherways','sex','birthdate','education','income','religion','identity','age','political_atti',\n                                'english_level','proceure_confu','whatwestudied','moretothisstudy','additional_thoughts','attention')\n```\n:::\n\n\nThe code below shows the survey items:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#selects all columns except the ones specified\nrow_values <- expertise3_clean %>%\n  select(-duration,-finished, -ip)%>%\n  #selects only the first row\n  filter(row_number() == 1)\n\n#Items in the questionnaire ----\n\n#unlist the row_values\nrow_values <- unlist(row_values)\n\nmy_list <- map(row_values, ~paste0(.))\n\nlibrary(stringr)\nmy_list <- str_replace(my_list, \"(?<! )\\\\n(?! )\", \"\")\nmy_list <- str_replace(my_list, \"[^\\\\s]*\\\\\\\\n[^\\\\s]*\", \"\")\nlist_string <- paste0(\"* \", paste(my_list, collapse = \"\\n* \"))\n\n##Show the survey items ----\ncat(list_string)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n* Human languages are structured to best convey ourthoughts and feelings. Words and their meanings likely form ideal matches.\n* There are absolutely no good reasons whywe use specific words to represent our thoughts. Any combination of sounds\ncould in principle refer to any idea.\n* The fact that elementary school stops at 5th grade is probably ideal for children's learning. This is likely the best way to organize K-12 schooling.\n* Middle school (grades 6-8) is separate from elementary school (grades K-5) largely because of decisions made by educators a long time ago. This may not be the most optimal way of organizing early education.\n* It’s not a coincidence thatwe send people cards on holidays. This tradition seems\nparticularly fitting.\n* The fact that we send people cards on holidays is only a convention. A different way of sending warm wishes could've been implemented just as easily.\n* There are good reasons why orange juiceis typically consumed for breakfast. There are features about it that make it\nparticularly suited for this meal (for example, its refreshing taste).\n* The current popularity of orange juice for breakfast reflectsin part  marketing campaigns that promoted\ndrinking orange juice in the morning. However, had history taken a different\nturn, orange juice could just as easily have been more popular for lunch or\ndinner.\n* It seems right to use white for wedding dresses. Othercolors, such as red and blue, have features that make them less suited for\nwedding dresses.\n* Even though white is thetraditional color for wedding dresses, this could have easily been different.\nWhen you really think about it, there is no reason why other, brighter, colors couldn’t\nbe used for wedding dresses.\n* It seems ideal that toothpaste istypically flavored with mint. Mint is inherently more refreshing than any other\nflavor that currently exists.\n* When you think about it, toothpaste could have easily beenflavored with something other than mint, such as cinnamon. Many pleasing flavors would work just as\nwell.\n* Traffic lights, with threedifferent colored lights signaling three speeds, seem like the most efficient\nand effective way to direct traffic. Another process likely would not work as\nwell.\n* The current design of traffic lights,with three different colors reflecting three different speeds, is entirely due\nto historical factors. This is probably not the most efficient or effective way\nto manage traffic.\n* Black seems like a good choice for the color of televisions. Other colors just would not work as well.\n* Theonly reason why most TVs are black is\nhistorical happenstance. TVs could practically be a\nvariety of colors.\n* How much do you know about language and linguistics? - Please use the slider to select your answer choice.\n* How much do you know about school and education systems? - Please use the slider to select your answer choice.\n* How much do you know about holiday customs and traditions? - Please use the slider to select your answer choice.\n* How much do you know about breakfast foods? - Please use the slider to select your answer choice.\n* How much do you know about weddings and wedding traditions? - Please use the slider to select your answer choice.\n* How much do you know about teeth and oral hygiene? - Please use the slider to select your answer choice.\n* How much do you know about transportation science and traffic signal systems? - Please use the slider to select your answer choice.\n* How much do you know about the manufacturing of consumer electronics (TVs, MP3 players, camcorders, etc.)? - Please use the slider to select your answer choice.\n* Have you ever taken a class that discussed language and linguistics?\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - High School\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - College\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - Graduate School\n* Have you ever read any books on language and linguistics?\n* If you answered \"yes\" above, please approximate the number of books you have read on this topic:\n* Have you ever read any magazine, newspaper, or academic articles on language and linguistics?\n* If you answered \"yes\" above, please approximate the number of articles you have read on this topic:\n* Have you ever taken a class that discussed school and education systems?\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - High School\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - College\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - Graduate School\n* Have you ever read any books on school and education systems?\n* If you answered \"yes\" above, please approximate the number of books you have read on this topic:\n* Have you ever read any magazine, newspaper, or academic articles on school and education systems?\n* If you answered \"yes\" above, please approximate the number of articles you have read on this topic:\n* Have you ever taken a class that discussed holiday customs and traditions?\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - High School\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - College\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - Graduate School\n* Have you ever read any books on holiday customs and traditions?\n* If you answered \"yes\" above, please approximate the number of books you have read on this topic:\n* Have you ever read any magazine, newspaper, or academic articles on holiday customs and traditions?\n* If you answered \"yes\" above, please approximate the number of articles you have read on this topic:\n* Have you ever taken a class that discussed breakfast foods?\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - High School\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - College\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - Graduate School\n* Have you ever read any books on breakfast foods?\n* If you answered \"yes\" above, please approximate the number of books you have read on this topic:\n* Have you ever read any magazine, newspaper, or academic articles on breakfast foods?\n* If you answered \"yes\" above, please approximate the number of articles you have read on this topic:\n* Have you ever taken a class that discussed weddings and wedding traditions?\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - High School\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - College\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - Graduate School\n* Have you ever read any books on weddings and wedding traditions?\n* If you answered \"yes\" above, please approximate the number of books you have read on this topic:\n* Have you ever read any magazine, newspaper, or academic articles on weddings and wedding traditions?\n* If you answered \"yes\" above, please approximate the number of articles you have read on this topic:\n* Have you ever taken a class that discussed teeth and oral hygiene?\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - High School\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - College\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - Graduate School\n* Have you ever read any books on teeth and oral hygiene?\n* If you answered \"yes\" above, please approximate the number of books you have read on this topic:\n* Have you ever read any magazine, newspaper, or academic articles on teeth and oral hygiene?\n* If you answered \"yes\" above, please approximate the number of articles you have read on this topic:\n* Have you ever taken a class that discussed transportation science and traffic signal systems?\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - High School\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - College\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - Graduate School\n* Have you ever read any books on transportation science and traffic signal systems?\n* If you answered \"yes\" above, please approximate the number of books you have read on this topic:\n* Have you ever read any magazine, newspaper, or academic articles on transportation science and traffic signal systems?\n* If you answered \"yes\" above, please approximate the number of articles you have read on this topic:\n* Have you ever taken a class that discussed the manufacturing of consumer electronics (TVs, MP3 players, camcorders, etc.)?\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - High School\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - College\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - Graduate School\n* Have you ever read any books on the manufacturing of consumer electronics (TVs, MP3 players, camcorders, etc.)?\n* If you answered \"yes\" above, please approximate the number of books you have read on this topic:\n* Have you ever read any magazine, newspaper, or academic articles on the manufacturing of consumer electronics (TVs, MP3 players, camcorders, etc.)?\n* If you answered \"yes\" above, please approximate the number of articles you have read on this topic:\n* I would prefer complex to simple problems.\n* I like to have the responsibility of handling a situation that requires a lot of thinking.\n* Thinking is not my idea of fun.\n* I would rather do something that requires little thought than something that is sure to challenge my thinking abilities.\n* I try to anticipate and avoid situations where there is a likely chance I will have to think in depth about something.\n* I find satisfaction in deliberating hard and long for hours.\n* I only think as hard as I have to.\n* I prefer to think about small, daily projects rather than long-term ones.\n* I like tasks that require little thought once I’ve learned them.\n* The idea of relying on thought to make my way to the top appeals to me.\n* I really enjoy a task that involves coming up with new solutions to problems.\n* Learning new ways to think doesn’t excite me very much.\n* I prefer my life to be filled with puzzles that I must solve.\n* The notion of thinking abstractly appeals to me.\n* I would prefer a task that is intellectual, difficult, and important to one that is somewhat important but does not require much thought.\n* I feel relief rather than satisfaction after completing a task that required a lot of mental effort.\n* It’s enough for me that something gets the job done; I don’t care about why or how it works.\n* I usually end up deliberating about issues even when they do not affect me personally.\n* In this survey you were asked about your knowledge in a variety of subject areas. \nAside from the ways you may have learned about these topics that were mentioned in the survey (i.e., reading or  a college class), how else may you have learned about these topics? Please name other ways that you have learned about the topics mentioned in the survey you just took.\n* Are you male or female?\n* Q60 - What is your date of birth? (mm/dd/yyyy)\n* What is the highest level of education you have completed?\n* Q64 - What is your yearly household income?\n* Q64 - What is your religious affiliation?\n* Q64 - What is your racial or ethnic identity?\n* Q64 - What is your age in years?\n* How would you describe your political attitudes? Please select one of the points on the scale below.\n* Please rate your overall ability in the English language:\n* 1. Did you find any aspect of the procedure odd or confusing?\n* 2. What did you think we were studying?\n* 3. Do you think that there may have been more to this study than meets the eye? If so, what do you think this might have been?\n* 4. Do you have any additional thoughts or comments about the study?\n* Thank you for completing this survey! We just have one last question for you. You will not be penalized for your answer to this question. Since you completed the whole survey, you will receive payment no matter what answer you give here.\n\t \n\n\tIt's very important to the quality and scientific aims of our study that participants pay attention (i.e., read the survey carefully, consider the response options, and avoid distractions).\n\n\t\n\n\tWere you paying attention while completing this survey?\n```\n:::\n:::\n\n\nData preparation for further analyses\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Attention check and deletion of cases that didn't attend or finish the study ----\nexpertise3_new<-expertise3_clean%>%\n  filter(attention==1&finished==1)\n\n\n##Exclude the participants that joined outside of US ----\n\n#view(expertise3_new)\nexpertise3_new<-expertise3_new%>%\n  filter(ip!=\"37.221.172.194\")\n\nexpertise3_new <- expertise3_new %>%\n  filter(!(ip %in% c(\"77.198.10.26\", \"83.233.218.246\", \"189.172.66.106\", \"190.167.6.137\")))\n\n##selecting the columns that we want to keep ----\nexpertise3_new<-expertise3_new%>%\n  select(-finished,-otherways,-birthdate,-proceure_confu,-whatwestudied,-moretothisstudy,-additional_thoughts,-attention)\n\n#adds a column to the dataframe, with the name \"id\"\nexpertise3_new<-cbind(ID = 1:nrow(expertise3_new), expertise3_new)\n\n# Numeric variables ----\n# Change the data type of the variables to numeric \nexpertise3_new <- expertise3_new %>%\n  mutate_at(vars(stih_lang, r_ih_lang, stih_school, r_ih_school, stih_cards, r_ih_cards, stih_breakfast, r_ih_breakfast, \n                 stih_weddings, r_ih_weddings, stih_teeth, r_ih_teeth, stih_traffic, r_ih_traffic, stih_tv, r_ih_tv), as.numeric)\n```\n:::\n\n\nCheck the correlations between inherence (the variables starting with st) and reverse inherence (the variables starting with r) scores to check whether it's appropriate for averaging\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Correlations between ih scores ----\n\n# Create a list of variable names\nvariables <- c(\"stih_lang\", \"r_ih_lang\", \"stih_school\", \"r_ih_school\", \"stih_cards\", \"r_ih_cards\", \"stih_breakfast\", \"r_ih_breakfast\", \"stih_weddings\", \"r_ih_weddings\", \"stih_teeth\", \"r_ih_teeth\", \"stih_traffic\", \"r_ih_traffic\", \"stih_tv\", \"r_ih_tv\")\n\n# Initialize an empty data frame to store the correlation coefficients\ncorrelations <- data.frame(variable1 = character(), variable2 = character(), correlation = numeric(), p.value = numeric(), conf.int = character())\n\n# Iterate over the pairs of variables\nfor (i in seq(1, length(variables), 2)) {\n  j <- i + 1\n  \n  # Calculate the Pearson correlation coefficient and test the statistical significance\n  correlation_test <- cor.test(expertise3_new[, variables[i]], expertise3_new[, variables[j]], method = \"pearson\")\n  \n  # Add the correlation coefficient, p-value, and confidence interval to the data frame\n  correlations <- rbind(correlations, data.frame(variable1 = variables[i], variable2 = variables[j], correlation = correlation_test$estimate, p.value = correlation_test$p.value, conf.int = paste(correlation_test$conf.int[1], correlation_test$conf.int[2], sep = \" - \")))\n}\n\n## View the correlation coefficients and statistical measures ----\ncorrelations\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          variable1      variable2 correlation      p.value\ncor       stih_lang      r_ih_lang  -0.6112615 8.942654e-22\ncor1    stih_school    r_ih_school  -0.5181904 4.547542e-15\ncor2     stih_cards     r_ih_cards  -0.5395115 2.001074e-16\ncor3 stih_breakfast r_ih_breakfast  -0.3813141 2.751024e-08\ncor4  stih_weddings  r_ih_weddings  -0.5222404 2.554910e-15\ncor5     stih_teeth     r_ih_teeth  -0.4231281 5.283701e-10\ncor6   stih_traffic   r_ih_traffic  -0.4832966 4.822560e-13\ncor7        stih_tv        r_ih_tv  -0.3396313 9.221208e-07\n                                    conf.int\ncor  -0.691555186062648 - -0.516045968176506\ncor1  -0.613091701381407 - -0.40854648901427\ncor2 -0.631233272015308 - -0.432907170777665\ncor3 -0.494193465446722 - -0.255790470305453\ncor4  -0.61654545073014 - -0.413161868080018\ncor5 -0.531226389630016 - -0.301474099514304\ncor6  -0.583183953827757 - -0.36901235203396\ncor7 -0.457126840023376 - -0.210484471909784\n```\n:::\n:::\n\n\nIt seems that each pairs have negative significant correlation, so we can take the average scores to calculate inherence scores\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Average of ih scores  ----\n#It seems that each pairs have negative significant correlation, so we can take the average scores to measure inherence scores\nexpertise3_new <- expertise3_new %>%\n  mutate(ih_lang = (stih_lang + (10 - r_ih_lang))/2,\n         ih_school = (stih_school + (10 - r_ih_school)) / 2,\n         ih_cards = (stih_cards + (10 - r_ih_cards)) / 2,\n         ih_breakfast = (stih_breakfast + (10 - r_ih_breakfast)) / 2,\n         ih_weddings = (stih_weddings + (10 - r_ih_weddings)) / 2,\n         ih_teeth = (stih_teeth + (10 - r_ih_teeth)) / 2,\n         ih_traffic = (stih_traffic + (10-r_ih_traffic)) / 2,\n         ih_tv = (stih_tv + (10-r_ih_tv)) / 2 )\n```\n:::\n\n\nCalculate \"Need for cognition\" scale scores\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Need for cognition scale scores  ----\n\n# change the data type of the variables to numeric\nexpertise3_new <- expertise3_new %>%\n  mutate_at(vars(needforcog1,needforcog2,needforcog3,needforcog4,needforcog5,needforcog6\n                 ,needforcog7,needforcog8,needforcog9,needforcog10,needforcog11,needforcog12,\n                 needforcog13,needforcog14,needforcog15,needforcog16,needforcog17,needforcog18), as.numeric)\n\n## Calculate needforcog scores  ----\n#add a new variable called needforcog, which is the sum of all the need for cognition items, the items are weighted according to the scoring key\nexpertise3_new <- expertise3_new %>%\n  group_by(ID)%>%\n  mutate(needforcog=(needforcog1+needforcog2+\n                       (10-needforcog3)+(10-needforcog4)+(10-needforcog5)+needforcog6+\n                       (10-needforcog7)+(10-needforcog8)+(10-needforcog9)+\n                       needforcog10+needforcog11+(10-needforcog12)+\n                       needforcog13+needforcog14+needforcog15+\n                       (10-needforcog16)+(10-needforcog17)+needforcog18)/18)\n```\n:::\n\n\nPrepare the expertise scores and other scores ready for analyses\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Replace expertise variables' NA values in the expertise columns with 0  ----\nvariables <- c('high_lang', 'colle_lang', 'grad_lang',\n               'numbook_lang','numarticle_lang',\n               'high_school', 'colle_school', 'grad_school',\n               'numbook_school','numarticle_school',\n               'high_cards', 'colle_cards', 'grad_cards',\n               'numbook_cards','numarticle_cards',\n               'high_breakfast', 'colle_breakfast', 'grad_breakfast',\n               'numbook_breakfast','numarticle_breakfast',\n               'high_weddings', 'colle_weddings', 'grad_weddings',\n               'numbook_weddings','numarticle_weddings',\n               'high_teeth', 'colle_teeth', 'grad_teeth',\n               'numbook_teeth','numarticle_teeth',\n               'high_traffic', 'colle_traffic', 'grad_traffic',\n               'numbook_traffic','numarticle_traffic',\n               'high_tv', 'colle_tv', 'grad_tv',\n               'numbook_tv','numarticle_tv')\n\nexpertise3_new[variables] <- lapply(expertise3_new[variables], \n                                    function(x) ifelse(is.na(x), 0, ifelse(x=='no',0,x)))\n\n# Missing values in the dataframe  ----\napply(is.na(expertise3_new), 2, sum)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                  ID                   ip             duration \n                   0                    0                    0 \n           stih_lang            r_ih_lang          stih_school \n                   0                    0                    0 \n         r_ih_school           stih_cards           r_ih_cards \n                   0                    0                    0 \n      stih_breakfast       r_ih_breakfast        stih_weddings \n                   0                    0                    0 \n       r_ih_weddings           stih_teeth           r_ih_teeth \n                   0                    1                    1 \n        stih_traffic         r_ih_traffic              stih_tv \n                   0                    0                    0 \n             r_ih_tv            know_lang          know_school \n                   0                    0                    0 \n          know_cards       know_breakfast        know_weddings \n                   0                    0                    0 \n          know_teeth         know_traffic              know_tv \n                   0                    1                    0 \n         course_lang            high_lang           colle_lang \n                   0                    0                    0 \n           grad_lang            book_lang         numbook_lang \n                   0                    0                    0 \n        article_lang      numarticle_lang        course_school \n                   0                    0                    1 \n         high_school         colle_school          grad_school \n                   0                    0                    0 \n         book_school       numbook_school       article_school \n                   0                    0                    1 \n   numarticle_school         course_cards           high_cards \n                   0                    0                    0 \n         colle_cards           grad_cards           book_cards \n                   0                    0                    0 \n       numbook_cards        article_cards     numarticle_cards \n                   0                    1                    0 \n    course_breakfast       high_breakfast      colle_breakfast \n                   0                    0                    0 \n      grad_breakfast       book_breakfast    numbook_breakfast \n                   0                    0                    0 \n   article_breakfast numarticle_breakfast      course_weddings \n                   1                    0                    0 \n       high_weddings       colle_weddings        grad_weddings \n                   0                    0                    0 \n       book_weddings     numbook_weddings     article_weddings \n                   1                    0                    0 \n numarticle_weddings         course_teeth           high_teeth \n                   0                    0                    0 \n         colle_teeth           grad_teeth           book_teeth \n                   0                    0                    0 \n       numbook_teeth        article_teeth     numarticle_teeth \n                   0                    0                    0 \n      course_traffic         high_traffic        colle_traffic \n                   0                    0                    0 \n        grad_traffic         book_traffic      numbook_traffic \n                   0                    0                    0 \n     article_traffic   numarticle_traffic            course_tv \n                   0                    0                    0 \n             high_tv             colle_tv              grad_tv \n                   0                    0                    0 \n             book_tv           numbook_tv           article_tv \n                   0                    0                    1 \n       numarticle_tv          needforcog1          needforcog2 \n                   0                    0                    0 \n         needforcog3          needforcog4          needforcog5 \n                   0                    0                    0 \n         needforcog6          needforcog7          needforcog8 \n                   0                    0                    0 \n         needforcog9         needforcog10         needforcog11 \n                   0                    0                    0 \n        needforcog12         needforcog13         needforcog14 \n                   0                    0                    0 \n        needforcog15         needforcog16         needforcog17 \n                   0                    0                    0 \n        needforcog18                  sex            education \n                   0                    0                    0 \n              income             religion             identity \n                   2                    2                    0 \n                 age       political_atti        english_level \n                   0                    0                    0 \n             ih_lang            ih_school             ih_cards \n                   0                    0                    0 \n        ih_breakfast          ih_weddings             ih_teeth \n                   0                    0                    1 \n          ih_traffic                ih_tv           needforcog \n                   0                    0                    0 \n```\n:::\n\n```{.r .cell-code}\n# Expertise Ready Df ----\n\n# Create new data frame as analyzable \nexpertise3_new<-expertise3_new%>%\n  select(-stih_lang,-r_ih_lang,-stih_school,-r_ih_school,\n         -stih_cards,-r_ih_cards,-stih_breakfast,-r_ih_breakfast,\n         -stih_weddings,-r_ih_weddings,\n         -stih_teeth,-r_ih_teeth,-stih_traffic,-r_ih_traffic,\n         -stih_tv,-r_ih_tv,\n         -course_lang,\n         -book_lang,-article_lang,\n         -course_school,\n         -book_school,-article_school,\n         -course_cards,\n         -book_cards,-article_cards,\n         -course_breakfast,\n         -book_breakfast,-article_breakfast,\n         -course_weddings,\n         -book_weddings,-article_weddings,\n         -course_teeth,\n         -book_teeth,-article_teeth,\n         -course_traffic,\n         -book_traffic,-article_traffic,\n         -course_tv,\n         -book_tv,-article_tv,\n         -needforcog1,-needforcog2,-needforcog3,-needforcog4,-needforcog5,-needforcog6,\n         -needforcog7,-needforcog8,-needforcog9,-needforcog10,-needforcog11,-needforcog12,\n         -needforcog13,-needforcog14,-needforcog15,-needforcog16,-needforcog17,-needforcog18)\n\n\n# change the data type of the variables to numeric\nexpertise3_new <- expertise3_new %>%\n  mutate_at(vars('know_lang','know_school','know_cards','know_breakfast',\n                 'know_weddings','know_teeth','know_traffic','know_tv',\n                 'high_lang', 'colle_lang', 'grad_lang',\n                 'numbook_lang','numarticle_lang',\n                 'high_school', 'colle_school', 'grad_school',\n                 'numbook_school','numarticle_school',\n                 'high_cards', 'colle_cards', 'grad_cards',\n                 'numbook_cards','numarticle_cards',\n                 'high_breakfast', 'colle_breakfast', 'grad_breakfast',\n                 'numbook_breakfast','numarticle_breakfast',\n                 'high_weddings', 'colle_weddings', 'grad_weddings',\n                 'numbook_weddings','numarticle_weddings',\n                 'high_teeth', 'colle_teeth', 'grad_teeth',\n                 'numbook_teeth','numarticle_teeth',\n                 'high_traffic', 'colle_traffic', 'grad_traffic',\n                 'numbook_traffic','numarticle_traffic',\n                 'high_tv', 'colle_tv', 'grad_tv',\n                 'numbook_tv','numarticle_tv'), as.numeric)\n```\n:::\n\n\nLong format of expertise dataset for factor analysis before standardization\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary('reshape2')\n\nexpertise3_factor <- melt(expertise3_new, id.vars = c(\"ID\",'duration',\"sex\" ,\"education\" ,\"income\",\"religion\",'identity','age','political_atti','english_level','needforcog'), \n                        measure.vars = c(\"ih_lang\", \"ih_school\",\"ih_cards\", \"ih_breakfast\", \"ih_weddings\", \n                                         \"ih_teeth\", \"ih_traffic\", \"ih_tv\",'know_lang','know_school','know_cards','know_breakfast',\n                                         'know_weddings','know_teeth','know_traffic','know_tv',\n                                         'high_lang', 'colle_lang', 'grad_lang',\n                                         'numbook_lang','numarticle_lang',\n                                         'high_school', 'colle_school', 'grad_school',\n                                         'numbook_school','numarticle_school',\n                                         'high_cards', 'colle_cards', 'grad_cards',\n                                         'numbook_cards','numarticle_cards',\n                                         'high_breakfast', 'colle_breakfast', 'grad_breakfast',\n                                         'numbook_breakfast','numarticle_breakfast',\n                                         'high_weddings', 'colle_weddings', 'grad_weddings',\n                                         'numbook_weddings','numarticle_weddings',\n                                         'high_teeth', 'colle_teeth', 'grad_teeth',\n                                         'numbook_teeth','numarticle_teeth',\n                                         'high_traffic', 'colle_traffic', 'grad_traffic',\n                                         'numbook_traffic','numarticle_traffic',\n                                         'high_tv', 'colle_tv', 'grad_tv',\n                                         'numbook_tv','numarticle_tv'),\n                        sep = \"_\", variable.name = \"Category\", value.name = \"Score\")\n\n# Split the Category column into two columns based on the underscore separator\nexpertise3_factor <- expertise3_factor %>% separate(Category, into = c(\"Category\", \"Score_Type\"), sep = \"_\")\n\n## spread the data from long to wide format  ----\nexpertise3_fact <- expertise3_factor %>% spread(Category, Score)\n\n# change the score type to a factor\nexpertise3_fact$Score_Type<-as.factor(expertise3_fact$Score_Type)\n\n# convert the column ih to numeric\nexpertise3_fact$ih<-as.numeric(expertise3_fact$ih)\n```\n:::\n\n\nFactor analysis for expertise variables with raw scores\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Import packages \n\nlibrary(psych) #PCA/EFA analysis\nlibrary(REdaS) #Produces KMO and Bartletts test\n\n# Create a new dataframe that include only related variables\nfactor_exp0<-expertise3_fact%>%\n  select(colle, grad, high, numarticle, numbook)\n\n# Check missing values\napply(is.na(factor_exp0), 2, sum)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     colle       grad       high numarticle    numbook \n         0          0          0          0          0 \n```\n:::\n\n```{.r .cell-code}\n# Since grad classes for TV category is missing (nobody takes any class in the sample), listwise deletion is applied here.\nbart_spher(factor_exp0, use = \"complete.obs\") ###### produces Bartletts test of spherecity \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\tBartlett's Test of Sphericity\n\nCall: bart_spher(x = factor_exp0, use = \"complete.obs\")\n\n     X2 = 1028.145\n     df = 10\np-value < 2.22e-16\n```\n:::\n\n```{.r .cell-code}\nKMO(factor_exp0)       ###### Kaiser-Meyer-Olkin measure, which is above .5.\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nKaiser-Meyer-Olkin factor adequacy\nCall: KMO(r = factor_exp0)\nOverall MSA =  0.58\nMSA for each item = \n     colle       grad       high numarticle    numbook \n      0.57       0.59       0.56       0.56       0.60 \n```\n:::\n\n```{.r .cell-code}\n# Let's check all the variables\nfa(factor_exp0, nfactors = 5, rotate =  \"oblimin\" )  \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFactor Analysis using method =  minres\nCall: fa(r = factor_exp0, nfactors = 5, rotate = \"oblimin\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n             MR1   MR2   MR3   MR4 MR5   h2   u2 com\ncolle       0.73  0.01 -0.01  0.07   0 0.56 0.44 1.0\ngrad       -0.05  0.54 -0.05  0.08   0 0.31 0.69 1.1\nhigh        0.73 -0.03  0.00 -0.06   0 0.51 0.49 1.0\nnumarticle -0.05 -0.05  0.56  0.01   0 0.28 0.72 1.0\nnumbook     0.14  0.37  0.38  0.06   0 0.57 0.43 2.3\n\n                       MR1  MR2  MR3  MR4  MR5\nSS loadings           1.11 0.53 0.52 0.08 0.00\nProportion Var        0.22 0.11 0.10 0.02 0.00\nCumulative Var        0.22 0.33 0.43 0.45 0.45\nProportion Explained  0.50 0.24 0.23 0.03 0.00\nCumulative Proportion 0.50 0.73 0.97 1.00 1.00\n\n With factor correlations of \n     MR1  MR2  MR3  MR4 MR5\nMR1 1.00 0.32 0.24 0.15   0\nMR2 0.32 1.00 0.52 0.72   0\nMR3 0.24 0.52 1.00 0.30   0\nMR4 0.15 0.72 0.30 1.00   0\nMR5 0.00 0.00 0.00 0.00   1\n\nMean item complexity =  1.3\nTest of the hypothesis that 5 factors are sufficient.\n\nThe degrees of freedom for the null model are  10  and the objective function was  0.65 with Chi Square of  1028.15\nThe degrees of freedom for the model are -5  and the objective function was  0 \n\nThe root mean square of the residuals (RMSR) is  0 \nThe df corrected root mean square of the residuals is  NA \n\nThe harmonic number of observations is  1592 with the empirical chi square  0  with prob <  NA \nThe total number of observations was  1592  with Likelihood Chi Square =  0  with prob <  NA \n\nTucker Lewis Index of factoring reliability =  1.01\nFit based upon off diagonal values = 1\nMeasures of factor score adequacy             \n                                                   MR1  MR2  MR3   MR4 MR5\nCorrelation of (regression) scores with factors   0.84 0.75 0.72  0.57   0\nMultiple R square of scores with factors          0.71 0.57 0.52  0.32   0\nMinimum correlation of possible factor scores     0.42 0.14 0.04 -0.36  -1\n```\n:::\n\n```{.r .cell-code}\n# We may reduce it to 2 factors like media and classes\nfa(factor_exp0, nfactors = 2, rotate =  \"oblimin\" )  \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFactor Analysis using method =  minres\nCall: fa(r = factor_exp0, nfactors = 2, rotate = \"oblimin\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n             MR1   MR2   h2    u2 com\ncolle       0.06  0.72 0.55 0.455 1.0\ngrad        0.36 -0.01 0.13 0.875 1.0\nhigh       -0.05  0.73 0.51 0.488 1.0\nnumarticle  0.33 -0.06 0.10 0.900 1.1\nnumbook     0.97  0.00 0.95 0.052 1.0\n\n                       MR1  MR2\nSS loadings           1.18 1.05\nProportion Var        0.24 0.21\nCumulative Var        0.24 0.45\nProportion Explained  0.53 0.47\nCumulative Proportion 0.53 1.00\n\n With factor correlations of \n     MR1  MR2\nMR1 1.00 0.35\nMR2 0.35 1.00\n\nMean item complexity =  1\nTest of the hypothesis that 2 factors are sufficient.\n\nThe degrees of freedom for the null model are  10  and the objective function was  0.65 with Chi Square of  1028.15\nThe degrees of freedom for the model are 1  and the objective function was  0 \n\nThe root mean square of the residuals (RMSR) is  0.01 \nThe df corrected root mean square of the residuals is  0.03 \n\nThe harmonic number of observations is  1592 with the empirical chi square  3.66  with prob <  0.056 \nThe total number of observations was  1592  with Likelihood Chi Square =  5.04  with prob <  0.025 \n\nTucker Lewis Index of factoring reliability =  0.96\nRMSEA index =  0.05  and the 90 % confidence intervals are  0.014 0.098\nBIC =  -2.34\nFit based upon off diagonal values = 1\nMeasures of factor score adequacy             \n                                                   MR1  MR2\nCorrelation of (regression) scores with factors   0.97 0.84\nMultiple R square of scores with factors          0.95 0.70\nMinimum correlation of possible factor scores     0.90 0.41\n```\n:::\n\n```{.r .cell-code}\nfactor_dia0<-fa(factor_exp0, nfactors = 2, rotate =  \"oblimin\" ) ##save the analysis as the object m1\nfa.diagram(factor_dia0,main=\"Expertise Variables\")  \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nIt is interesting that grad courses are more related to media category than classes although the correlation strength -r values- were low. It may be related to active participation or effect of history (books/articles can be mostly read during grad courses or the remembered ones at least?). So here, in my sense, we may collect these items under active vs. passive involvement or contemporary (closer) expertise vs. older expertise.\n\nLong format of expertise dataset\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary('reshape2')\n\nexpertise3_long <- melt(expertise3_new, id.vars = c(\"ID\",'duration',\"sex\" ,\"education\" ,\"income\",\"religion\",'identity','age','political_atti','english_level','needforcog'), \n                        measure.vars = c(\"ih_lang\", \"ih_school\",\"ih_cards\", \"ih_breakfast\", \"ih_weddings\", \n                                         \"ih_teeth\", \"ih_traffic\", \"ih_tv\",'know_lang','know_school','know_cards','know_breakfast',\n                                         'know_weddings','know_teeth','know_traffic','know_tv',\n                                         'high_lang', 'colle_lang', 'grad_lang',\n                                         'numbook_lang','numarticle_lang',\n                                         'high_school', 'colle_school', 'grad_school',\n                                         'numbook_school','numarticle_school',\n                                         'high_cards', 'colle_cards', 'grad_cards',\n                                         'numbook_cards','numarticle_cards',\n                                         'high_breakfast', 'colle_breakfast', 'grad_breakfast',\n                                         'numbook_breakfast','numarticle_breakfast',\n                                         'high_weddings', 'colle_weddings', 'grad_weddings',\n                                         'numbook_weddings','numarticle_weddings',\n                                         'high_teeth', 'colle_teeth', 'grad_teeth',\n                                         'numbook_teeth','numarticle_teeth',\n                                         'high_traffic', 'colle_traffic', 'grad_traffic',\n                                         'numbook_traffic','numarticle_traffic',\n                                         'high_tv', 'colle_tv', 'grad_tv',\n                                         'numbook_tv','numarticle_tv'),\n                        sep = \"_\", variable.name = \"Category\", value.name = \"Score\")\n\n# Split the Category column into two columns based on the underscore separator\nexpertise3_long <- expertise3_long %>% separate(Category, into = c(\"Category\", \"Score_Type\"), sep = \"_\")\n\n## spread the data from long to wide format  ----\nexpertise3_ready <- expertise3_long %>% spread(Category, Score)\n\n# change the score type to a factor\nexpertise3_ready$Score_Type<-as.factor(expertise3_ready$Score_Type)\n\n# convert the column ih to numeric\nexpertise3_ready$ih<-as.numeric(expertise3_ready$ih)\n```\n:::\n\n\nFactor analysis for expertise variables with standardized scores\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Import packages \n\nlibrary(psych) #PCA/EFA analysis\nlibrary(REdaS) #Produces KMO and Bartletts test\n\n# Create a new dataframe that include only related variables\nfactor_exp<-expertise3_ready%>%\n  select(colle, grad, high, numarticle, numbook)\n\n# Check missing values\napply(is.na(factor_exp), 2, sum)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     colle       grad       high numarticle    numbook \n         0          0          0          0          0 \n```\n:::\n\n```{.r .cell-code}\n# Since grad classes for TV category is missing (nobody takes any class in the sample), listwise deletion is applied here.\nbart_spher(factor_exp, use = \"complete.obs\") ###### produces Bartletts test of spherecity \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\tBartlett's Test of Sphericity\n\nCall: bart_spher(x = factor_exp, use = \"complete.obs\")\n\n     X2 = 1028.145\n     df = 10\np-value < 2.22e-16\n```\n:::\n\n```{.r .cell-code}\nKMO(factor_exp)       ###### Kaiser-Meyer-Olkin measure, which is above .5.\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nKaiser-Meyer-Olkin factor adequacy\nCall: KMO(r = factor_exp)\nOverall MSA =  0.58\nMSA for each item = \n     colle       grad       high numarticle    numbook \n      0.57       0.59       0.56       0.56       0.60 \n```\n:::\n\n```{.r .cell-code}\n# Let's check all the variables\nfa(factor_exp, nfactors = 5, rotate =  \"oblimin\" )  \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFactor Analysis using method =  minres\nCall: fa(r = factor_exp, nfactors = 5, rotate = \"oblimin\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n             MR1   MR2   MR3   MR4 MR5   h2   u2 com\ncolle       0.73  0.01 -0.01  0.07   0 0.56 0.44 1.0\ngrad       -0.05  0.54 -0.05  0.08   0 0.31 0.69 1.1\nhigh        0.73 -0.03  0.00 -0.06   0 0.51 0.49 1.0\nnumarticle -0.05 -0.05  0.56  0.01   0 0.28 0.72 1.0\nnumbook     0.14  0.37  0.38  0.06   0 0.57 0.43 2.3\n\n                       MR1  MR2  MR3  MR4  MR5\nSS loadings           1.11 0.53 0.52 0.08 0.00\nProportion Var        0.22 0.11 0.10 0.02 0.00\nCumulative Var        0.22 0.33 0.43 0.45 0.45\nProportion Explained  0.50 0.24 0.23 0.03 0.00\nCumulative Proportion 0.50 0.73 0.97 1.00 1.00\n\n With factor correlations of \n     MR1  MR2  MR3  MR4 MR5\nMR1 1.00 0.32 0.24 0.15   0\nMR2 0.32 1.00 0.52 0.72   0\nMR3 0.24 0.52 1.00 0.30   0\nMR4 0.15 0.72 0.30 1.00   0\nMR5 0.00 0.00 0.00 0.00   1\n\nMean item complexity =  1.3\nTest of the hypothesis that 5 factors are sufficient.\n\nThe degrees of freedom for the null model are  10  and the objective function was  0.65 with Chi Square of  1028.15\nThe degrees of freedom for the model are -5  and the objective function was  0 \n\nThe root mean square of the residuals (RMSR) is  0 \nThe df corrected root mean square of the residuals is  NA \n\nThe harmonic number of observations is  1592 with the empirical chi square  0  with prob <  NA \nThe total number of observations was  1592  with Likelihood Chi Square =  0  with prob <  NA \n\nTucker Lewis Index of factoring reliability =  1.01\nFit based upon off diagonal values = 1\nMeasures of factor score adequacy             \n                                                   MR1  MR2  MR3   MR4 MR5\nCorrelation of (regression) scores with factors   0.84 0.75 0.72  0.57   0\nMultiple R square of scores with factors          0.71 0.57 0.52  0.32   0\nMinimum correlation of possible factor scores     0.42 0.14 0.04 -0.36  -1\n```\n:::\n\n```{.r .cell-code}\n# So we can reduce it to 2 factors\nfa(factor_exp, nfactors = 2, rotate =  \"oblimin\" )  \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFactor Analysis using method =  minres\nCall: fa(r = factor_exp, nfactors = 2, rotate = \"oblimin\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n             MR1   MR2   h2    u2 com\ncolle       0.06  0.72 0.55 0.455 1.0\ngrad        0.36 -0.01 0.13 0.875 1.0\nhigh       -0.05  0.73 0.51 0.488 1.0\nnumarticle  0.33 -0.06 0.10 0.900 1.1\nnumbook     0.97  0.00 0.95 0.052 1.0\n\n                       MR1  MR2\nSS loadings           1.18 1.05\nProportion Var        0.24 0.21\nCumulative Var        0.24 0.45\nProportion Explained  0.53 0.47\nCumulative Proportion 0.53 1.00\n\n With factor correlations of \n     MR1  MR2\nMR1 1.00 0.35\nMR2 0.35 1.00\n\nMean item complexity =  1\nTest of the hypothesis that 2 factors are sufficient.\n\nThe degrees of freedom for the null model are  10  and the objective function was  0.65 with Chi Square of  1028.15\nThe degrees of freedom for the model are 1  and the objective function was  0 \n\nThe root mean square of the residuals (RMSR) is  0.01 \nThe df corrected root mean square of the residuals is  0.03 \n\nThe harmonic number of observations is  1592 with the empirical chi square  3.66  with prob <  0.056 \nThe total number of observations was  1592  with Likelihood Chi Square =  5.04  with prob <  0.025 \n\nTucker Lewis Index of factoring reliability =  0.96\nRMSEA index =  0.05  and the 90 % confidence intervals are  0.014 0.098\nBIC =  -2.34\nFit based upon off diagonal values = 1\nMeasures of factor score adequacy             \n                                                   MR1  MR2\nCorrelation of (regression) scores with factors   0.97 0.84\nMultiple R square of scores with factors          0.95 0.70\nMinimum correlation of possible factor scores     0.90 0.41\n```\n:::\n:::\n\n\nFigure for the analysis\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Figure for the analysis\n\nfactor_dia1<-fa(factor_exp, nfactors = 2, rotate =  \"oblimin\" ) ##save the analysis as the object m1\nfa.diagram(factor_dia1,main=\"Expertise Variables\")  \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nWe can start to analyze our models with using hlm:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Import packages ----\nlibrary(lme4) \n\nlibrary(lmerTest)\n\n## Model 1 ----\n\n# Need for cognition and perceived expertise as fixed effects\nModel.1<-lmer(ih ~needforcog*know+(1|Score_Type)+(1|ID),   \n              data=expertise3_ready)\nsummary(Model.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: ih ~ needforcog * know + (1 | Score_Type) + (1 | ID)\n   Data: expertise3_ready\n\nREML criterion at convergence: 6306.8\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.86029 -0.65622 -0.01566  0.60575  2.91171 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.4976   0.7054  \n Score_Type (Intercept) 0.3950   0.6285  \n Residual               2.6737   1.6352  \nNumber of obs: 1590, groups:  ID, 199; Score_Type, 8\n\nFixed effects:\n                  Estimate Std. Error         df t value Pr(>|t|)    \n(Intercept)      4.997e+00  5.355e-01  1.788e+02   9.332  < 2e-16 ***\nneedforcog      -4.924e-02  7.979e-02  6.742e+02  -0.617  0.53738    \nknow             2.462e-02  8.385e-03  1.512e+03   2.936  0.00338 ** \nneedforcog:know -3.528e-03  1.364e-03  1.536e+03  -2.587  0.00978 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) ndfrcg know  \nneedforcog  -0.883              \nknow        -0.707  0.748       \nnedfrcg:knw  0.687 -0.776 -0.969\n```\n:::\n\n```{.r .cell-code}\nconfint(Model.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                       2.5 %        97.5 %\n.sig01           0.587590004  0.8248621076\n.sig02           0.374383811  1.0706622905\n.sigma           1.575182111  1.6970943748\n(Intercept)      3.957100675  6.0397943974\nneedforcog      -0.205646024  0.1069229731\nknow             0.008125139  0.0410084056\nneedforcog:know -0.006198133 -0.0008540352\n```\n:::\n:::\n\n\nSo it seems that the interaction between perceived knowledge and need for cognition are related to inherence significantly. Interestingly, this interaction is negatively related to inherence scores. We may argue that perceived knowledge is misleading, but need for cognition moderates this relation as an individual difference. Therefore, people seek out tasks that challenge their abilities may show less inherent bias contained explanations even if they think they know that area (maybe false, maybe true). It may be related to Dunning--Kruger effect (\\[Kruger & Dunning, 1999\\](https://www.sciencedirect.com/science/article/pii/B9780123855220000056#bb0315)). People may overestimate their abilities, but judge the explanations with inherence bias. Not suprisingly, people who are seeking for cognitive activities (need for cognition) may also show more effort for the explanations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Model 2----\n\n# Need for cognition and courses without perceived expertise\nModel.2<-lmer(ih ~colle*grad*high*needforcog+(1|Score_Type)+(1|ID),   \n              data=expertise3_ready)\nsummary(Model.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: ih ~ colle * grad * high * needforcog + (1 | Score_Type) + (1 |  \n    ID)\n   Data: expertise3_ready\n\nREML criterion at convergence: 6342.3\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-3.06886 -0.65955 -0.00945  0.60546  2.92112 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.5140   0.7169  \n Score_Type (Intercept) 0.3412   0.5841  \n Residual               2.6709   1.6343  \nNumber of obs: 1591, groups:  ID, 199; Score_Type, 8\n\nFixed effects:\n                             Estimate Std. Error         df t value Pr(>|t|)\n(Intercept)                   5.87258    0.38024   63.47208  15.444  < 2e-16\ncolle                         0.54561    0.27133 1545.86757   2.011  0.04451\ngrad                         -0.85560    1.68374 1508.30307  -0.508  0.61142\nhigh                          0.66059    0.30457 1516.73954   2.169  0.03024\nneedforcog                   -0.17360    0.05241  218.11623  -3.312  0.00108\ncolle:grad                   -0.46827    0.55591 1516.80044  -0.842  0.39973\ncolle:high                   -0.20613    0.10441 1497.78661  -1.974  0.04853\ngrad:high                     3.82571    3.16695 1539.49890   1.208  0.22723\ncolle:needforcog             -0.07348    0.03947 1539.44580  -1.862  0.06284\ngrad:needforcog               0.11034    0.22311 1508.41103   0.495  0.62099\nhigh:needforcog              -0.11272    0.04952 1513.55100  -2.276  0.02298\ncolle:grad:high              -0.37437    1.12072 1549.26610  -0.334  0.73839\ncolle:grad:needforcog         0.06625    0.07437 1517.29021   0.891  0.37319\ncolle:high:needforcog         0.03566    0.01880 1491.09477   1.897  0.05801\ngrad:high:needforcog         -0.47626    0.40288 1544.15953  -1.182  0.23733\ncolle:grad:high:needforcog    0.04489    0.13863 1549.67710   0.324  0.74614\n                              \n(Intercept)                ***\ncolle                      *  \ngrad                          \nhigh                       *  \nneedforcog                 ** \ncolle:grad                    \ncolle:high                 *  \ngrad:high                     \ncolle:needforcog           .  \ngrad:needforcog               \nhigh:needforcog            *  \ncolle:grad:high               \ncolle:grad:needforcog         \ncolle:high:needforcog      .  \ngrad:high:needforcog          \ncolle:grad:high:needforcog    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n```{.r .cell-code}\nconfint(Model.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                                  2.5 %       97.5 %\n.sig01                      0.598606290  0.835634616\n.sig02                      0.347419790  0.996378660\n.sigma                      1.568004029  1.689350498\n(Intercept)                 5.134572967  6.610505700\ncolle                       0.016649477  1.076093721\ngrad                       -4.140756108  2.432766190\nhigh                        0.066832480  1.256153612\nneedforcog                 -0.276131710 -0.071002920\ncolle:grad                 -1.554365884  0.616436828\ncolle:high                 -0.410147597 -0.002428289\ngrad:high                  -2.365704723 10.002012570\ncolle:needforcog           -0.150598433  0.003505473\ngrad:needforcog            -0.325405201  0.545660671\nhigh:needforcog            -0.209484183 -0.016083268\ncolle:grad:high            -2.560835721  1.816788781\ncolle:grad:needforcog      -0.078858868  0.211548900\ncolle:high:needforcog      -0.001028691  0.072367667\ngrad:high:needforcog       -1.261993522  0.311527501\ncolle:grad:high:needforcog -0.226163204  0.315346380\n```\n:::\n:::\n\n\nThe second model includes what I call true expertise with classes. Here only the interaction between high school courses and need for cognition is significantly related to inherence bias. This may be misleading because after new analyses with a latent variable (only classes), this effect may not be there. However, if we would interpret this result, I would probably say that high school classes may work as perceived knowledge, they are probably superficial and forgotten already in adulthood, so even if they took these courses, they didn't become experts and evaluate the inherent bias included sentences are more satisfactory.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Model 3 ----\n\n# Books and need for cognition as fixed effects\nModel.3<-lmer(ih ~numarticle*numbook*needforcog+(1|Score_Type)+(1|ID),   \n              data=expertise3_ready)\nsummary(Model.3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: ih ~ numarticle * numbook * needforcog + (1 | Score_Type) + (1 |  \n    ID)\n   Data: expertise3_ready\n\nREML criterion at convergence: 6361.4\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-3.08300 -0.65140 -0.01693  0.60347  2.90687 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.5231   0.7232  \n Score_Type (Intercept) 0.3524   0.5936  \n Residual               2.6785   1.6366  \nNumber of obs: 1591, groups:  ID, 199; Score_Type, 8\n\nFixed effects:\n                                Estimate Std. Error         df t value Pr(>|t|)\n(Intercept)                    6.083e+00  3.860e-01  6.373e+01  15.761  < 2e-16\nnumarticle                     6.222e-03  1.108e-02  1.560e+03   0.561 0.574609\nnumbook                       -2.822e-02  1.011e-01  1.572e+03  -0.279 0.780157\nneedforcog                    -1.981e-01  5.332e-02  2.238e+02  -3.714 0.000257\nnumarticle:numbook            -6.733e-04  1.921e-03  1.562e+03  -0.350 0.726011\nnumarticle:needforcog         -1.545e-03  1.817e-03  1.468e+03  -0.850 0.395409\nnumbook:needforcog             2.001e-03  1.522e-02  1.572e+03   0.132 0.895396\nnumarticle:numbook:needforcog  1.241e-04  2.688e-04  1.566e+03   0.462 0.644375\n                                 \n(Intercept)                   ***\nnumarticle                       \nnumbook                          \nneedforcog                    ***\nnumarticle:numbook               \nnumarticle:needforcog            \nnumbook:needforcog               \nnumarticle:numbook:needforcog    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) nmrtcl numbok ndfrcg nmrtcl:nm nmrtcl:nd nmbk:n\nnumarticle  -0.142                                                \nnumbook     -0.157 -0.020                                         \nneedforcog  -0.821  0.169  0.178                                  \nnmrtcl:nmbk  0.109 -0.180 -0.780 -0.118                           \nnmrtcl:ndfr  0.142 -0.980  0.022 -0.178  0.179                    \nnmbk:ndfrcg  0.162  0.020 -0.993 -0.189  0.750    -0.020          \nnmrtcl:nmb: -0.114  0.213  0.775  0.127 -0.998    -0.221    -0.746\nfit warnings:\nSome predictor variables are on very different scales: consider rescaling\n```\n:::\n\n```{.r .cell-code}\nconfint(Model.3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                                     2.5 %        97.5 %\n.sig01                         0.604855443  0.8420590252\n.sig02                         0.353924573  1.0126366846\n.sigma                         1.574595027  1.6963782485\n(Intercept)                    5.333419261  6.8330466620\nnumarticle                    -0.015513078  0.0278662759\nnumbook                       -0.226234014  0.1693551715\nneedforcog                    -0.302556547 -0.0936433471\nnumarticle:numbook            -0.004431782  0.0030896405\nnumarticle:needforcog         -0.005094864  0.0020171548\nnumbook:needforcog            -0.027750007  0.0318199690\nnumarticle:numbook:needforcog -0.000402417  0.0006499776\n```\n:::\n:::\n\n\nThis is also interesting, books, articles and need for cognition interaction is significantly related to inherence scores and negatively (but the two way interaction between books and need for cognition is positive). Similar to previous model, if we would interpret this, I would say that people who read books about that topic (active participation/experience) may have less tendency for inherent bias and need for cognition moderates this relation, so if you read many books but less seek cognitive activities, you probably still have inherent bias more. Articles are interesting here because their two way interaction is negative and its correlation between books and need for cognition is negative, which is similar to high school classes and perceived expertise. It may be because magazine articles like from popular science area, etc. offers superficial information and sometimes false information. So even if you feel more knowledgeable in that area, it may be misleading here.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}