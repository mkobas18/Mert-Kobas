{
  "hash": "86b3034715caaf3466493c16ec5f7487",
  "result": {
    "markdown": "---\ntitle: \"Expertise3 Data Analysis from the very beginning\"\nauthor: \"MK\"\ndate: \"2022-12-26\"\ncategories: [example data, code, analysis]\nimage: \"data.png\"\n---\n\n\nThis post includes the trial analyses of an example data related to expertise.\n\nImport necessary packages and expertise data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(purrr)\n\nExpertise3 <- read_csv(\"~/Desktop/Expertise3_December 23, 2022_10.53.csv\")\n```\n:::\n\n\nCreates a new dataframe called expertise3_clean, which is a copy of the original dataframe called Expertise3 and removes the second row of the dataframe\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexpertise3_clean<-Expertise3%>% \n  slice(-2) \n\nexpertise3_clean <- expertise3_clean %>%\n  select(-StartDate, -EndDate, -Status, -IPAddress, -Progress,-ResponseId,-RecordedDate,\n         -RecipientLastName, -RecipientFirstName,-RecipientEmail, -ExternalReference, \n         -LocationLatitude,-LocationLongitude, -DistributionChannel, -UserLanguage)\n```\n:::\n\n\nCreate a variable called column_names and assign it the names of the columns in the dataframe and change the column names\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncolumn_names <- names(expertise3_clean)\ncolnames(expertise3_clean) <- c('duration', 'finished', 'stih_lang','r_ih_lang','stih_school','r_ih_school','stih_cards','r_ih_cards','stih_breakfast','r_ih_breakfast','stih_weddings','r_ih_weddings','stih_teeth','r_ih_teeth','stih_traffic','r_ih_traffic','stih_tv','r_ih_tv', 'know_lang','know_school','know_cards','know_breakfast','know_weddings','know_teeth','know_traffic','know_tv', 'course_lang','high_lang', 'colle_lang', 'grad_lang',\n'book_lang','numbook_lang','article_lang','numarticle_lang','course_school','high_school', 'colle_school', 'grad_school','book_school','numbook_school','article_school','numarticle_school','course_cards','high_cards', 'colle_cards', 'grad_cards',\n 'book_cards','numbook_cards','article_cards','numarticle_cards','course_breakfast','high_breakfast', 'colle_breakfast', 'grad_breakfast','book_breakfast','numbook_breakfast','article_breakfast','numarticle_breakfast','course_weddings','high_weddings', 'colle_weddings', 'grad_weddings','book_weddings','numbook_weddings','article_weddings','numarticle_weddings','course_teeth','high_teeth', 'colle_teeth', 'grad_teeth', 'book_teeth','numbook_teeth','article_teeth','numarticle_teeth','course_traffic','high_traffic', 'colle_traffic', 'grad_traffic','book_traffic','numbook_traffic','article_traffic','numarticle_traffic','course_tv','high_tv', 'colle_tv', 'grad_tv','book_tv','numbook_tv','article_tv','numarticle_tv','needforcog1','needforcog2','needforcog3','needforcog4','needforcog5','needforcog6','needforcog7','needforcog8','needforcog9','needforcog10','needforcog11','needforcog12','needforcog13','needforcog14','needforcog15','needforcog16','needforcog17','needforcog18','otherways','sex','birthdate','education','income','religion','identity','age','political_atti','english_level','proceure_confu','whatwestudied','moretothisstudy','additional_thoughts','attention')\n```\n:::\n\n\nThe code below shows the survey items:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#selects all columns except the ones specified\nrow_values <- expertise3_clean %>%\n  select(-duration,-finished)%>%\n  #selects only the first row\n  filter(row_number() == 1)\n\n#unlist the row_values\nrow_values <- unlist(row_values)\n\nmy_list <- map(row_values, ~paste0(.))\n\nlibrary(stringr)\nmy_list <- str_replace(my_list, \"(?<! )\\\\n(?! )\", \" \")\nmy_list <- str_replace(my_list, \"[^\\\\s]*\\\\\\\\n[^\\\\s]*\", \" \")\nlist_string <- paste0(\"* \", paste(my_list, collapse = \"\\n* \"))\n#Show the survey items\ncat(list_string)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n* Human languages are structured to best convey our thoughts and feelings. Words and their meanings likely form ideal matches.\n* There are absolutely no good reasons why we use specific words to represent our thoughts. Any combination of sounds\ncould in principle refer to any idea.\n* The fact that elementary school stops at 5th grade is probably ideal for children's learning. This is likely the best way to organize K-12 schooling.\n* Middle school (grades 6-8) is separate from elementary school (grades K-5) largely because of decisions made by educators a long time ago. This may not be the most optimal way of organizing early education.\n* It’s not a coincidence that we send people cards on holidays. This tradition seems\nparticularly fitting.\n* The fact that we send people cards on holidays is only a convention. A different way of sending warm wishes could've been implemented just as easily.\n* There are good reasons why orange juice is typically consumed for breakfast. There are features about it that make it\nparticularly suited for this meal (for example, its refreshing taste).\n* The current popularity of orange juice for breakfast reflects in part  marketing campaigns that promoted\ndrinking orange juice in the morning. However, had history taken a different\nturn, orange juice could just as easily have been more popular for lunch or\ndinner.\n* It seems right to use white for wedding dresses. Other colors, such as red and blue, have features that make them less suited for\nwedding dresses.\n* Even though white is the traditional color for wedding dresses, this could have easily been different.\nWhen you really think about it, there is no reason why other, brighter, colors couldn’t\nbe used for wedding dresses.\n* It seems ideal that toothpaste is typically flavored with mint. Mint is inherently more refreshing than any other\nflavor that currently exists.\n* When you think about it, toothpaste could have easily been flavored with something other than mint, such as cinnamon. Many pleasing flavors would work just as\nwell.\n* Traffic lights, with three different colored lights signaling three speeds, seem like the most efficient\nand effective way to direct traffic. Another process likely would not work as\nwell.\n* The current design of traffic lights, with three different colors reflecting three different speeds, is entirely due\nto historical factors. This is probably not the most efficient or effective way\nto manage traffic.\n* Black seems like a good choice for the color of televisions. Other colors just would not work as well.\n* The only reason why most TVs are black is\nhistorical happenstance. TVs could practically be a\nvariety of colors.\n* How much do you know about language and linguistics? - Please use the slider to select your answer choice.\n* How much do you know about school and education systems? - Please use the slider to select your answer choice.\n* How much do you know about holiday customs and traditions? - Please use the slider to select your answer choice.\n* How much do you know about breakfast foods? - Please use the slider to select your answer choice.\n* How much do you know about weddings and wedding traditions? - Please use the slider to select your answer choice.\n* How much do you know about teeth and oral hygiene? - Please use the slider to select your answer choice.\n* How much do you know about transportation science and traffic signal systems? - Please use the slider to select your answer choice.\n* How much do you know about the manufacturing of consumer electronics (TVs, MP3 players, camcorders, etc.)? - Please use the slider to select your answer choice.\n* Have you ever taken a class that discussed language and linguistics?\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - High School\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - College\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - Graduate School\n* Have you ever read any books on language and linguistics?\n* If you answered \"yes\" above, please approximate the number of books you have read on this topic:\n* Have you ever read any magazine, newspaper, or academic articles on language and linguistics?\n* If you answered \"yes\" above, please approximate the number of articles you have read on this topic:\n* Have you ever taken a class that discussed school and education systems?\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - High School\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - College\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - Graduate School\n* Have you ever read any books on school and education systems?\n* If you answered \"yes\" above, please approximate the number of books you have read on this topic:\n* Have you ever read any magazine, newspaper, or academic articles on school and education systems?\n* If you answered \"yes\" above, please approximate the number of articles you have read on this topic:\n* Have you ever taken a class that discussed holiday customs and traditions?\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - High School\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - College\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - Graduate School\n* Have you ever read any books on holiday customs and traditions?\n* If you answered \"yes\" above, please approximate the number of books you have read on this topic:\n* Have you ever read any magazine, newspaper, or academic articles on holiday customs and traditions?\n* If you answered \"yes\" above, please approximate the number of articles you have read on this topic:\n* Have you ever taken a class that discussed breakfast foods?\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - High School\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - College\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - Graduate School\n* Have you ever read any books on breakfast foods?\n* If you answered \"yes\" above, please approximate the number of books you have read on this topic:\n* Have you ever read any magazine, newspaper, or academic articles on breakfast foods?\n* If you answered \"yes\" above, please approximate the number of articles you have read on this topic:\n* Have you ever taken a class that discussed weddings and wedding traditions?\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - High School\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - College\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - Graduate School\n* Have you ever read any books on weddings and wedding traditions?\n* If you answered \"yes\" above, please approximate the number of books you have read on this topic:\n* Have you ever read any magazine, newspaper, or academic articles on weddings and wedding traditions?\n* If you answered \"yes\" above, please approximate the number of articles you have read on this topic:\n* Have you ever taken a class that discussed teeth and oral hygiene?\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - High School\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - College\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - Graduate School\n* Have you ever read any books on teeth and oral hygiene?\n* If you answered \"yes\" above, please approximate the number of books you have read on this topic:\n* Have you ever read any magazine, newspaper, or academic articles on teeth and oral hygiene?\n* If you answered \"yes\" above, please approximate the number of articles you have read on this topic:\n* Have you ever taken a class that discussed transportation science and traffic signal systems?\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - High School\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - College\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - Graduate School\n* Have you ever read any books on transportation science and traffic signal systems?\n* If you answered \"yes\" above, please approximate the number of books you have read on this topic:\n* Have you ever read any magazine, newspaper, or academic articles on transportation science and traffic signal systems?\n* If you answered \"yes\" above, please approximate the number of articles you have read on this topic:\n* Have you ever taken a class that discussed the manufacturing of consumer electronics (TVs, MP3 players, camcorders, etc.)?\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - High School\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - College\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - Graduate School\n* Have you ever read any books on the manufacturing of consumer electronics (TVs, MP3 players, camcorders, etc.)?\n* If you answered \"yes\" above, please approximate the number of books you have read on this topic:\n* Have you ever read any magazine, newspaper, or academic articles on the manufacturing of consumer electronics (TVs, MP3 players, camcorders, etc.)?\n* If you answered \"yes\" above, please approximate the number of articles you have read on this topic:\n* I would prefer complex to simple problems.\n* I like to have the responsibility of handling a situation that requires a lot of thinking.\n* Thinking is not my idea of fun.\n* I would rather do something that requires little thought than something that is sure to challenge my thinking abilities.\n* I try to anticipate and avoid situations where there is a likely chance I will have to think in depth about something.\n* I find satisfaction in deliberating hard and long for hours.\n* I only think as hard as I have to.\n* I prefer to think about small, daily projects rather than long-term ones.\n* I like tasks that require little thought once I’ve learned them.\n* The idea of relying on thought to make my way to the top appeals to me.\n* I really enjoy a task that involves coming up with new solutions to problems.\n* Learning new ways to think doesn’t excite me very much.\n* I prefer my life to be filled with puzzles that I must solve.\n* The notion of thinking abstractly appeals to me.\n* I would prefer a task that is intellectual, difficult, and important to one that is somewhat important but does not require much thought.\n* I feel relief rather than satisfaction after completing a task that required a lot of mental effort.\n* It’s enough for me that something gets the job done; I don’t care about why or how it works.\n* I usually end up deliberating about issues even when they do not affect me personally.\n* In this survey you were asked about your knowledge in a variety of subject areas. \nAside from the ways you may have learned about these topics that were mentioned in the survey (i.e., reading or  a college class), how else may you have learned about these topics? Please name other ways that you have learned about the topics mentioned in the survey you just took.\n* Are you male or female?\n* Q60 - What is your date of birth? (mm/dd/yyyy)\n* What is the highest level of education you have completed?\n* Q64 - What is your yearly household income?\n* Q64 - What is your religious affiliation?\n* Q64 - What is your racial or ethnic identity?\n* Q64 - What is your age in years?\n* How would you describe your political attitudes? Please select one of the points on the scale below.\n* Please rate your overall ability in the English language:\n* 1. Did you find any aspect of the procedure odd or confusing?\n* 2. What did you think we were studying?\n* 3. Do you think that there may have been more to this study than meets the eye? If so, what do you think this might have been?\n* 4. Do you have any additional thoughts or comments about the study?\n* Thank you for completing this survey! We just have one last question for you. You will not be penalized for your answer to this question. Since you completed the whole survey, you will receive payment no matter what answer you give here. \n\t \n\n\tIt's very important to the quality and scientific aims of our study that participants pay attention (i.e., read the survey carefully, consider the response options, and avoid distractions).\n\n\t\n\n\tWere you paying attention while completing this survey?\n```\n:::\n:::\n\n\nData preparation for further analyses\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Attention check and deletion of cases that didn't attend or finish the study\nexpertise3_new<-expertise3_clean%>%\n  filter(attention==1&finished==1)\n\n#Exclude the variables that will not be used in the later analyses\nexpertise3_new<-expertise3_new%>%\n  select(-finished,-otherways,-birthdate,-proceure_confu,-whatwestudied,-moretothisstudy,-additional_thoughts,-attention)\n\n#adds a column to the dataframe, with the name \"id\"\nexpertise3_new<-cbind(ID = 1:nrow(expertise3_new), expertise3_new)\n\n# change the data type of the variables to numeric\nexpertise3_new <- expertise3_new %>%\n  mutate_at(vars(stih_lang, r_ih_lang, stih_school, r_ih_school, stih_cards, r_ih_cards, stih_breakfast, r_ih_breakfast, \n                 stih_weddings, r_ih_weddings, stih_teeth, r_ih_teeth, stih_traffic, r_ih_traffic, stih_tv, r_ih_tv), as.numeric)\n\n# Create a list of variable names\nvariables <- c(\"stih_lang\", \"r_ih_lang\", \"stih_school\", \"r_ih_school\", \"stih_cards\", \"r_ih_cards\", \"stih_breakfast\", \"r_ih_breakfast\", \"stih_weddings\", \"r_ih_weddings\", \"stih_teeth\", \"r_ih_teeth\", \"stih_traffic\", \"r_ih_traffic\", \"stih_tv\", \"r_ih_tv\")\n```\n:::\n\n\nCheck the correlations between inherence (the variables starting with st) and reverse inherence (the variables starting with r) scores to check whether it's appropriate for averaging\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Initialize an empty data frame to store the correlation coefficients\ncorrelations <- data.frame(variable1 = character(), variable2 = character(), correlation = numeric(), p.value = numeric(), conf.int = character())\n\n# Iterate over the pairs of variables\nfor (i in seq(1, length(variables), 2)) {\n  j <- i + 1\n  \n  # Calculate the Pearson correlation coefficient and test the statistical significance\n  correlation_test <- cor.test(expertise3_new[, variables[i]], expertise3_new[, variables[j]], method = \"pearson\")\n  \n  # Add the correlation coefficient, p-value, and confidence interval to the data frame\n  correlations <- rbind(correlations, data.frame(variable1 = variables[i], variable2 = variables[j], correlation = correlation_test$estimate, p.value = correlation_test$p.value, conf.int = paste(correlation_test$conf.int[1], correlation_test$conf.int[2], sep = \" - \")))\n}\n\n# View the correlation coefficients and statistical measures\ncorrelations\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          variable1      variable2 correlation      p.value\ncor       stih_lang      r_ih_lang  -0.6068527 6.542313e-22\ncor1    stih_school    r_ih_school  -0.5223201 1.125194e-15\ncor2     stih_cards     r_ih_cards  -0.5433698 4.572854e-17\ncor3 stih_breakfast r_ih_breakfast  -0.3592235 1.317523e-07\ncor4  stih_weddings  r_ih_weddings  -0.5284362 4.539085e-16\ncor5     stih_teeth     r_ih_teeth  -0.4440395 3.226080e-11\ncor6   stih_traffic   r_ih_traffic  -0.4869944 1.514193e-13\ncor7        stih_tv        r_ih_tv  -0.3471023 3.653644e-07\n                                    conf.int\ncor  -0.686956408754957 - -0.512178848971074\ncor1  -0.61552619526787 - -0.414704679541722\ncor2 -0.633457573965096 - -0.438748248188404\ncor3 -0.473241603677093 - -0.233368287331007\ncor4 -0.620746208212983 - -0.421675200702786\ncor5 -0.548226097515688 - -0.326281347959786\ncor6  -0.58521523210812 - -0.374689532050954\ncor7 -0.462424268063511 - -0.220232239305416\n```\n:::\n:::\n\n\nIt seems that each pairs have negative significant correlation, so we can take the average scores to calculate inherence scores\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexpertise3_new <- expertise3_new %>%\n  mutate(ih_lang = (stih_lang + (10 - r_ih_lang)) / 2,\n    ih_school = (stih_school + (10 - r_ih_school)) / 2,\n    ih_cards = (stih_cards + (10 - r_ih_cards)) / 2,\n    ih_breakfast = (stih_breakfast + (10 - r_ih_breakfast)) / 2,\n    ih_weddings = (stih_weddings + (10 - r_ih_weddings)) / 2,\n    ih_teeth = (stih_teeth + (10 - r_ih_teeth)) / 2,\n    ih_traffic = (stih_traffic + (10-r_ih_traffic)) / 2,\n    ih_tv = (stih_tv + (10-r_ih_tv)) / 2 )\n```\n:::\n\n\nCalculate \"Need for cognition\" scale scores\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# change the data type of the variables to numeric\nexpertise3_new <- expertise3_new %>%\n  mutate_at(vars(needforcog1,needforcog2,needforcog3,needforcog4,needforcog5,needforcog6\n                 ,needforcog7,needforcog8,needforcog9,needforcog10,needforcog11,needforcog12,\n                   needforcog13,needforcog14,needforcog15,needforcog16,needforcog17,needforcog18), as.numeric)\n\n#add a new variable called needforcog, which is the sum of all the need for cognition items, the items are summed or extracted according to the normal or reverse items\nexpertise3_new <- expertise3_new %>%\n  mutate(needforcog=(needforcog1+needforcog2-needforcog3-needforcog4-needforcog5+needforcog6\n         -needforcog7-needforcog8-needforcog9+needforcog10+needforcog11-needforcog12+\n         needforcog13+needforcog14+needforcog15+needforcog16-needforcog17+needforcog18))\n```\n:::\n\n\nPrepare the expertise scores and other scores ready for analyses\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Replace all NA values in the expertise columns with 0\nexpertise3_new <- apply(expertise3_new, 2, function(x) ifelse(is.na(x), 0, x))\n# Turn the output to a dataframe\nexpertise3_new <- data.frame(expertise3_new)\n\n# Create new data frame as analyzable \nexpertise3_new<-expertise3_new%>%\n  select(-stih_lang,-r_ih_lang,-stih_school,-r_ih_school,\n         -stih_cards,-r_ih_cards,-stih_breakfast,-r_ih_breakfast,-stih_weddings,-r_ih_weddings,\n         -stih_teeth,-r_ih_teeth,-stih_traffic,-r_ih_traffic,-stih_tv,-r_ih_tv,\n         -course_lang,\n         -book_lang,-article_lang,\n         -course_school,\n         -book_school,-article_school,\n         -course_cards,\n         -book_cards,-article_cards,\n         -course_breakfast,\n         -book_breakfast,-article_breakfast,\n         -course_weddings,\n         -book_weddings,-article_weddings,\n         -course_teeth,\n         -book_teeth,-article_teeth,\n         -course_traffic,\n         -book_traffic,-article_traffic,\n         -course_tv,\n         -book_tv,-article_tv,\n         -needforcog1,-needforcog2,-needforcog3,-needforcog4,-needforcog5,-needforcog6,\n         -needforcog7,-needforcog8,-needforcog9,-needforcog10,-needforcog11,-needforcog12,\n         -needforcog13,-needforcog14,-needforcog15,-needforcog16,-needforcog17,-needforcog18)\n\n\n\n# change the data type of the variables to numeric\nexpertise3_new <- expertise3_new %>%\n  mutate_at(vars('know_lang','know_school','know_cards','know_breakfast',\n                 'know_weddings','know_teeth','know_traffic','know_tv',\n                 'high_lang', 'colle_lang', 'grad_lang',\n                 'numbook_lang','numarticle_lang',\n                 'high_school', 'colle_school', 'grad_school',\n                 'numbook_school','numarticle_school',\n                 'high_cards', 'colle_cards', 'grad_cards',\n                 'numbook_cards','numarticle_cards',\n                 'high_breakfast', 'colle_breakfast', 'grad_breakfast',\n                 'numbook_breakfast','numarticle_breakfast',\n                 'high_weddings', 'colle_weddings', 'grad_weddings',\n                 'numbook_weddings','numarticle_weddings',\n                 'high_teeth', 'colle_teeth', 'grad_teeth',\n                 'numbook_teeth','numarticle_teeth',\n                 'high_traffic', 'colle_traffic', 'grad_traffic',\n                 'numbook_traffic','numarticle_traffic',\n                 'high_tv', 'colle_tv', 'grad_tv',\n                 'numbook_tv','numarticle_tv'), as.numeric)\n\n# Select the variables to standardize\nvars_to_standardize <- c('know_lang','know_school','know_cards','know_breakfast',\n                         'know_weddings','know_teeth','know_traffic','know_tv',\n                         'high_lang', 'colle_lang', 'grad_lang',\n                         'numbook_lang','numarticle_lang',\n                         'high_school', 'colle_school', 'grad_school',\n                         'numbook_school','numarticle_school',\n                         'high_cards', 'colle_cards', 'grad_cards',\n                         'numbook_cards','numarticle_cards',\n                         'high_breakfast', 'colle_breakfast', 'grad_breakfast',\n                         'numbook_breakfast','numarticle_breakfast',\n                         'high_weddings', 'colle_weddings', 'grad_weddings',\n                         'numbook_weddings','numarticle_weddings',\n                         'high_teeth', 'colle_teeth', 'grad_teeth',\n                         'numbook_teeth','numarticle_teeth',\n                         'high_traffic', 'colle_traffic', 'grad_traffic',\n                         'numbook_traffic','numarticle_traffic',\n                         'high_tv', 'colle_tv', 'grad_tv',\n                         'numbook_tv','numarticle_tv')\n\n# Standardize the selected variables\nexpertise3_new[, vars_to_standardize] <- scale(expertise3_new[, vars_to_standardize])\n\n# Now we are ready for analysis\n\nhead(expertise3_new)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   ID duration   know_lang know_school know_cards know_breakfast know_weddings\n1   1      403 -0.92933457 -0.09219094 -1.1258315      0.7943452    -0.1799059\n2   2      473  1.39400185 -0.09219094 -0.7512429     -0.6065023    -0.2230326\n3   3      442 -1.60521425 -2.05124849 -1.8750087     -2.2332930    -1.4305799\n4   4      337  0.04224248 -0.13572556 -0.1893600     -0.6065023     0.2513609\n5   5      376  0.38018232  0.82203591  0.1852286      0.7039680    -0.9130596\n6   6      618  0.04224248 -0.13572556  1.1685236      1.0202884     1.8039217\n  know_teeth know_traffic    know_tv  high_lang colle_lang  grad_lang\n1 -1.5832390   0.40296860  0.1980935 -0.6364644 -0.5394097 -0.1521967\n2 -0.2551931  -0.01538048  1.3258444 -0.6364644 -0.5394097 -0.1521967\n3 -2.2029937  -1.06125318 -0.8168823 -0.6364644 -0.5394097 -0.1521967\n4  0.6744390   0.36113369 -0.1778235 -0.6364644 -0.5394097 -0.1521967\n5  1.1613891  -0.01538048  0.8371523 -0.1776703 -0.5394097 -0.1521967\n6 -0.7864115  -0.72657392  1.9273115 -0.6364644 -0.5394097 -0.1521967\n  numbook_lang numarticle_lang high_school colle_school grad_school\n1  -0.49509859      -0.2633927  -0.2455616   -0.3847762  -0.1141508\n2  -0.49509859      -0.2633927  -0.2455616   -0.3847762  -0.1141508\n3  -0.49509859      -0.2633927  -0.2455616   -0.3847762  -0.1141508\n4  -0.49509859      -0.2633927  -0.2455616   -0.3847762  -0.1141508\n5  -0.07774275      -0.2633927  -0.2455616   -0.3847762  -0.1141508\n6  -0.49509859      -0.2633927  -0.2455616   -0.3847762  -0.1141508\n  numbook_school numarticle_school high_cards colle_cards grad_cards\n1     -0.3351031        -0.2475552 -0.2950077   -0.175091 -0.1028024\n2     -0.3351031        -0.2475552 -0.2950077   -0.175091 -0.1028024\n3     -0.3351031        -0.2475552 -0.2950077   -0.175091 -0.1028024\n4     -0.3351031        -0.2475552 -0.2950077   -0.175091 -0.1028024\n5     -0.3351031        -0.2475552 -0.2950077   -0.175091 -0.1028024\n6     -0.3351031         9.3827397 -0.2950077   -0.175091 -0.1028024\n  numbook_cards numarticle_cards high_breakfast colle_breakfast grad_breakfast\n1    -0.3549311       -0.3570679      0.4510718      -0.2243077     -0.1218696\n2    -0.3549311       -0.3570679     -0.2910141      -0.2243077     -0.1218696\n3    -0.3549311       -0.3570679     -0.2910141      -0.2243077     -0.1218696\n4     0.6479213       -0.1076095     -0.2910141      -0.2243077     -0.1218696\n5    -0.3549311       -0.3570679     -0.2910141      -0.2243077     -0.1218696\n6    -0.3549311       -0.3570679     -0.2910141      -0.2243077     -0.1218696\n  numbook_breakfast numarticle_breakfast high_weddings colle_weddings\n1         -0.235773           -0.2393171    -0.1795269     -0.2260702\n2         -0.235773           -0.2393171    -0.1795269     -0.2260702\n3         -0.235773           -0.2393171    -0.1795269     -0.2260702\n4         -0.235773           -0.2393171    -0.1795269     -0.2260702\n5         -0.235773           -0.2393171    -0.1795269     -0.2260702\n6         -0.235773           -0.2393171    -0.1795269     -0.2260702\n  grad_weddings numbook_weddings numarticle_weddings high_teeth colle_teeth\n1   -0.09411928       -0.3825866          -0.4102006 -0.3312423  -0.2490223\n2   -0.09411928       -0.3825866          -0.4102006 -0.3312423  -0.2490223\n3   -0.09411928       -0.3825866          -0.4102006 -0.3312423  -0.2490223\n4   -0.09411928       -0.3825866          -0.4102006 -0.3312423  -0.2490223\n5   -0.09411928       -0.3825866          -0.4102006 -0.3312423  -0.2490223\n6   -0.09411928       -0.3825866          -0.4102006 -0.3312423  -0.2490223\n   grad_teeth numbook_teeth numarticle_teeth high_traffic colle_traffic\n1 -0.09925954    -0.2779491      -0.16517115   -0.2045128    -0.1362734\n2 -0.09925954    -0.2779491      -0.16517115   -0.2045128    -0.1362734\n3 -0.09925954    -0.2779491      -0.16517115   -0.2045128    -0.1362734\n4 -0.09925954     1.3192794      -0.05229037   -0.2045128    -0.1362734\n5 -0.09925954    -0.2779491      -0.16517115   -0.2045128    -0.1362734\n6 -0.09925954    -0.2779491      -0.16517115   -0.2045128    -0.1362734\n  grad_traffic numbook_traffic numarticle_traffic    high_tv   colle_tv grad_tv\n1    -0.070014       -0.254263         -0.1112561 -0.1611468 -0.1414978     NaN\n2    -0.070014       -0.254263         -0.1112561 -0.1611468 -0.1414978     NaN\n3    -0.070014       -0.254263         -0.1112561 -0.1611468 -0.1414978     NaN\n4    -0.070014       -0.254263         -0.1112561 -0.1611468 -0.1414978     NaN\n5    -0.070014       -0.254263         -0.1112561 -0.1611468 -0.1414978     NaN\n6    -0.070014       -0.254263         -0.1112561 -0.1611468 -0.1414978     NaN\n  numbook_tv numarticle_tv sex education income religion identity age\n1 -0.2473571    -0.1178382   1         3 105000     none    white  26\n2 -0.2473571    -0.1178382   1         5  53000  atheist    white  38\n3 -0.2473571    -0.1178382   2         2 15.000     none    white  22\n4 -0.2473571    -0.1178382   2         4 150000   jewish    white  55\n5 -0.2473571    -0.1178382   1         3  45000     None    White  29\n6 -0.2473571    -0.1178382   1         3  60000 catholic    white  58\n  political_atti english_level ih_lang ih_school ih_cards ih_breakfast\n1              4             4     4.0       4.0      2.5          4.5\n2              2             4     5.0       5.0      5.0          5.0\n3              5             4     6.0       4.5      4.0          5.5\n4              5             4     5.0       2.5      2.0          3.0\n5              2             4     4.5       4.0      3.0          6.0\n6              4             4     5.5       3.5      5.0          3.5\n  ih_weddings ih_teeth ih_traffic ih_tv needforcog\n1         3.5      8.0        8.5   9.0         43\n2         5.0      5.0        5.0   5.0         11\n3         5.5      6.5        6.0   4.5          7\n4         1.0      1.0        5.0   1.0         17\n5         3.5      5.5        5.0   4.0         47\n6         3.5      5.5        4.5   4.0         10\n```\n:::\n:::\n\n\nTransform the data to the long format for analyses:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary('reshape2')\n\nexpertise3_long <- melt(expertise3_new, id.vars = c(\"ID\",'duration',\"sex\" ,\"education\" ,\"income\",\"religion\",'identity','age','political_atti','english_level','needforcog'),\n          measure.vars = c(\"ih_lang\", \"ih_school\",\"ih_cards\", \"ih_breakfast\", \"ih_weddings\", \n\"ih_teeth\", \"ih_traffic\", \"ih_tv\",'know_lang','know_school','know_cards','know_breakfast',\n'know_weddings','know_teeth','know_traffic','know_tv','high_lang', 'colle_lang', 'grad_lang',\n'numbook_lang','numarticle_lang','high_school', 'colle_school', 'grad_school',\n'numbook_school','numarticle_school','high_cards', 'colle_cards', 'grad_cards',\n'numbook_cards','numarticle_cards', 'high_breakfast', 'colle_breakfast', 'grad_breakfast',\n'numbook_breakfast','numarticle_breakfast','high_weddings', 'colle_weddings', 'grad_weddings','numbook_weddings','numarticle_weddings','high_teeth', 'colle_teeth', 'grad_teeth',\n'numbook_teeth','numarticle_teeth','high_traffic', 'colle_traffic', 'grad_traffic',\n'numbook_traffic','numarticle_traffic','high_tv', 'colle_tv', 'grad_tv','numbook_tv','numarticle_tv'),sep = \"_\", variable.name = \"Category\", value.name = \"Score\")\n\n# Split the Category column into two columns based on the underscore separator\nexpertise3_long <- expertise3_long %>% separate(Category, into = c(\"Category\", \"Score_Type\"), sep = \"_\")\n\n#spread the data from long to wide format\nexpertise3_ready <- expertise3_long %>% spread(Category, Score)\n\n#change the score type to a factor\nexpertise3_ready$Score_Type<-as.factor(expertise3_ready$Score_Type)\n\nexpertise3_ready$ih<-as.numeric(expertise3_ready$ih)\n\n\n#view the data\nhead(expertise3_ready)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   ID duration sex education income religion identity age political_atti\n1   1      403   1         3 105000     none    white  26              4\n2   2      473   1         5  53000  atheist    white  38              2\n3   3      442   2         2 15.000     none    white  22              5\n4   4      337   2         4 150000   jewish    white  55              5\n5   5      376   1         3  45000     None    White  29              2\n6   6      618   1         3  60000 catholic    white  58              4\n  english_level needforcog Score_Type              colle               grad\n1             4         43       lang -0.539409671771391 -0.152196749527831\n2             4         11       lang -0.539409671771391 -0.152196749527831\n3             4          7       lang -0.539409671771391 -0.152196749527831\n4             4         17       lang -0.539409671771391 -0.152196749527831\n5             4         47       lang -0.539409671771391 -0.152196749527831\n6             4         10       lang -0.539409671771391 -0.152196749527831\n                high  ih               know         numarticle\n1 -0.636464435362702 4.0 -0.929334565418051 -0.263392653035181\n2 -0.636464435362702 5.0   1.39400184812708 -0.263392653035181\n3 -0.636464435362702 6.0  -1.60521424935845 -0.263392653035181\n4 -0.636464435362702 5.0 0.0422424802462751 -0.263392653035181\n5 -0.177670284076514 4.5  0.380182322216475 -0.263392653035181\n6 -0.636464435362702 5.5 0.0422424802462751 -0.263392653035181\n              numbook\n1  -0.495098592648002\n2  -0.495098592648002\n3  -0.495098592648002\n4  -0.495098592648002\n5 -0.0777427542174549\n6  -0.495098592648002\n```\n:::\n:::\n\n\nWe can start to analyze our models with using hlm:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Start to analyze\n\nlibrary(lme4) \nModel.Null<-lmer(ih ~1+(1|Score_Type)+(1|ID),  \n                 data=expertise3_ready)\nsummary(Model.Null)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML ['lmerMod']\nFormula: ih ~ 1 + (1 | Score_Type) + (1 | ID)\n   Data: expertise3_ready\n\nREML criterion at convergence: 6485.1\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.99061 -0.64339 -0.00129  0.60758  2.86910 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.5764   0.7592  \n Score_Type (Intercept) 0.3589   0.5991  \n Residual               2.7064   1.6451  \nNumber of obs: 1632, groups:  ID, 204; Score_Type, 8\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)   4.8768     0.2221   21.95\n```\n:::\n\n```{.r .cell-code}\n# get pseudo R^2\nperformance::icc(Model.Null)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Intraclass Correlation Coefficient\n\n    Adjusted ICC: 0.257\n  Unadjusted ICC: 0.257\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Now add infoavoid as a fixed effect\n# same slope for all sites\nModel.1<-lmer(ih ~needforcog+(1|Score_Type)+(1|ID),   \n              data=expertise3_ready)\nsummary(Model.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML ['lmerMod']\nFormula: ih ~ needforcog + (1 | Score_Type) + (1 | ID)\n   Data: expertise3_ready\n\nREML criterion at convergence: 6313.2\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.4258 -0.6418 -0.0155  0.6080  2.8740 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.4809   0.6935  \n Score_Type (Intercept) 0.3589   0.5991  \n Residual               2.7064   1.6451  \nNumber of obs: 1632, groups:  ID, 204; Score_Type, 8\n\nFixed effects:\n              Estimate Std. Error t value\n(Intercept)     6.0313     0.6741   8.947\nneedforcog  1  -0.9063     1.1085  -0.818\nneedforcog  2  -1.0313     1.1085  -0.930\nneedforcog  3  -2.1250     0.9051  -2.348\nneedforcog  4  -1.0729     0.8262  -1.299\nneedforcog  5  -1.0313     1.1085  -0.930\nneedforcog  6  -0.5938     1.1085  -0.536\nneedforcog  7  -2.0313     0.9051  -2.244\nneedforcog  8  -0.7813     0.7838  -0.997\nneedforcog  9  -2.3438     1.1085  -2.114\nneedforcog -1  -0.6563     1.1085  -0.592\nneedforcog -2  -0.6563     1.1085  -0.592\nneedforcog -3  -0.4688     0.9051  -0.518\nneedforcog -4  -0.4063     1.1085  -0.366\nneedforcog -5  -0.9063     1.1085  -0.818\nneedforcog -6  -0.4688     0.9051  -0.518\nneedforcog -7  -1.9063     1.1085  -1.720\nneedforcog -9  -0.2813     1.1085  -0.254\nneedforcog 10  -0.9375     0.7155  -1.310\nneedforcog 11  -0.3259     0.7257  -0.449\nneedforcog 12  -0.9509     0.7257  -1.310\nneedforcog 13  -1.3813     0.7573  -1.824\nneedforcog 14  -1.1146     0.8262  -1.349\nneedforcog 15  -0.6875     0.9051  -0.760\nneedforcog 16  -1.3646     0.8262  -1.652\nneedforcog 17  -1.2634     0.7257  -1.741\nneedforcog 18  -1.1979     0.8262  -1.450\nneedforcog 19  -0.5000     0.7838  -0.638\nneedforcog 20  -1.5313     0.8262  -1.853\nneedforcog 21  -1.2188     0.7573  -1.609\nneedforcog 22  -1.4375     0.7838  -1.834\nneedforcog 23  -1.8438     0.9051  -2.037\nneedforcog 24  -0.9688     0.9051  -1.070\nneedforcog 25  -1.5938     1.1085  -1.438\nneedforcog 26  -1.0313     0.8262  -1.248\nneedforcog 27  -1.4688     1.1085  -1.325\nneedforcog 28  -0.7344     0.7838  -0.937\nneedforcog 29  -0.7604     0.8262  -0.920\nneedforcog 30  -1.5156     0.7838  -1.934\nneedforcog 31  -0.3125     0.9051  -0.345\nneedforcog 32  -0.2396     0.8262  -0.290\nneedforcog 33  -0.9479     0.8262  -1.147\nneedforcog 34  -1.3229     0.7390  -1.790\nneedforcog 35  -0.8938     0.7573  -1.180\nneedforcog 36  -1.1354     0.8262  -1.374\nneedforcog 37  -0.9219     0.7838  -1.176\nneedforcog 38  -2.1563     1.1085  -1.945\nneedforcog 39  -0.9271     0.8262  -1.122\nneedforcog 40  -1.5000     0.9051  -1.657\nneedforcog 41  -0.4063     0.9051  -0.449\nneedforcog 42  -1.3438     1.1085  -1.212\nneedforcog 43  -0.8229     0.8262  -0.996\nneedforcog 44   1.0937     1.1085   0.987\nneedforcog 45  -0.5063     0.7573  -0.669\nneedforcog 46  -2.2031     0.7838  -2.811\nneedforcog 47  -1.2656     0.7838  -1.615\nneedforcog 48  -1.3646     0.8262  -1.652\nneedforcog 50  -1.4688     0.9051  -1.623\nneedforcog 51  -1.6979     0.8262  -2.055\nneedforcog 52  -2.0000     0.7390  -2.706\nneedforcog 53  -1.8125     0.7838  -2.312\nneedforcog 54  -1.6563     1.1085  -1.494\nneedforcog 55  -3.9063     1.1085  -3.524\nneedforcog 56  -2.3438     0.7838  -2.990\nneedforcog 57  -0.7813     0.9051  -0.863\nneedforcog 58  -1.7813     0.9051  -1.968\nneedforcog 59  -0.9063     1.1085  -0.818\nneedforcog 60  -1.3750     0.9051  -1.519\nneedforcog 66  -1.0000     0.9051  -1.105\nneedforcog 68  -0.6563     1.1085  -0.592\nneedforcog 69  -2.4688     1.1085  -2.227\nneedforcog 70  -3.0938     1.1085  -2.791\nneedforcog 74  -3.0313     1.1085  -2.734\nneedforcog 78  -1.9063     1.1085  -1.720\nneedforcog-21   0.4687     1.1085   0.423\nneedforcog-30   0.7812     1.1085   0.705\nneedforcog-34  -1.4063     0.9051  -1.554\nneedforcog-36  -0.7813     1.1085  -0.705\n```\n:::\n\n```{.r .cell-code}\n# get pseudo R^2\nperformance::icc(Model.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Intraclass Correlation Coefficient\n\n    Adjusted ICC: 0.237\n  Unadjusted ICC: 0.213\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# compare the two models\n\nanova(Model.Null,Model.1)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nrefitting model(s) with ML (instead of REML)\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nData: expertise3_ready\nModels:\nModel.Null: ih ~ 1 + (1 | Score_Type) + (1 | ID)\nModel.1: ih ~ needforcog + (1 | Score_Type) + (1 | ID)\n           npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)   \nModel.Null    4 6491.9 6513.5 -3241.9   6483.9                        \nModel.1      81 6526.6 6963.8 -3182.3   6364.6 119.25 77   0.001448 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nOther hierarchical stages will be run, based on this comparison, we can say that need for cognition scores are related to inherence significantly.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}