{
  "hash": "f9fe5b6aaefd566f5b8d851014f4df44",
  "result": {
    "markdown": "---\ntitle: \"Expertise6 Data Analysis from the very beginning\"\nauthor: \"MK\"\ndate: \"2023-01-16\"\ncategories: [example data, code, analysis]\nimage: \"newdata.png\"\n---\n\n\nThis post includes the trial analyses of an example data related to expertise.\n\nNotes: Need for closure scores haven't been calculated, the reverse items will be checked. Notes: For the long format, internet article, documentaries and podcast series should be added.\n\n### Import necessary packages and expertise data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Import library ----\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(purrr)\nlibrary(jmv)\nlibrary(psych)\nlibrary(DescTools)\nlibrary(stats)\nlibrary(factoextra)\nlibrary('reshape2')\nlibrary(lme4) \nlibrary(lmerTest)\n\n\n#Read the csv file ----\nExpertise6 <- read_csv(\"~/Desktop/Expertise6_5.8.14_December 23, 2022_10.51 2.csv\")\n```\n:::\n\n\nCreates a new dataframe called expertise6_clean, which is a copy of the original dataframe called expertise6 and removes the second row of the dataframe and create a variable called column_names and assign it the names of the columns in the dataframe and change the column names\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#removes the second row of the dataframe\nexpertise6_clean<-Expertise6%>% \n  slice(-2) \n\n\n#selects all columns except the ones listed \nexpertise6_clean <- expertise6_clean %>%\n  select(-StartDate, -EndDate, -Status, -Progress,-ResponseId,-RecordedDate,-RecipientLastName, -RecipientFirstName,-RecipientEmail, -ExternalReference, -LocationLatitude,-LocationLongitude, -DistributionChannel, -UserLanguage)\n\n#create a variable called column_names and assign it the names of the columns in the dataframe\ncolumn_names <- names(expertise6_clean)\n\ncolumn_names\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  [1] \"IPAddress\"             \"Duration (in seconds)\" \"Finished\"             \n  [4] \"Q39\"                   \"Q40\"                   \"Q41\"                  \n  [7] \"Q42\"                   \"Q43\"                   \"Q44\"                  \n [10] \"Q45\"                   \"Q46\"                   \"Q47\"                  \n [13] \"Q48\"                   \"Q49\"                   \"Q50\"                  \n [16] \"Q51\"                   \"Q52\"                   \"Q53\"                  \n [19] \"Q54\"                   \"Q147_1\"                \"Q149_1\"               \n [22] \"Q151_1\"                \"Q153_1\"                \"Q155_1\"               \n [25] \"Q157_1\"                \"Q159_1\"                \"Q161_1\"               \n [28] \"Q69\"                   \"Q3_1\"                  \"Q3_2\"                 \n [31] \"Q3_3\"                  \"Q70\"                   \"Q4\"                   \n [34] \"Q71\"                   \"Q5\"                    \"Q204\"                 \n [37] \"Q209...51\"             \"Q206\"                  \"Q210\"                 \n [40] \"Q208\"                  \"Q211...55\"             \"Q73\"                  \n [43] \"Q74_1\"                 \"Q74_2\"                 \"Q74_3\"                \n [46] \"Q75\"                   \"Q76...61\"              \"Q77\"                  \n [49] \"Q78...63\"              \"Q214...64\"             \"Q219...65\"            \n [52] \"Q216\"                  \"Q214...67\"             \"Q218\"                 \n [55] \"Q215...69\"             \"Q80...70\"              \"Q81_1\"                \n [58] \"Q81_2\"                 \"Q81_3\"                 \"Q82\"                  \n [61] \"Q83\"                   \"Q84\"                   \"Q85\"                  \n [64] \"Q217...78\"             \"Q222\"                  \"Q219...80\"            \n [67] \"Q223...81\"             \"Q221...82\"             \"Q224\"                 \n [70] \"Q87\"                   \"Q88_1\"                 \"Q88_2\"                \n [73] \"Q88_3\"                 \"Q89\"                   \"Q90\"                  \n [76] \"Q91\"                   \"Q92\"                   \"Q226\"                 \n [79] \"Q231...93\"             \"Q228\"                  \"Q232\"                 \n [82] \"Q230\"                  \"Q233...97\"             \"Q94\"                  \n [85] \"Q95_1\"                 \"Q95_2\"                 \"Q95_3\"                \n [88] \"Q96\"                   \"Q97\"                   \"Q98\"                  \n [91] \"Q99\"                   \"Q245...106\"            \"Q246...107\"           \n [94] \"Q247...108\"            \"Q248...109\"            \"Q249...110\"           \n [97] \"Q250...111\"            \"Q101\"                  \"Q102_1\"               \n[100] \"Q102_2\"                \"Q102_3\"                \"Q103\"                 \n[103] \"Q104\"                  \"Q105\"                  \"Q106\"                 \n[106] \"Q235...120\"            \"Q240\"                  \"Q237...122\"           \n[109] \"Q241...123\"            \"Q239...124\"            \"Q242\"                 \n[112] \"Q108\"                  \"Q109_1\"                \"Q109_2\"               \n[115] \"Q109_3\"                \"Q110\"                  \"Q111\"                 \n[118] \"Q112\"                  \"Q113\"                  \"Q244\"                 \n[121] \"Q249...135\"            \"Q246...136\"            \"Q250...137\"           \n[124] \"Q248...138\"            \"Q251...139\"            \"Q115\"                 \n[127] \"Q116_1\"                \"Q116_2\"                \"Q116_3\"               \n[130] \"Q117\"                  \"Q118\"                  \"Q119\"                 \n[133] \"Q120\"                  \"Q253\"                  \"Q258\"                 \n[136] \"Q255\"                  \"Q259\"                  \"Q257\"                 \n[139] \"Q251...153\"            \"Q167\"                  \"Q169\"                 \n[142] \"Q171\"                  \"Q173\"                  \"Q175\"                 \n[145] \"Q177\"                  \"Q179\"                  \"Q181\"                 \n[148] \"Q183\"                  \"Q185\"                  \"Q187\"                 \n[151] \"Q189\"                  \"Q191\"                  \"Q193\"                 \n[154] \"Q195\"                  \"Q197\"                  \"Q199\"                 \n[157] \"Q201\"                  \"Q203\"                  \"Q205\"                 \n[160] \"Q207\"                  \"Q209...175\"            \"Q211...176\"           \n[163] \"Q213\"                  \"Q215...178\"            \"Q217...179\"           \n[166] \"Q219...180\"            \"Q221...181\"            \"Q223...182\"           \n[169] \"Q225\"                  \"Q227\"                  \"Q229\"                 \n[172] \"Q231...186\"            \"Q233...187\"            \"Q235...188\"           \n[175] \"Q237...189\"            \"Q239...190\"            \"Q241...191\"           \n[178] \"Q243\"                  \"Q245...193\"            \"Q247...194\"           \n[181] \"Q249...195\"            \"Q58\"                   \"Q60_1\"                \n[184] \"Q62\"                   \"Q64_1\"                 \"Q64_2\"                \n[187] \"Q64_3\"                 \"Q64_4\"                 \"Q66\"                  \n[190] \"Q68\"                   \"Q72\"                   \"Q74\"                  \n[193] \"Q76...207\"             \"Q78...208\"             \"Q80...209\"            \n[196] \"SurveyOrder\"          \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n##Change the column names ----\ncolnames(expertise6_clean) <- c('ip','duration', 'finished', 'stih_food','r_ih_food',\n                        'stih_sports','r_ih_sports','stih_school','r_ih_school',\n                        'stih_architect','r_ih_architect','stih_product','r_ih_product',\n                        'stih_langu','r_ih_langu','stih_network','r_ih_network',\n                        'stih_anthro','r_ih_anthro',\n                        'know_food','know_sports','know_school','know_architect',\n                        'know_product','know_langu','know_network','know_anthro',\n                        'course_food','high_food', 'colle_food', 'grad_food',\n                        'book_food','numbook_food','article_food','numarticle_food',\n                        'inarti_food','numinarti_food', 'docu_food', 'numdocu_food',\n                        'radio_food','numradio_food',\n                        'course_sports','high_sports', 'colle_sports', 'grad_sports',\n                        'book_sports','numbook_sports','article_sports','numarticle_sports',\n                        'inarti_sports','numinarti_sports', 'docu_sports', 'numdocu_sports',\n                        'radio_sports','numradio_sports',\n                        'course_school','high_school', 'colle_school', 'grad_school',\n                        'book_school','numbook_school','article_school','numarticle_school',\n                        'inarti_school','numinarti_school', 'docu_school', 'numdocu_school',\n                        'radio_school','numradio_school',\n                        'course_architect','high_architect', 'colle_architect','grad_architect',\n                        'book_architect','numbook_architect','article_architect',\n                        'numarticle_architect',\n                        'inarti_architect','numinarti_architect', \n                        'docu_architect', 'numdocu_architect',\n                        'radio_architect','numradio_architect',\n                        'course_product','high_product', 'colle_product', 'grad_product',\n                        'book_product','numbook_product','article_product',\n                        'numarticle_product',\n                        'inarti_product','numinarti_product', 'docu_product', 'numdocu_product',\n                        'radio_product','numradio_product',\n                        'course_langu','high_langu', 'colle_langu', 'grad_langu',\n                        'book_langu','numbook_langu','article_langu','numarticle_langu',\n                        'inarti_langu','numinarti_langu', 'docu_langu', 'numdocu_langu',\n                        'radio_langu','numradio_langu',\n                        'course_network','high_network', 'colle_network', 'grad_network',\n                        'book_network','numbook_network','article_network','numarticle_network',\n                        'inarti_network','numinarti_network', 'docu_network', 'numdocu_network',\n                        'radio_network','numradio_network',\n                        'course_anthro','high_anthro', 'colle_anthro', 'grad_anthro',\n                        'book_anthro','numbook_anthro','article_anthro','numarticle_anthro',\n                        'inarti_anthro','numinarti_anthro', 'docu_anthro', 'numdocu_anthro',\n                        'radio_anthro','numradio_anthro',\n                        'needforclo1','needforclo2','needforclo3','needforclo4','needforclo5',\n                        'needforclo6','needforclo7','needforclo8','needforclo9','needforclo10',\n                        'needforclo11','needforclo12','needforclo13','needforclo14',\n                        'needforclo15','needforclo16','needforclo17','needforclo18',\n                        'needforclo19','needforclo20','needforclo21','needforclo22',\n                        'needforclo23','needforclo24','needforclo25','needforclo26',\n                        'needforclo27','needforclo28','needforclo29','needforclo30',\n                        'needforclo31','needforclo32','needforclo33','needforclo34',\n                        'needforclo35','needforclo36','needforclo37','needforclo38',\n                        'needforclo39','needforclo40','needforclo41','needforclo42',\n                        'sex','birthdate','education','income','religion',\n                        'identity','age','political_atti','english_level','proceure_confu',\n                        'whatwestudied','moretothisstudy','additional_thoughts','attention', \n                        'surveyorder')\n```\n:::\n\n\n### Questionnaire Items\n\nThe code below shows the survey items:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#selects all columns except the ones specified\nrow_values <- expertise6_clean %>%\n  select(-duration,-finished, -ip, -surveyorder)%>%\n  #selects only the first row\n  filter(row_number() == 1)\n\n#Items in the questionnaire ----\n\n#unlist the row_values\nrow_values <- unlist(row_values)\n\nmy_list <- map(row_values, ~paste0(.))\n\nlibrary(stringr)\nmy_list <- str_replace(my_list, \"(?<! )\\\\n(?! )\", \"\")\nmy_list <- str_replace(my_list, \"[^\\\\s]*\\\\\\\\n[^\\\\s]*\", \"\")\nlist_string <- paste0(\"* \", paste(my_list, collapse = \"\\n* \"))\n\n##Show the survey items ----\ncat(list_string)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n* There are good reasons why drive-thrus are typically associated with fast-food restaurants. There are things about drive-thrus that make them less suited for fancy or upscale restaurants.\n* Even though we don’t typically see drive-thrus at fancy restaurants, there is no real reason why fancy restaurants don’t have drive-thrus. This practice could have easily been different.\n* It seems natural that we sing the national anthem before sporting events, rather than after. The national anthem just fits better at the beginning of a sports game.\n* When you really think about, singing the national anthem before sporting events is just a convention. If history had unfolded differently, it’s possible that we would be singing it after the events, or maybe even not at all.\n* It seems right that students receive letter grades in school (A, B, C, …) to measure their performance. Another evaluative process would likely not work as well.\n* It’s likely that evaluating students’ performance with letter grades (A, B, C, …) is not the best practice. Another evaluative process may be more effective than letter grading.\n* It probably works best for homes to be painted with neutral or muted colors (e.g., white, gray). Bright or neon-colored homes (e.g., red, purple) would not be practical.\n* Although homes are usually painted with neutral or muted colors (e.g., white, gray), this could have been done differently.  It’s quite possible that homes could have been painted with brighter colors (e.g., red, purple).\n* It seems ideal that forks usually have three or four prongs, rather than more prongs (for example, six or seven). Three or four prongs on forks is probably the best design.\n* There’s no good reason why forks only have three or four prongs. Forks with more prongs (for example, six or seven) would work just as well; in principle, forks could have been designed that way too.\n* It seems natural that the letter “s” at the end of a word is used to indicate plurality (as in \"cats\" or \"trees\"). Another way of signaling plurality would not work as well.\n* Had historical events unfolded differently, it’s possible that the English language would indicate plurality in a different way than using the letter “s” (as in \"cats\" or \"trees\"). There’s nothing inherently special about the letter “s” for this purpose.\n* There are good reasons why we use digits, rather than letters, to call people on the phone.  Using digits seems like the optimal way to make phone calls.\n* The only reason why we use digits, rather than letters, to call people is historical happenstance.  Phones calls could have just as easily been made with a variety of symbols other than digits (for example, letters).\n* It seems natural that people wear black to funerals. There is something about the color black that indicates mourning.\n* When you think about it, colors other than black could have just as easily become associated with funerals (for example, white). Had history taken a different turn, another color may now signal mourning and sadness.\n* How much do you know about the food industry and restaurant business? - Please use the slider to select your answer choice.\n* How much do you know about sports and sports management? - Please use the slider to select your answer choice.\n* How much do you know about school and education systems? - Please use the slider to select your answer choice.\n* How much do you know about architecture and home design? - Please use the slider to select your answer choice.\n* How much do you know about product design and usability? - Please use the slider to select your answer choice.\n* How much do you know about language and linguistics? - Please use the slider to select your answer choice.\n* How much do you know about telecommunication and network systems? - Please use the slider to select your answer choice.\n* How much do you know about anthropology and funeral rites? - Please use the slider to select your answer choice.\n* Have you ever taken a class that discussed the food industry and restaurant business?\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - High School\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - College\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - Graduate School\n* Have you ever read any books on the food industry and restaurant business?\n* If you answered \"yes\" above, please approximate the number of books you have read on this topic:\n* Have you ever read any magazine, newspaper, or academic articles on the food industry and restaurant business?\n* If you answered \"yes\" above, please approximate the number of articles you have read on this topic:\n* Have you ever read any articles while browsing the internet  (e.g., on Reddit, Pinterest, Buzzfeed) on the food industry and restaurant business?\n* If you answered \"yes\" above, please approximate the number of articles on the internet you have read on this topic:\n* Have you ever seen any documentaries (e,g., on the Discovery channel) on the food industry and restaurant business?\n* If you answered \"yes\" above, please approximate the number of documentaries you have seen on this topic:\n* Have you ever heard any radio shows or podcasts (e.g., NPR) on the food industry and restaurant business?\n* If you answered \"yes\" above, please approximate the number of radio shows or podcasts you have heard on this topic:\n* Have you ever taken a class that discussed sports and sports management?\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - High School\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - College\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - Graduate School\n* Have you ever read any books on sports and sports management?\n* If you answered \"yes\" above, please approximate the number of books you have read on this topic:\n* Have you ever read any magazine, newspaper, or academic articles on sports and sports management?\n* If you answered \"yes\" above, please approximate the number of articles you have read on this topic:\n* Have you ever read any articles while browsing the internet  (e.g., on Reddit, Pinterest, Buzzfeed) on sports and sports management?\n* If you answered \"yes\" above, please approximate the number of articles on the internet you have read on this topic:\n* Have you ever watched any documentaries (e,g., on the Discovery channel) on sports and sports management?\n* If you answered \"yes\" above, please approximate the number of documentaries you have seen on this topic:\n* Have you ever heard any radio shows or podcasts (e.g., NPR) on sports and sports management?\n* If you answered \"yes\" above, please approximate the number of radio shows or podcasts you have heard on this topic:\n* Have you ever taken a class that discussed school and education systems?\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - High School\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - College\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - Graduate School\n* Have you ever read any books on school and education systems?\n* If you answered \"yes\" above, please approximate the number of books you have read on this topic:\n* Have you ever read any magazine, newspaper, or academic articles on school and education systems?\n* If you answered \"yes\" above, please approximate the number of articles you have read on this topic:\n* Have you ever read any articles while browsing the internet  (e.g., on Reddit, Pinterest, Buzzfeed) on school and education systems?\n* If you answered \"yes\" above, please approximate the number of articles on the internet you have read on this topic:\n* Have you ever watched any documentaries (e,g., on the Discovery channel) on school and education systems?\n* If you answered \"yes\" above, please approximate the number of documentaries you have seen on this topic:\n* Have you ever heard any radio shows or podcasts (e.g., NPR) have you heard on school and education systems?\n* If you answered \"yes\" above, please approximate the number of radio shows or podcasts you have seen on this topic:\n* Have you ever taken a class that discussed architecture and home design?\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - High School\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - College\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - Graduate School\n* Have you ever read any books on architecture and home design?\n* If you answered \"yes\" above, please approximate the number of books you have read on this topic:\n* Have you ever read any magazine, newspaper, or academic articles on architecture and home design?\n* If you answered \"yes\" above, please approximate the number of articles you have read on this topic:\n* Have you ever read any articles while browsing the internet  (e.g., on Reddit, Pinterest, Buzzfeed) on architecture and home design?\n* If you answered \"yes\" above, please approximate the number of articles on the internet you have read on this topic:\n* Have you ever seen any documentaries (e,g., on the Discovery channel) on architecture and home design?\n* If you answered \"yes\" above, please approximate the number of documentaries you have seen on this topic:\n* Have you ever heard any radio shows or podcasts (e.g., NPR) on architecture and home design?\n* If you answered \"yes\" above, please approximate the number of radio shows or podcasts you have heard on this topic:\n* Have you ever taken a class that discussed product design and usability?\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - High School\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - College\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - Graduate School\n* Have you ever read any books on product design and usability?\n* If you answered \"yes\" above, please approximate the number of books you have read on this topic:\n* Have you ever read any magazine, newspaper, or academic articles on product design and usabilityt?\n* If you answered \"yes\" above, please approximate the number of articles you have read on this topic:\n* Have you ever read any articles while browsing the internet  (e.g., on Reddit, Pinterest, Buzzfeed) on product design and usability?\n* If you answered \"yes\" above, please approximate the number of articles on the internet you have read on this topic:\n* Have you ever seen any documentaries (e,g., on the Discovery channel) on product design and usability?\n* If you answered \"yes\" above, please approximate the number of documentaries you have seen on this topic:\n* Have you ever heard any radio shows or podcasts (e.g., on Reddit, Pinterest, Buzzfeed) on product design and usability?\n* If you answered \"yes\" above, please approximate the number of radio shows or podcasts you have heard on this topic:\n* Have you ever taken a class that discussed language and linguistics?\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - High School\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - College\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - Graduate School\n* Have you ever read any books on language and linguistics?\n* If you answered \"yes\" above, please approximate the number of books you have read on this topic:\n* Have you ever read any magazine, newspaper, or academic articles on language and linguistics?\n* If you answered \"yes\" above, please approximate the number of articles you have read on this topic:\n* Have you ever read any articles while browsing the internet  (e.g., on Reddit, Pinterest, Buzzfeed) on language and linguistics?\n* If you answered \"yes\" above, please approximate the number of articles on the internet you have read on this topic:\n* Have you ever seen any documentaries (e,g., on the Discovery channel) on language and linguistics?\n* If you answered \"yes\" above, please approximate the number of documentaries you have seen on this topic:\n* Have you ever heard any radio shows or podcasts (e.g., NPR) on language and linguistics?\n* If you answered \"yes\" above, please approximate the number of radio shows or podcasts you have heard on this topic:\n* Have you ever taken a class that discussed telecommunication and network systems?\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - High School\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - College\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - Graduate School\n* Have you ever read any books on telecommunication and network systems?\n* If you answered \"yes\" above, please approximate the number of books you have read on this topic:\n* Have you ever read any magazine, newspaper, or academic articles on telecommunication and network systems?\n* If you answered \"yes\" above, please approximate the number of articles you have read on this topic:\n* Have you ever read any articles while browsing the internet  (e.g., on Reddit, Pinterest, Buzzfeed) on telecommunication and network systems?\n* If you answered \"yes\" above, please approximate the number of articles on the internet you have read on this topic:\n* Have you ever seen any documentaries (e,g., on the Discovery channel) on telecommunication and network systems?\n* If you answered \"yes\" above, please approximate the number of documentaries you have seen on this topic:\n* Have you ever heard any radio shows or podcasts (e.g., NPR) on telecommunication and network systems?\n* If you answered \"yes\" above, please approximate the number of radio shows or podcasts you have heard on this topic:\n* Have you ever taken a class that discussed anthropology and funeral rites?\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - High School\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - College\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - Graduate School\n* Have you ever read any books on anthropology and funeral rites?\n* If you answered \"yes\" above, please approximate the number of books you have read on this topic:\n* Have you ever read any magazine, newspaper, or academic articles on anthropology and funeral rites?\n* If you answered \"yes\" above, please approximate the number of articles you have read on this topic:\n* Have you ever read any articles while browsing the internet  (e.g., on Reddit, Pinterest, Buzzfeed) on anthropology and funeral rites?\n* If you answered \"yes\" above, please approximate the number of articles on the internet you have read on this topic:\n* Have you ever seen any documentaries (e,g., on the Discovery channel) on anthropology and funeral rites?\n* If you answered \"yes\" above, please approximate the number of documentaries you have seen on this topic:\n* Have you ever heard any radio shows or podcasts (e.g., NPR) on anthropology and funeral rites?\n* If you answered \"yes\" above, please approximate the number of radio shows or podcasts you have heard on this topic:\n* I enjoy having a clear and structured mode of life.\n* I like to have a place for everything and everything in its place.\n* I find that establishing a consistent routine enables me to enjoy life more\n* I find that a well ordered life with regular hours suits my temperament.\n* My personal space is usually messy and disorganized.\n* I believe that orderliness and organization are among the most important characteristics of a good student.\n* I think that having clear rules and order at work is essential for success.\n* I think that I would learn best in a class that lacks clearly stated objectives and requirements.\n* I dislike the routine aspects of my work (studies).\n* I hate to change my plans at the last minute.\n* I dislike it when a person's statement could mean many different things.\n* I feel uncomfortable when someone's meaning or intention is unclear to me.\n* I feel uncomfortable when I don't understand the reason why an event occurred in my life.\n* It's annoying to listen to someone who cannot seem to make up his or her mind.\n* When I am confused about an important issue, I feel very upset.\n* I like to know what people are thinking all the time.\n* In most social conflicts, I can easily see which side is right and which is wrong.\n* I'd rather know bad news than stay in a state of uncertainty.\n* I don't like situations that are uncertain.\n* When thinking about a problem, I consider as many different opinions on the issue as possible.\n* When considering most conflict situations, I can usually see how both sides could be right.\n* I always see many possible solutions to problems I face.\n* I do not usually consult many different opinions before forming own view.\n* Even after I've made up my mind about something, I am always eager to consider a different opinion.\n* I prefer interacting with people whose opinions are very different from my own.\n* I dislike questions which could be answered in many different ways.\n* I feel irritated when one person disagrees with what everyone else in a group believes.\n* I like to have friends who are unpredictable.\n* When dining out, I like to go to places where I have been before so that I know what to expect.\n* I don't like to go into a situation without knowing what I can expect from it\n* I think it is fun to change my plans at the last moment.\n* I enjoy the uncertainty of going into a new situation without knowing what might happen.\n* I don't like to be with people who are capable of unexpected actions.\n* I prefer to socialize with familiar friends because I know what to expect from them.\n* I dislike unpredictable situations.\n* When I go shopping, I have difficulty deciding exactly what it is that I want.\n* When faced with a problem I usually see the one best solution very quickly.\n* I tend to put off making important decisions until the last possible moment.\n* I usually make important decisions quickly and confidently.\n* I would describe myself as indecisive.\n* I tend to struggle with most decisions.\n* When trying to solve a problem I often see so many possible options that it's confusing.\n* Are you male or female?\n* Q60 - What is your date of birth? (mm/dd/yyyy)\n* What is the highest level of education you have completed?\n* Q64 - What is your yearly household income?\n* Q64 - What is your religious affiliation?\n* Q64 - What is your racial or ethnic identity?\n* Q64 - What is your age in years?\n* How would you describe your political attitudes? Please select one of the points on the scale below.\n* Please rate your overall ability in the English language:\n* 1. Did you find any aspect of the procedure odd or confusing?\n* 2. What did you think we were studying?\n* 3. Do you think that there may have been more to this study than meets the eye? If so, what do you think this might have been?\n* 4. Do you have any additional thoughts or comments about the study?\n* Thank you for completing this survey! We just have one last question for you. You will not be penalized for your answer to this question. Since you completed the whole survey, you will receive payment no matter what answer you give here.\n\t \n\n\tIt's very important to the quality and scientific aims of our study that participants pay attention (i.e., read the survey carefully, consider the response options, and avoid distractions).\n\n\t\n\n\tWere you paying attention while completing this survey?\n```\n:::\n:::\n\n\n### Exclusion Criterias\n\nData preparation for further analyses\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Attention check and deletion of cases that didn't attend or finish the study ----\nexpertise6_new<-expertise6_clean%>%\n  filter(attention==1&finished==1)\n\n##Exclude the participants that joined outside of US ----\n\n#view(expertise6_new)\nexpertise6_new<-expertise6_new%>%\n  filter(ip!=\"66.42.251.231\")\n\nexpertise6_new <- expertise6_new %>%\n  filter(!(ip %in% c(\"74.219.142.226\", \"24.12.92.17\", \"184.88.52.194\", \n                     \"97.103.220.145\", \"76.250.238.38\")))\n\n##selecting the columns that we want to keep ----\nexpertise6_new<-expertise6_new%>%\n  select(-finished,-birthdate,-proceure_confu,-whatwestudied,-moretothisstudy,-additional_thoughts,-attention, -surveyorder)\n\n#adds a column to the dataframe, with the name \"id\"\nexpertise6_new<-cbind(ID = 1:nrow(expertise6_new), expertise6_new)\n\n# Numeric variables ----\n# Change the data type of the variables to numeric \nexpertise6_new <- expertise6_new %>%\n  mutate_at(vars(stih_food, r_ih_food, stih_sports, r_ih_sports, stih_school, r_ih_school, stih_architect, r_ih_architect, \n                 stih_product, r_ih_product, stih_langu, r_ih_langu, stih_network, r_ih_network, stih_anthro, r_ih_anthro), as.numeric)\n```\n:::\n\n\n## Correlation for ih scores\n\nCheck the correlations between inherence (the variables starting with st) and reverse inherence (the variables starting with r) scores to check whether it's appropriate for averaging\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Correlations between ih scores ----\n\n# Create a list of variable names\nvariables <- c(\"stih_food\", \"r_ih_food\", \"stih_sports\", \"r_ih_sports\", \"stih_school\", \"r_ih_school\", \"stih_architect\", \"r_ih_architect\", \"stih_product\", \"r_ih_product\", \"stih_langu\", \"r_ih_langu\", \"stih_network\", \"r_ih_network\", \"stih_anthro\", \"r_ih_anthro\")\n\n# Initialize an empty data frame to store the correlation coefficients\ncorrelations <- data.frame(variable1 = character(), variable2 = character(), correlation = numeric(), p.value = numeric(), conf.int = character())\n\n# Iterate over the pairs of variables\nfor (i in seq(1, length(variables), 2)) {\n  j <- i + 1\n  \n  # Calculate the Pearson correlation coefficient and test the statistical significance\n  correlation_test <- cor.test(expertise6_new[, variables[i]], expertise6_new[, variables[j]], method = \"pearson\")\n  \n  # Add the correlation coefficient, p-value, and confidence interval to the data frame\n  correlations <- rbind(correlations, data.frame(variable1 = variables[i], variable2 = variables[j], correlation = correlation_test$estimate, p.value = correlation_test$p.value, conf.int = paste(correlation_test$conf.int[1], correlation_test$conf.int[2], sep = \" - \")))}\n\n## View the correlation coefficients and statistical measures ----\ncorrelations\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          variable1      variable2 correlation      p.value\ncor       stih_food      r_ih_food  -0.4463858 8.615861e-11\ncor1    stih_sports    r_ih_sports  -0.5753914 2.560996e-18\ncor2    stih_school    r_ih_school  -0.7267533 8.094747e-33\ncor3 stih_architect r_ih_architect  -0.6411731 1.275191e-23\ncor4   stih_product   r_ih_product  -0.6038601 1.834983e-20\ncor5     stih_langu     r_ih_langu  -0.5635496 1.740797e-17\ncor6   stih_network   r_ih_network  -0.5898848 2.202312e-19\ncor7    stih_anthro    r_ih_anthro  -0.5442449 3.376865e-16\n                                    conf.int\ncor  -0.553035668300546 - -0.325343101426943\ncor1 -0.662979980870432 - -0.472263234194049\ncor2  -0.787333698376605 - -0.65227324221234\ncor3 -0.717624383417027 - -0.549452592494561\ncor4 -0.686743816747194 - -0.505475832894281\ncor5 -0.653042937790108 - -0.458533845459347\ncor6  -0.675100225685499 - -0.48913522712539\ncor7 -0.636777009250974 - -0.436258848944745\n```\n:::\n:::\n\n\n## IH scores calculation\n\nIt seems that each pairs have negative significant correlation, so we can take the average scores to calculate inherence scores\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Average of ih scores  ----\n#It seems that each pairs have negative significant correlation, so we can take the average scores to measure inherence scores\nexpertise6_new <- expertise6_new %>%\n  mutate(ih_food = (stih_food + (10 - r_ih_food))/2,\n         ih_sports = (stih_sports + (10 - r_ih_sports)) / 2,\n         ih_school = (stih_school + (10 - r_ih_school)) / 2,\n         ih_architect = (stih_architect + (10 - r_ih_architect)) / 2,\n         ih_product = (stih_product + (10 - r_ih_product)) / 2,\n         ih_langu = (stih_langu + (10 - r_ih_langu)) / 2,\n         ih_network = (stih_network + (10-r_ih_network)) / 2,\n         ih_anthro = (stih_anthro + (10-r_ih_anthro)) / 2 )\n```\n:::\n\n\n## Need for Cognition scores calculation\n\nCalculate \"Need for cognition\" scale scores\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Need for cognition scale scores  ----\n\n# change the data type of the variables to numeric\nexpertise6_new <- expertise6_new %>%\n  mutate_at(vars( 'needforclo1','needforclo2','needforclo3','needforclo4','needforclo5',\n                        'needforclo6','needforclo7','needforclo8','needforclo9','needforclo10',\n                        'needforclo11','needforclo12','needforclo13','needforclo14',\n                        'needforclo15','needforclo16','needforclo17','needforclo18',\n                        'needforclo19','needforclo20','needforclo21','needforclo22',\n                        'needforclo23','needforclo24','needforclo25','needforclo26',\n                        'needforclo27','needforclo28','needforclo29','needforclo30',\n                        'needforclo31','needforclo32','needforclo33','needforclo34',\n                        'needforclo35','needforclo36','needforclo37','needforclo38',\n                        'needforclo39','needforclo40','needforclo41','needforclo42'), as.numeric)\n\n## Calculate needforclo scores  ----\n#add a new variable called needforclo, which is the sum of all the need for cognition items, the items are weighted according to the scoring key\nexpertise6_new <- expertise6_new %>%\n  group_by(ID)%>%\n  mutate(needforclo=(needforclo1+needforclo2+needforclo3+needforclo4+\n                       (10-needforclo5)+needforclo6+needforclo7+(10-needforclo8)+\n                       (10-needforclo9)+needforclo10+ needforclo11+needforclo12+\n                       needforclo13+needforclo14+needforclo15+needforclo16+\n                       needforclo17+needforclo18+needforclo19+(10-needforclo20)+\n                       (10-needforclo21)+(10-needforclo22)+needforclo23+(10-needforclo24)+\n                       (10-needforclo25)+needforclo26+needforclo27+(10-needforclo28)+\n                       needforclo29+needforclo30+(10-needforclo31)+(10-needforclo32)+\n                       needforclo33+needforclo34+needforclo35+(10-needforclo36)+\n                       needforclo37+(10-needforclo38)+needforclo39+(10-needforclo40)+\n                       (10-needforclo41)+needforclo42)/42)\n```\n:::\n\n\n## Data preparation\n\nPrepare the expertise scores and other scores ready for analyses\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Replace expertise variables' NA values in the expertise columns with 0  ----\nvariables <- c('know_food','know_sports','know_school','know_architect',\n                        'know_product','know_langu','know_network','know_anthro',\n                        'high_food', 'colle_food', 'grad_food',\n                        'numbook_food','numarticle_food',\n                        'numinarti_food', 'numdocu_food',\n                        'numradio_food',\n                        'high_sports', 'colle_sports', 'grad_sports',\n                        'numbook_sports','numarticle_sports',\n                        'numinarti_sports', 'numdocu_sports',\n                        'numradio_sports',\n                        'high_school', 'colle_school', 'grad_school',\n                        'numbook_school','numarticle_school',\n                        'numinarti_school','numdocu_school',\n                        'numradio_school',\n                        'high_architect', 'colle_architect','grad_architect',\n                        'numbook_architect',\n                        'numarticle_architect',\n                        'numinarti_architect', \n                        'numdocu_architect',\n                        'numradio_architect',\n                        'high_product', 'colle_product', 'grad_product',\n                        'numbook_product',\n                        'numarticle_product',\n                       'numinarti_product','numdocu_product',\n                        'numradio_product',\n                        'high_langu', 'colle_langu', 'grad_langu',\n                        'numbook_langu','numarticle_langu',\n                        'numinarti_langu', 'numdocu_langu',\n                        'numradio_langu',\n                        'high_network', 'colle_network', 'grad_network',\n                        'numbook_network','numarticle_network',\n                        'numinarti_network', 'numdocu_network',\n                        'numradio_network',\n                        'high_anthro', 'colle_anthro','grad_anthro',\n                        'numbook_anthro','numarticle_anthro',\n                        'numinarti_anthro', 'numdocu_anthro',\n                        'numradio_anthro')\n\nexpertise6_new <- expertise6_new %>%\n  mutate_at(vars('know_food','know_sports','know_school','know_architect',\n                        'know_product','know_langu','know_network','know_anthro',\n                        'high_food', 'colle_food', 'grad_food',\n                        'numbook_food','numarticle_food',\n                        'numinarti_food', 'numdocu_food',\n                        'numradio_food',\n                        'high_sports', 'colle_sports', 'grad_sports',\n                        'numbook_sports','numarticle_sports',\n                        'numinarti_sports', 'numdocu_sports',\n                        'numradio_sports',\n                        'high_school', 'colle_school', 'grad_school',\n                        'numbook_school','numarticle_school',\n                        'numinarti_school','numdocu_school',\n                        'numradio_school',\n                        'high_architect', 'colle_architect','grad_architect',\n                        'numbook_architect',\n                        'numarticle_architect',\n                        'numinarti_architect', \n                        'numdocu_architect',\n                        'numradio_architect',\n                        'high_product', 'colle_product', 'grad_product',\n                        'numbook_product',\n                        'numarticle_product',\n                       'numinarti_product','numdocu_product',\n                        'numradio_product',\n                        'high_langu', 'colle_langu', 'grad_langu',\n                        'numbook_langu','numarticle_langu',\n                        'numinarti_langu', 'numdocu_langu',\n                        'numradio_langu',\n                        'high_network', 'colle_network', 'grad_network',\n                        'numbook_network','numarticle_network',\n                        'numinarti_network', 'numdocu_network',\n                        'numradio_network',\n                        'high_anthro', 'colle_anthro','grad_anthro',\n                        'numbook_anthro','numarticle_anthro',\n                        'numinarti_anthro', 'numdocu_anthro',\n                        'numradio_anthro'), as.numeric)\n\n\n#expertise6_new[variables] <- lapply(expertise6_new[variables], \n                                    #function(x) ifelse(is.na(x), 0, ifelse(x=='no',0, x)))\n\nexpertise6_new[variables] <- lapply(expertise6_new[variables], function(x) replace(x, is.na(x) | !is.numeric(x) , 0))\n```\n:::\n\n\n## Detect the ID of missing values of expertise scores\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmissing_ids <- unlist(mapply(function(x) expertise6_new$ID[which(is.na(expertise6_new[x]))], c('know_food','know_sports','know_school','know_architect',\n                        'know_product','know_langu','know_network','know_anthro',\n                        'high_food', 'colle_food', 'grad_food',\n                        'numbook_food','numarticle_food',\n                        'numinarti_food', 'numdocu_food',\n                        'numradio_food',\n                        'high_sports', 'colle_sports', 'grad_sports',\n                        'numbook_sports','numarticle_sports',\n                        'numinarti_sports', 'numdocu_sports',\n                        'numradio_sports',\n                        'high_school', 'colle_school', 'grad_school',\n                        'numbook_school','numarticle_school',\n                        'numinarti_school','numdocu_school',\n                        'numradio_school',\n                        'high_architect', 'colle_architect','grad_architect',\n                        'numbook_architect',\n                        'numarticle_architect',\n                        'numinarti_architect', \n                        'numdocu_architect',\n                        'numradio_architect',\n                        'high_product', 'colle_product', 'grad_product',\n                        'numbook_product',\n                        'numarticle_product',\n                       'numinarti_product','numdocu_product',\n                        'numradio_product',\n                        'high_langu', 'colle_langu', 'grad_langu',\n                        'numbook_langu','numarticle_langu',\n                        'numinarti_langu', 'numdocu_langu',\n                        'numradio_langu',\n                        'high_network', 'colle_network', 'grad_network',\n                        'numbook_network','numarticle_network',\n                        'numinarti_network', 'numdocu_network',\n                        'numradio_network',\n                        'high_anthro', 'colle_anthro','grad_anthro',\n                        'numbook_anthro','numarticle_anthro',\n                        'numinarti_anthro', 'numdocu_anthro',\n                        'numradio_anthro')))\n\ncat(ifelse(length(missing_ids[missing_ids > 0]) > 0, paste(\"The following IDs are missing:\",missing_ids[missing_ids != 0]), \"There is no missing IDs.\"),\"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThere is no missing IDs. \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Expertise Ready Df ----\n\n#','high_food','colle_food','grad_food','numbook_food','numarticle_food','numinarti_food','numdocu_food','numradio_food','high_sports','colle_sports','grad_sports','numbook_sports','numarticle_sports','numinarti_sports','numdocu_sports','numradio_sports','high_school','colle_school','grad_school','numbook_school','numarticle_school','numinarti_school','numdocu_school','numradio_school','high_architect','colle_architect','grad_architect','numbook_architect','numarticle_architect','numinarti_architect','numdocu_architect','numradio_architect','high_product','colle_product','grad_product','numbook_product','numarticle_product','numinarti_product','numdocu_product','numradio_product','high_langu','colle_langu','grad_langu','numbook_langu','numarticle_langu','numinarti_langu','numdocu_langu','numradio_langu','high_network','colle_network','grad_network','numbook_network','numarticle_network','numinarti_network','numdocu_network','numradio_network','high_anthro','colle_anthro','grad_anthro','numbook_anthro','numarticle_anthro','numinarti_anthro','numdocu_anthro','numradio_anthro\n\n# change the data type of the variables to numeric\nexpertise6_new<-expertise6_new%>%mutate_at(vars('know_food','know_sports','know_school','know_architect','know_product','know_langu','know_network','know_anthro'),as.numeric)\n```\n:::\n\n\n\n## Long format\n\nLong format of expertise dataset for factor analysis\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexpertise6_factor <- melt(expertise6_new, id.vars = c(\"ID\",'duration',\"sex\" ,\"education\" ,\"income\",\"religion\",'identity','age','political_atti','english_level','needforclo'), \n                        measure.vars = c(\"ih_food\", \"ih_sports\",\"ih_school\",\"ih_architect\",\n                                         \"ih_product\",\"ih_langu\",\"ih_network\",\"ih_anthro\",\n                       'know_food','know_sports','know_school','know_architect',\n                        'know_product','know_langu','know_network','know_anthro',\n                        'high_food', 'colle_food', 'grad_food',\n                        'numbook_food','numarticle_food',\n                        'numinarti_food', 'numdocu_food',\n                        'numradio_food',\n                        'high_sports', 'colle_sports', 'grad_sports',\n                        'numbook_sports','numarticle_sports',\n                        'numinarti_sports', 'numdocu_sports',\n                        'numradio_sports',\n                        'high_school', 'colle_school', 'grad_school',\n                        'numbook_school','numarticle_school',\n                        'numinarti_school','numdocu_school',\n                        'numradio_school',\n                        'high_architect', 'colle_architect','grad_architect',\n                        'numbook_architect',\n                        'numarticle_architect',\n                        'numinarti_architect', \n                        'numdocu_architect',\n                        'numradio_architect',\n                        'high_product', 'colle_product', 'grad_product',\n                        'numbook_product',\n                        'numarticle_product',\n                       'numinarti_product','numdocu_product',\n                        'numradio_product',\n                        'high_langu', 'colle_langu', 'grad_langu',\n                        'numbook_langu','numarticle_langu',\n                        'numinarti_langu', 'numdocu_langu',\n                        'numradio_langu',\n                        'high_network', 'colle_network', 'grad_network',\n                        'numbook_network','numarticle_network',\n                        'numinarti_network', 'numdocu_network',\n                        'numradio_network',\n                        'high_anthro', 'colle_anthro','grad_anthro',\n                        'numbook_anthro','numarticle_anthro',\n                        'numinarti_anthro', 'numdocu_anthro',\n                        'numradio_anthro'),\n                        sep = \"_\", variable.name = \"Category\", value.name = \"Score\")\n\n# Split the Category column into two columns based on the underscore separator\nexpertise6_factor <- expertise6_factor %>% separate(Category, into = c(\"Category\", \"Score_Type\"), sep = \"_\")\n\n## spread the data from long to wide format  ----\nexpertise6_fact <- expertise6_factor %>% spread(Category, Score)\n\n# change the score type to a factor\nexpertise6_fact$Score_Type<-as.factor(expertise6_fact$Score_Type)\n\n# convert the column ih to numeric\nexpertise6_fact$ih<-as.numeric(expertise6_fact$ih)\n```\n:::\n\n\n\n###Create another data frame to winsorize expertise variables before factor analysis\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexpertise6_wins_fact<-expertise6_fact\n```\n:::\n\n\n\n## Winsorize the variables at 1%\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# winsorize the variables (at 1%)\nexpertise6_fact1 <- expertise6_fact%>%\n  mutate(numarticle=Winsorize(numarticle, probs = c(0,0.99)), \n         numbook=Winsorize(numbook, probs = c(0,0.99)),\n         high=Winsorize(high, probs = c(0,0.99)),\n         colle=Winsorize(colle, probs = c(0,0.99)),\n         grad=Winsorize(grad,na.rm=TRUE, probs = c(0,0.99)),\n         numdocu=Winsorize(numdocu,na.rm=TRUE, probs = c(0,0.99)),\n         numinarti=Winsorize(numinarti,na.rm=TRUE, probs = c(0,0.99)),\n         numradio=Winsorize(numradio,na.rm=TRUE, probs = c(0,0.99)))\n```\n:::\n\n\n\n## Factor analysis\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Factor analysis for expertise variables with raw scores ----\n\n# Import packages \n\nlibrary(psych) #PCA/EFA analysis\nlibrary(REdaS) #Produces KMO and Bartletts test\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: grid\n```\n:::\n\n```{.r .cell-code}\nlibrary(GPArotation)\n\n\n# Create a new dataframe that include only related variables\nfactor_exp<-expertise6_fact1%>%\n  select(colle, grad, high, numarticle, numbook, numdocu, numinarti, numradio)\n\n# Check missing values\napply(is.na(factor_exp), 2, sum)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     colle       grad       high numarticle    numbook    numdocu  numinarti \n         0          0          0          0          0          0          0 \n  numradio \n         0 \n```\n:::\n:::\n\n\n\n\nFactor analysis for expertise variables with raw scores\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Since grad classes for TV category is missing (nobody takes any class in the sample), listwise deletion is applied here.\nbart_spher(factor_exp, use = \"complete.obs\") ###### produces Bartletts test of spherecity \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\tBartlett's Test of Sphericity\n\nCall: bart_spher(x = factor_exp, use = \"complete.obs\")\n\n     X2 = 3334.573\n     df = 28\np-value < 2.22e-16\n```\n:::\n\n```{.r .cell-code}\nKMO(factor_exp)       ###### Kaiser-Meyer-Olkin measure, which is above .5.\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nKaiser-Meyer-Olkin factor adequacy\nCall: KMO(r = factor_exp)\nOverall MSA =  0.79\nMSA for each item = \n     colle       grad       high numarticle    numbook    numdocu  numinarti \n      0.77       0.83       0.70       0.79       0.81       0.83       0.77 \n  numradio \n      0.84 \n```\n:::\n\n```{.r .cell-code}\n#Check eigenvalues\n\nfa.parallel(factor_exp)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nParallel analysis suggests that the number of factors =  3  and the number of components =  2 \n```\n:::\n:::\n\n\n### Factor analysis\n\n::: {.cell}\n\n```{.r .cell-code}\n# So we can reduce it to 2 factors\nfa(factor_exp, nfactors = 2, rotate =  \"oblimin\" )  \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFactor Analysis using method =  minres\nCall: fa(r = factor_exp, nfactors = 2, rotate = \"oblimin\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n             MR1   MR2   h2   u2 com\ncolle      -0.04  0.83 0.66 0.34 1.0\ngrad        0.09  0.35 0.16 0.84 1.1\nhigh       -0.03  0.40 0.15 0.85 1.0\nnumarticle  0.77  0.06 0.63 0.37 1.0\nnumbook     0.44  0.47 0.62 0.38 2.0\nnumdocu     0.58 -0.06 0.30 0.70 1.0\nnuminarti   0.89 -0.04 0.76 0.24 1.0\nnumradio    0.30  0.04 0.10 0.90 1.0\n\n                       MR1  MR2\nSS loadings           2.10 1.30\nProportion Var        0.26 0.16\nCumulative Var        0.26 0.42\nProportion Explained  0.62 0.38\nCumulative Proportion 0.62 1.00\n\n With factor correlations of \n     MR1  MR2\nMR1 1.00 0.49\nMR2 0.49 1.00\n\nMean item complexity =  1.2\nTest of the hypothesis that 2 factors are sufficient.\n\nThe degrees of freedom for the null model are  28  and the objective function was  2.18 with Chi Square of  3334.57\nThe degrees of freedom for the model are 13  and the objective function was  0.08 \n\nThe root mean square of the residuals (RMSR) is  0.03 \nThe df corrected root mean square of the residuals is  0.05 \n\nThe harmonic number of observations is  1536 with the empirical chi square  90.74  with prob <  1e-13 \nThe total number of observations was  1536  with Likelihood Chi Square =  123.66  with prob <  3.8e-20 \n\nTucker Lewis Index of factoring reliability =  0.928\nRMSEA index =  0.074  and the 90 % confidence intervals are  0.063 0.087\nBIC =  28.27\nFit based upon off diagonal values = 0.99\nMeasures of factor score adequacy             \n                                                   MR1  MR2\nCorrelation of (regression) scores with factors   0.93 0.88\nMultiple R square of scores with factors          0.87 0.77\nMinimum correlation of possible factor scores     0.73 0.54\n```\n:::\n\n```{.r .cell-code}\n# Figure for the analysis\n\nM1<-fa(factor_exp, nfactors = 2, rotate =  \"oblimin\" ) ##save the analysis as the object m1\nfa.diagram(M1,main=\"Expertise Variables\")  \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\nSo here we have two factors and we can investigate them as classes and media-literature part, let's extract values for now, but creating these categories with averaging related variables may be a better way for the sake of conceptual understanding.\n\n### Extracting factor values\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfactor_exp_score <- factanal(factor_exp, factors=2, scores=\"regression\", rotation = \"oblimin\", na.rm=TRUE)\n\nhead(factor_exp_score$scores)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         Factor1    Factor2\n[1,]  0.02119905 -0.4233947\n[2,]  0.22119338 -0.4679913\n[3,] -0.14582577 -0.3159511\n[4,]  0.17926929 -0.5193750\n[5,] -0.26945985 -0.2581370\n[6,]  0.82731651 -0.4607261\n```\n:::\n\n```{.r .cell-code}\nfactor_exp_comb <- bind_cols(factor_exp, data.frame(factor_exp_score$scores))\n\nfactor_exp_comb$class<-factor_exp_comb$Factor1\n\nfactor_exp_comb$media_grad<-factor_exp_comb$Factor2\n```\n:::\n\n\n### Histogram and descriptives for factor scores\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndescriptives(dat=factor_exp_comb, vars(Factor1, Factor2),\n             sd=T)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n DESCRIPTIVES\n\n Descriptives                                            \n ─────────────────────────────────────────────────────── \n                         Factor1          Factor2        \n ─────────────────────────────────────────────────────── \n   N                              1536            1536   \n   Missing                           0               0   \n   Mean                  -5.305905e-16    8.812395e-16   \n   Median                   -0.2694598      -0.2581370   \n   Standard deviation        0.9977506       0.9220806   \n   Minimum                   -2.856278       -3.778751   \n   Maximum                    7.978103        6.476149   \n ─────────────────────────────────────────────────────── \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(factor_exp_comb$Factor1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(factor_exp_comb$Factor2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\n## Winsorize the variables at 1%\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# winsorize the variables (at 1%)\nexpertise6_ready <- expertise6_fact%>%\n  mutate(numarticle_wins_1=Winsorize(numarticle, probs = c(0,0.99)), \n         numbook_wins_1=Winsorize(numbook, probs = c(0,0.99)),\n         high_wins_1=Winsorize(high, probs = c(0,0.99)),\n         colle_wins_1=Winsorize(colle, probs = c(0,0.99)),\n         grad_wins_1=Winsorize(grad,na.rm=TRUE, probs = c(0,0.99)),\n         docu_wins_1=Winsorize(numdocu,na.rm=TRUE, probs = c(0,0.99)),\n         inarti_wins_1=Winsorize(numinarti,na.rm=TRUE, probs = c(0,0.99)),\n         radio_wins_1=Winsorize(numradio,na.rm=TRUE, probs = c(0,0.99)))\n\n#check descriptives\ndescriptives(dat=expertise6_ready, vars(docu_wins_1,inarti_wins_1, radio_wins_1 ), median=F, n=F, missing=T, sd=T, skew =T)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n DESCRIPTIVES\n\n Descriptives                                                            \n ─────────────────────────────────────────────────────────────────────── \n                          docu_wins_1    inarti_wins_1    radio_wins_1   \n ─────────────────────────────────────────────────────────────────────── \n   Missing                          0                0               0   \n   Mean                      2.194010         9.023438       0.7988281   \n   Standard deviation        5.699259         26.33146        2.767993   \n   Minimum                   0.000000         0.000000        0.000000   \n   Maximum                   38.25000         191.2500        20.00000   \n   Skewness                  4.286225         4.959908        5.015617   \n   Std. error skewness     0.06243908       0.06243908      0.06243908   \n ─────────────────────────────────────────────────────────────────────── \n```\n:::\n\n```{.r .cell-code}\ndescriptives(dat=expertise6_ready, vars(high_wins_1, colle_wins_1), median=F, n=F, missing=T, sd=T)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n DESCRIPTIVES\n\n Descriptives                                          \n ───────────────────────────────────────────────────── \n                         high_wins_1    colle_wins_1   \n ───────────────────────────────────────────────────── \n   Missing                         0               0   \n   Mean                    0.3147135       0.5794271   \n   Standard deviation      0.8859480        1.522536   \n   Minimum                  0.000000        0.000000   \n   Maximum                  4.650000        10.00000   \n ───────────────────────────────────────────────────── \n```\n:::\n\n```{.r .cell-code}\ndescriptives(dat=expertise6_ready, vars(numarticle_wins_1, numbook_wins_1, grad_wins_1), median=F, n=F, missing=T, sd=T)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n DESCRIPTIVES\n\n Descriptives                                                                 \n ──────────────────────────────────────────────────────────────────────────── \n                         numarticle_wins_1    numbook_wins_1    grad_wins_1   \n ──────────────────────────────────────────────────────────────────────────── \n   Missing                               0                 0              0   \n   Mean                           5.916667          1.592448     0.07291667   \n   Standard deviation             16.05380          4.371575      0.4732818   \n   Minimum                        0.000000          0.000000       0.000000   \n   Maximum                        100.0000          30.00000       4.000000   \n ──────────────────────────────────────────────────────────────────────────── \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Standardize the Selected variables  ----\nvars_to_standardize <- c(\"know\", \"docu_wins_1\",\"inarti_wins_1\", \"radio_wins_1\",\"high_wins_1\", \"colle_wins_1\",\n                         \"numarticle_wins_1\", \"numbook_wins_1\", \"grad_wins_1\")\n\n\n\nexpertise6_ready<-expertise6_ready %>% \n  group_by(Score_Type) %>% \n  mutate_at(vars(vars_to_standardize), scale)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsum(is.na(expertise6_ready$grad_wins_1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 384\n```\n:::\n:::\n\n\n\n## Take the means of winsorized variables to create our new categories: media and class\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexpertise6_ready <- expertise6_ready %>% replace_na(list(colle_wins_1 = NA, high_wins_1 = NA, grad_wins_1 = NA, docu_wins_1= NA,inarti_wins_1 = NA, radio_wins_1 = NA, numarticle_wins_1 = NA, numbook_wins_1 = NA))\n\n\nexpertise6_ready <- expertise6_ready %>% mutate(class_wins1 = mapply(function(x, y, z) {\n  ifelse(is.na(x), (y + z)/2, \n         ifelse(is.na(y), (x + z)/2, \n                ifelse(is.na(z), (x + y)/2, (x + y + z)/3)))}, \n  grad_wins_1, colle_wins_1, high_wins_1))\n\nexpertise6_ready <- expertise6_ready %>% mutate(media_wins1 = mapply(function(x, y, z, t, r) {\n  ifelse(is.na(x), (y + z + t + r)/4, \n         ifelse(is.na(y), (x + z + t + r)/4,\n                ifelse(is.na(z), (x + y + t + r)/4, \n                       ifelse(is.na(t),(x + y + z + r)/4, \n                              ifelse(is.na(r),(x + y + z + t)/4,\n                                     (x + y + z + t + r)/5)))))}, \n  docu_wins_1,inarti_wins_1, radio_wins_1, numbook_wins_1, grad_wins_1))\n```\n:::\n\n\n\n## Center the variables\n\n::: {.cell}\n\n```{.r .cell-code}\nexpertise6_ready$know_cent <- scale(expertise6_ready$know, center = TRUE, scale = FALSE)\n\nexpertise6_ready$needforclo_cent <- scale(expertise6_ready$needforclo, center = TRUE, scale = FALSE)\n\nexpertise6_ready$class_wins1_cent <- scale(expertise6_ready$class_wins1, center = TRUE, scale = FALSE)\n\nexpertise6_ready$media_wins1_cent <- scale(expertise6_ready$media_wins1, center = TRUE, scale = FALSE)\n```\n:::\n\n\n\n## HLM Models\n\n### Model objective expertise with grad courses, books and magazines\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Model objective expertise ----\n\nModel_obj<-lmer(ih ~class_wins1_cent*needforclo_cent+class_wins1_cent+needforclo_cent+\n                (1|Score_Type)+(1|ID), data=expertise6_ready)\nsummary(Model_obj)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: ih ~ class_wins1_cent * needforclo_cent + class_wins1_cent +  \n    needforclo_cent + (1 | Score_Type) + (1 | ID)\n   Data: expertise6_ready\n\nREML criterion at convergence: 6320.8\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.93455 -0.65040  0.02653  0.66826  2.56442 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.7734   0.8794  \n Score_Type (Intercept) 0.7745   0.8801  \n Residual               3.1236   1.7674  \nNumber of obs: 1528, groups:  ID, 191; Score_Type, 8\n\nFixed effects:\n                                   Estimate Std. Error         df t value\n(Intercept)                         5.10303    0.32079    7.58400  15.907\nclass_wins1_cent                   -0.07206    0.06840 1503.58449  -1.053\nneedforclo_cent                     0.18572    0.11261  189.81958   1.649\nclass_wins1_cent:needforclo_cent    0.02288    0.08782 1510.75809   0.261\n                                 Pr(>|t|)    \n(Intercept)                      4.24e-07 ***\nclass_wins1_cent                    0.292    \nneedforclo_cent                     0.101    \nclass_wins1_cent:needforclo_cent    0.794    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cls_1_ ndfrc_\nclss_wns1_c  0.000              \nnedfrcl_cnt  0.000  0.013       \nclss_wn1_:_  0.004  0.093 -0.050\n```\n:::\n:::\n\n\n### Model objective expertise with high school and college courses\n\n\n::: {.cell}\n\n```{.r .cell-code}\nModel_obj2<-lmer(ih ~media_wins1_cent * needforclo_cent +media_wins1_cent + needforclo_cent+(1|Score_Type)+(1|ID), data=expertise6_ready)\nsummary(Model_obj2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: ih ~ media_wins1_cent * needforclo_cent + media_wins1_cent +  \n    needforclo_cent + (1 | Score_Type) + (1 | ID)\n   Data: expertise6_ready\n\nREML criterion at convergence: 6315.2\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.94495 -0.65906  0.02574  0.65749  2.56054 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.7589   0.8711  \n Score_Type (Intercept) 0.7772   0.8816  \n Residual               3.1176   1.7657  \nNumber of obs: 1528, groups:  ID, 191; Score_Type, 8\n\nFixed effects:\n                                   Estimate Std. Error         df t value\n(Intercept)                         5.10200    0.32119    7.57087  15.884\nmedia_wins1_cent                   -0.18698    0.07638 1505.70217  -2.448\nneedforclo_cent                     0.18792    0.11181  188.37578   1.681\nmedia_wins1_cent:needforclo_cent   -0.07847    0.10019 1474.34770  -0.783\n                                 Pr(>|t|)    \n(Intercept)                      4.37e-07 ***\nmedia_wins1_cent                   0.0145 *  \nneedforclo_cent                    0.0945 .  \nmedia_wins1_cent:needforclo_cent   0.4336    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) md_w1_ ndfrc_\nmd_wns1_cnt  0.000              \nnedfrcl_cnt  0.000  0.014       \nmd_wns1_c:_  0.004  0.094 -0.036\n```\n:::\n:::\n\n\n### Model of perceived expertise and need for cognition\n\n\n::: {.cell}\n\n```{.r .cell-code}\nModel_perc<-lmer(ih ~know_cent*needforclo_cent+ know_cent + needforclo_cent +\n                (1|Score_Type)+(1|ID), data=expertise6_ready)\nsummary(Model_perc)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: ih ~ know_cent * needforclo_cent + know_cent + needforclo_cent +  \n    (1 | Score_Type) + (1 | ID)\n   Data: expertise6_ready\n\nREML criterion at convergence: 6321.4\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.92364 -0.65771  0.01587  0.66300  2.56876 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.7989   0.8938  \n Score_Type (Intercept) 0.7760   0.8809  \n Residual               3.1140   1.7646  \nNumber of obs: 1528, groups:  ID, 191; Score_Type, 8\n\nFixed effects:\n                            Estimate Std. Error         df t value Pr(>|t|)    \n(Intercept)                  5.10150    0.32130    7.60464  15.878 4.19e-07 ***\nknow_cent                   -0.06434    0.05369 1451.39265  -1.198    0.231    \nneedforclo_cent              0.18403    0.11385  185.25861   1.616    0.108    \nknow_cent:needforclo_cent   -0.02139    0.07491 1397.20099  -0.286    0.775    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) knw_cn ndfrc_\nknow_cent    0.001              \nnedfrcl_cnt  0.000  0.046       \nknw_cnt:nd_  0.012  0.063 -0.040\n```\n:::\n:::\n\n\n### Model of objective expertise, perceived expertise and need for cognition without interaction\n\n\n::: {.cell}\n\n```{.r .cell-code}\nModel.1.1<-lmer(ih ~class_wins1_cent + media_wins1_cent + know_cent +needforclo_cent +(1|Score_Type)+(1|ID),   \n              data=expertise6_ready)\nsummary(Model.1.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: \nih ~ class_wins1_cent + media_wins1_cent + know_cent + needforclo_cent +  \n    (1 | Score_Type) + (1 | ID)\n   Data: expertise6_ready\n\nREML criterion at convergence: 6319.9\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.93086 -0.66258  0.01667  0.66013  2.53736 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.7605   0.8721  \n Score_Type (Intercept) 0.7758   0.8808  \n Residual               3.1202   1.7664  \nNumber of obs: 1528, groups:  ID, 191; Score_Type, 8\n\nFixed effects:\n                   Estimate Std. Error         df t value Pr(>|t|)    \n(Intercept)         5.10291    0.32094    7.57320  15.900 4.32e-07 ***\nclass_wins1_cent    0.02859    0.08345 1490.43898   0.343    0.732    \nmedia_wins1_cent   -0.18826    0.09341 1513.35766  -2.015    0.044 *  \nknow_cent          -0.02028    0.05887 1416.04242  -0.345    0.731    \nneedforclo_cent     0.18335    0.11195  183.73314   1.638    0.103    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cls_1_ md_w1_ knw_cn\nclss_wns1_c  0.000                     \nmd_wns1_cnt  0.000 -0.481              \nknow_cent    0.001 -0.211 -0.216       \nnedfrcl_cnt  0.000  0.000 -0.001  0.046\n```\n:::\n:::\n\n\n### Model of objective expertise, perceived expertise and need for cognition with interaction\n\n\n::: {.cell}\n\n```{.r .cell-code}\nModel.2.1<-lmer(ih ~class_wins1_cent + media_wins1_cent  + know_cent+ needforclo_cent +\n                  class_wins1_cent*needforclo_cent + media_wins1_cent*needforclo_cent +\n                  know_cent*needforclo_cent+(1|Score_Type)+(1|ID),data=expertise6_ready)\nsummary(Model.2.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: \nih ~ class_wins1_cent + media_wins1_cent + know_cent + needforclo_cent +  \n    class_wins1_cent * needforclo_cent + media_wins1_cent * needforclo_cent +  \n    know_cent * needforclo_cent + (1 | Score_Type) + (1 | ID)\n   Data: expertise6_ready\n\nREML criterion at convergence: 6326.7\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.92927 -0.66314  0.01952  0.65538  2.56700 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.7658   0.8751  \n Score_Type (Intercept) 0.7735   0.8795  \n Residual               3.1215   1.7668  \nNumber of obs: 1528, groups:  ID, 191; Score_Type, 8\n\nFixed effects:\n                                   Estimate Std. Error         df t value\n(Intercept)                       5.102e+00  3.206e-01  7.580e+00  15.917\nclass_wins1_cent                  3.627e-02  8.392e-02  1.487e+03   0.432\nmedia_wins1_cent                 -2.009e-01  9.422e-02  1.505e+03  -2.133\nknow_cent                        -1.764e-02  5.906e-02  1.411e+03  -0.299\nneedforclo_cent                   1.831e-01  1.124e-01  1.831e+02   1.629\nclass_wins1_cent:needforclo_cent  1.133e-01  1.156e-01  1.442e+03   0.980\nmedia_wins1_cent:needforclo_cent -1.560e-01  1.334e-01  1.512e+03  -1.170\nknow_cent:needforclo_cent        -7.489e-03  8.398e-02  1.403e+03  -0.089\n                                 Pr(>|t|)    \n(Intercept)                      4.25e-07 ***\nclass_wins1_cent                   0.6656    \nmedia_wins1_cent                   0.0331 *  \nknow_cent                          0.7653    \nneedforclo_cent                    0.1050    \nclass_wins1_cent:needforclo_cent   0.3272    \nmedia_wins1_cent:needforclo_cent   0.2423    \nknow_cent:needforclo_cent          0.9290    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cls_1_ md_w1_ knw_cn ndfrc_ c_1_:_ m_1_:_\nclss_wns1_c  0.001                                          \nmd_wns1_cnt -0.001 -0.484                                   \nknow_cent    0.001 -0.203 -0.221                            \nnedfrcl_cnt  0.000 -0.004 -0.001  0.043                     \nclss_wn1_:_  0.000  0.088 -0.069  0.032 -0.028              \nmd_wns1_c:_ -0.001 -0.068  0.124 -0.053 -0.002 -0.560       \nknw_cnt:nd_  0.011  0.034 -0.052  0.055 -0.021 -0.190 -0.235\n```\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}