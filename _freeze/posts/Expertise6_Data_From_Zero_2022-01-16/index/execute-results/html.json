{
  "hash": "fb12f0d6aa40a83cccd78036dfc61306",
  "result": {
    "markdown": "---\ntitle: \"Expertise6 Data Analysis from the very beginning\"\nauthor: \"MK\"\ndate: \"2023-02-23\"\ncategories: [example data, code, analysis]\nimage: \"newdata.png\"\n---\n\n\nThis post includes the trial analyses of an example data related to expertise.\n\nNotes: Need for closure scores haven't been calculated, the reverse items will be checked. Notes: For the long format, internet article, documentaries and podcast series should be added.\n\n### Import necessary packages and expertise data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Import library ----\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(purrr)\nlibrary(jmv)\nlibrary(psych)\nlibrary(DescTools)\nlibrary(stats)\nlibrary(factoextra)\nlibrary(reshape2)\nlibrary(lme4) \nlibrary(lmerTest)\nlibrary(effects)\n\n\n#Read the csv file ----\nExpertise6 <- read_csv(\"~/Desktop/Expertise6_5.8.14_December 23, 2022_10.51 2.csv\")\n```\n:::\n\n\nCreates a new dataframe called expertise6_clean, which is a copy of the original dataframe called expertise6 and removes the second row of the dataframe and create a variable called column_names and assign it the names of the columns in the dataframe and change the column names\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#removes the second row of the dataframe\nexpertise6_clean<-Expertise6%>% \n  slice(-2) \n\n\n#selects all columns except the ones listed \nexpertise6_clean <- expertise6_clean %>%\n  select(-StartDate, -EndDate, -Status, -Progress,-ResponseId,-RecordedDate,-RecipientLastName, -RecipientFirstName,-RecipientEmail, -ExternalReference, -LocationLatitude,-LocationLongitude, -DistributionChannel, -UserLanguage)\n\n#create a variable called column_names and assign it the names of the columns in the dataframe\ncolumn_names <- names(expertise6_clean)\n\ncolumn_names\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  [1] \"IPAddress\"             \"Duration (in seconds)\" \"Finished\"             \n  [4] \"Q39\"                   \"Q40\"                   \"Q41\"                  \n  [7] \"Q42\"                   \"Q43\"                   \"Q44\"                  \n [10] \"Q45\"                   \"Q46\"                   \"Q47\"                  \n [13] \"Q48\"                   \"Q49\"                   \"Q50\"                  \n [16] \"Q51\"                   \"Q52\"                   \"Q53\"                  \n [19] \"Q54\"                   \"Q147_1\"                \"Q149_1\"               \n [22] \"Q151_1\"                \"Q153_1\"                \"Q155_1\"               \n [25] \"Q157_1\"                \"Q159_1\"                \"Q161_1\"               \n [28] \"Q69\"                   \"Q3_1\"                  \"Q3_2\"                 \n [31] \"Q3_3\"                  \"Q70\"                   \"Q4\"                   \n [34] \"Q71\"                   \"Q5\"                    \"Q204\"                 \n [37] \"Q209...51\"             \"Q206\"                  \"Q210\"                 \n [40] \"Q208\"                  \"Q211...55\"             \"Q73\"                  \n [43] \"Q74_1\"                 \"Q74_2\"                 \"Q74_3\"                \n [46] \"Q75\"                   \"Q76...61\"              \"Q77\"                  \n [49] \"Q78...63\"              \"Q214...64\"             \"Q219...65\"            \n [52] \"Q216\"                  \"Q214...67\"             \"Q218\"                 \n [55] \"Q215...69\"             \"Q80...70\"              \"Q81_1\"                \n [58] \"Q81_2\"                 \"Q81_3\"                 \"Q82\"                  \n [61] \"Q83\"                   \"Q84\"                   \"Q85\"                  \n [64] \"Q217...78\"             \"Q222\"                  \"Q219...80\"            \n [67] \"Q223...81\"             \"Q221...82\"             \"Q224\"                 \n [70] \"Q87\"                   \"Q88_1\"                 \"Q88_2\"                \n [73] \"Q88_3\"                 \"Q89\"                   \"Q90\"                  \n [76] \"Q91\"                   \"Q92\"                   \"Q226\"                 \n [79] \"Q231...93\"             \"Q228\"                  \"Q232\"                 \n [82] \"Q230\"                  \"Q233...97\"             \"Q94\"                  \n [85] \"Q95_1\"                 \"Q95_2\"                 \"Q95_3\"                \n [88] \"Q96\"                   \"Q97\"                   \"Q98\"                  \n [91] \"Q99\"                   \"Q245...106\"            \"Q246...107\"           \n [94] \"Q247...108\"            \"Q248...109\"            \"Q249...110\"           \n [97] \"Q250...111\"            \"Q101\"                  \"Q102_1\"               \n[100] \"Q102_2\"                \"Q102_3\"                \"Q103\"                 \n[103] \"Q104\"                  \"Q105\"                  \"Q106\"                 \n[106] \"Q235...120\"            \"Q240\"                  \"Q237...122\"           \n[109] \"Q241...123\"            \"Q239...124\"            \"Q242\"                 \n[112] \"Q108\"                  \"Q109_1\"                \"Q109_2\"               \n[115] \"Q109_3\"                \"Q110\"                  \"Q111\"                 \n[118] \"Q112\"                  \"Q113\"                  \"Q244\"                 \n[121] \"Q249...135\"            \"Q246...136\"            \"Q250...137\"           \n[124] \"Q248...138\"            \"Q251...139\"            \"Q115\"                 \n[127] \"Q116_1\"                \"Q116_2\"                \"Q116_3\"               \n[130] \"Q117\"                  \"Q118\"                  \"Q119\"                 \n[133] \"Q120\"                  \"Q253\"                  \"Q258\"                 \n[136] \"Q255\"                  \"Q259\"                  \"Q257\"                 \n[139] \"Q251...153\"            \"Q167\"                  \"Q169\"                 \n[142] \"Q171\"                  \"Q173\"                  \"Q175\"                 \n[145] \"Q177\"                  \"Q179\"                  \"Q181\"                 \n[148] \"Q183\"                  \"Q185\"                  \"Q187\"                 \n[151] \"Q189\"                  \"Q191\"                  \"Q193\"                 \n[154] \"Q195\"                  \"Q197\"                  \"Q199\"                 \n[157] \"Q201\"                  \"Q203\"                  \"Q205\"                 \n[160] \"Q207\"                  \"Q209...175\"            \"Q211...176\"           \n[163] \"Q213\"                  \"Q215...178\"            \"Q217...179\"           \n[166] \"Q219...180\"            \"Q221...181\"            \"Q223...182\"           \n[169] \"Q225\"                  \"Q227\"                  \"Q229\"                 \n[172] \"Q231...186\"            \"Q233...187\"            \"Q235...188\"           \n[175] \"Q237...189\"            \"Q239...190\"            \"Q241...191\"           \n[178] \"Q243\"                  \"Q245...193\"            \"Q247...194\"           \n[181] \"Q249...195\"            \"Q58\"                   \"Q60_1\"                \n[184] \"Q62\"                   \"Q64_1\"                 \"Q64_2\"                \n[187] \"Q64_3\"                 \"Q64_4\"                 \"Q66\"                  \n[190] \"Q68\"                   \"Q72\"                   \"Q74\"                  \n[193] \"Q76...207\"             \"Q78...208\"             \"Q80...209\"            \n[196] \"SurveyOrder\"          \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n##Change the column names ----\ncolnames(expertise6_clean) <- c('ip','duration', 'finished', 'stih_food','r_ih_food',\n                        'stih_sports','r_ih_sports','stih_school','r_ih_school',\n                        'stih_architect','r_ih_architect','stih_product','r_ih_product',\n                        'stih_langu','r_ih_langu','stih_network','r_ih_network',\n                        'stih_anthro','r_ih_anthro',\n                        'know_food','know_sports','know_school','know_architect',\n                        'know_product','know_langu','know_network','know_anthro',\n                        'course_food','high_food', 'colle_food', 'grad_food',\n                        'book_food','numbook_food','article_food','numarticle_food',\n                        'inarti_food','numinarti_food', 'docu_food', 'numdocu_food',\n                        'radio_food','numradio_food',\n                        'course_sports','high_sports', 'colle_sports', 'grad_sports',\n                        'book_sports','numbook_sports','article_sports','numarticle_sports',\n                        'inarti_sports','numinarti_sports', 'docu_sports', 'numdocu_sports',\n                        'radio_sports','numradio_sports',\n                        'course_school','high_school', 'colle_school', 'grad_school',\n                        'book_school','numbook_school','article_school','numarticle_school',\n                        'inarti_school','numinarti_school', 'docu_school', 'numdocu_school',\n                        'radio_school','numradio_school',\n                        'course_architect','high_architect', 'colle_architect','grad_architect',\n                        'book_architect','numbook_architect','article_architect',\n                        'numarticle_architect',\n                        'inarti_architect','numinarti_architect', \n                        'docu_architect', 'numdocu_architect',\n                        'radio_architect','numradio_architect',\n                        'course_product','high_product', 'colle_product', 'grad_product',\n                        'book_product','numbook_product','article_product',\n                        'numarticle_product',\n                        'inarti_product','numinarti_product', 'docu_product', 'numdocu_product',\n                        'radio_product','numradio_product',\n                        'course_langu','high_langu', 'colle_langu', 'grad_langu',\n                        'book_langu','numbook_langu','article_langu','numarticle_langu',\n                        'inarti_langu','numinarti_langu', 'docu_langu', 'numdocu_langu',\n                        'radio_langu','numradio_langu',\n                        'course_network','high_network', 'colle_network', 'grad_network',\n                        'book_network','numbook_network','article_network','numarticle_network',\n                        'inarti_network','numinarti_network', 'docu_network', 'numdocu_network',\n                        'radio_network','numradio_network',\n                        'course_anthro','high_anthro', 'colle_anthro', 'grad_anthro',\n                        'book_anthro','numbook_anthro','article_anthro','numarticle_anthro',\n                        'inarti_anthro','numinarti_anthro', 'docu_anthro', 'numdocu_anthro',\n                        'radio_anthro','numradio_anthro',\n                        'needforclo1','needforclo2','needforclo3','needforclo4','needforclo5',\n                        'needforclo6','needforclo7','needforclo8','needforclo9','needforclo10',\n                        'needforclo11','needforclo12','needforclo13','needforclo14',\n                        'needforclo15','needforclo16','needforclo17','needforclo18',\n                        'needforclo19','needforclo20','needforclo21','needforclo22',\n                        'needforclo23','needforclo24','needforclo25','needforclo26',\n                        'needforclo27','needforclo28','needforclo29','needforclo30',\n                        'needforclo31','needforclo32','needforclo33','needforclo34',\n                        'needforclo35','needforclo36','needforclo37','needforclo38',\n                        'needforclo39','needforclo40','needforclo41','needforclo42',\n                        'sex','birthdate','education','income','religion',\n                        'identity','age','political_atti','english_level','proceure_confu',\n                        'whatwestudied','moretothisstudy','additional_thoughts','attention', \n                        'surveyorder')\n```\n:::\n\n\n### Questionnaire Items\n\nThe code below shows the survey items:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#selects all columns except the ones specified\nrow_values <- expertise6_clean %>%\n  select(-duration,-finished, -ip, -surveyorder)%>%\n  #selects only the first row\n  filter(row_number() == 1)\n\n#Items in the questionnaire ----\n\n#unlist the row_values\nrow_values <- unlist(row_values)\n\nmy_list <- map(row_values, ~paste0(.))\n\nlibrary(stringr)\nmy_list <- str_replace(my_list, \"(?<! )\\\\n(?! )\", \"\")\nmy_list <- str_replace(my_list, \"[^\\\\s]*\\\\\\\\n[^\\\\s]*\", \"\")\nlist_string <- paste0(\"* \", paste(my_list, collapse = \"\\n* \"))\n\n##Show the survey items ----\ncat(list_string)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n* There are good reasons why drive-thrus are typically associated with fast-food restaurants. There are things about drive-thrus that make them less suited for fancy or upscale restaurants.\n* Even though we don’t typically see drive-thrus at fancy restaurants, there is no real reason why fancy restaurants don’t have drive-thrus. This practice could have easily been different.\n* It seems natural that we sing the national anthem before sporting events, rather than after. The national anthem just fits better at the beginning of a sports game.\n* When you really think about, singing the national anthem before sporting events is just a convention. If history had unfolded differently, it’s possible that we would be singing it after the events, or maybe even not at all.\n* It seems right that students receive letter grades in school (A, B, C, …) to measure their performance. Another evaluative process would likely not work as well.\n* It’s likely that evaluating students’ performance with letter grades (A, B, C, …) is not the best practice. Another evaluative process may be more effective than letter grading.\n* It probably works best for homes to be painted with neutral or muted colors (e.g., white, gray). Bright or neon-colored homes (e.g., red, purple) would not be practical.\n* Although homes are usually painted with neutral or muted colors (e.g., white, gray), this could have been done differently.  It’s quite possible that homes could have been painted with brighter colors (e.g., red, purple).\n* It seems ideal that forks usually have three or four prongs, rather than more prongs (for example, six or seven). Three or four prongs on forks is probably the best design.\n* There’s no good reason why forks only have three or four prongs. Forks with more prongs (for example, six or seven) would work just as well; in principle, forks could have been designed that way too.\n* It seems natural that the letter “s” at the end of a word is used to indicate plurality (as in \"cats\" or \"trees\"). Another way of signaling plurality would not work as well.\n* Had historical events unfolded differently, it’s possible that the English language would indicate plurality in a different way than using the letter “s” (as in \"cats\" or \"trees\"). There’s nothing inherently special about the letter “s” for this purpose.\n* There are good reasons why we use digits, rather than letters, to call people on the phone.  Using digits seems like the optimal way to make phone calls.\n* The only reason why we use digits, rather than letters, to call people is historical happenstance.  Phones calls could have just as easily been made with a variety of symbols other than digits (for example, letters).\n* It seems natural that people wear black to funerals. There is something about the color black that indicates mourning.\n* When you think about it, colors other than black could have just as easily become associated with funerals (for example, white). Had history taken a different turn, another color may now signal mourning and sadness.\n* How much do you know about the food industry and restaurant business? - Please use the slider to select your answer choice.\n* How much do you know about sports and sports management? - Please use the slider to select your answer choice.\n* How much do you know about school and education systems? - Please use the slider to select your answer choice.\n* How much do you know about architecture and home design? - Please use the slider to select your answer choice.\n* How much do you know about product design and usability? - Please use the slider to select your answer choice.\n* How much do you know about language and linguistics? - Please use the slider to select your answer choice.\n* How much do you know about telecommunication and network systems? - Please use the slider to select your answer choice.\n* How much do you know about anthropology and funeral rites? - Please use the slider to select your answer choice.\n* Have you ever taken a class that discussed the food industry and restaurant business?\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - High School\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - College\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - Graduate School\n* Have you ever read any books on the food industry and restaurant business?\n* If you answered \"yes\" above, please approximate the number of books you have read on this topic:\n* Have you ever read any magazine, newspaper, or academic articles on the food industry and restaurant business?\n* If you answered \"yes\" above, please approximate the number of articles you have read on this topic:\n* Have you ever read any articles while browsing the internet  (e.g., on Reddit, Pinterest, Buzzfeed) on the food industry and restaurant business?\n* If you answered \"yes\" above, please approximate the number of articles on the internet you have read on this topic:\n* Have you ever seen any documentaries (e,g., on the Discovery channel) on the food industry and restaurant business?\n* If you answered \"yes\" above, please approximate the number of documentaries you have seen on this topic:\n* Have you ever heard any radio shows or podcasts (e.g., NPR) on the food industry and restaurant business?\n* If you answered \"yes\" above, please approximate the number of radio shows or podcasts you have heard on this topic:\n* Have you ever taken a class that discussed sports and sports management?\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - High School\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - College\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - Graduate School\n* Have you ever read any books on sports and sports management?\n* If you answered \"yes\" above, please approximate the number of books you have read on this topic:\n* Have you ever read any magazine, newspaper, or academic articles on sports and sports management?\n* If you answered \"yes\" above, please approximate the number of articles you have read on this topic:\n* Have you ever read any articles while browsing the internet  (e.g., on Reddit, Pinterest, Buzzfeed) on sports and sports management?\n* If you answered \"yes\" above, please approximate the number of articles on the internet you have read on this topic:\n* Have you ever watched any documentaries (e,g., on the Discovery channel) on sports and sports management?\n* If you answered \"yes\" above, please approximate the number of documentaries you have seen on this topic:\n* Have you ever heard any radio shows or podcasts (e.g., NPR) on sports and sports management?\n* If you answered \"yes\" above, please approximate the number of radio shows or podcasts you have heard on this topic:\n* Have you ever taken a class that discussed school and education systems?\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - High School\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - College\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - Graduate School\n* Have you ever read any books on school and education systems?\n* If you answered \"yes\" above, please approximate the number of books you have read on this topic:\n* Have you ever read any magazine, newspaper, or academic articles on school and education systems?\n* If you answered \"yes\" above, please approximate the number of articles you have read on this topic:\n* Have you ever read any articles while browsing the internet  (e.g., on Reddit, Pinterest, Buzzfeed) on school and education systems?\n* If you answered \"yes\" above, please approximate the number of articles on the internet you have read on this topic:\n* Have you ever watched any documentaries (e,g., on the Discovery channel) on school and education systems?\n* If you answered \"yes\" above, please approximate the number of documentaries you have seen on this topic:\n* Have you ever heard any radio shows or podcasts (e.g., NPR) have you heard on school and education systems?\n* If you answered \"yes\" above, please approximate the number of radio shows or podcasts you have seen on this topic:\n* Have you ever taken a class that discussed architecture and home design?\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - High School\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - College\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - Graduate School\n* Have you ever read any books on architecture and home design?\n* If you answered \"yes\" above, please approximate the number of books you have read on this topic:\n* Have you ever read any magazine, newspaper, or academic articles on architecture and home design?\n* If you answered \"yes\" above, please approximate the number of articles you have read on this topic:\n* Have you ever read any articles while browsing the internet  (e.g., on Reddit, Pinterest, Buzzfeed) on architecture and home design?\n* If you answered \"yes\" above, please approximate the number of articles on the internet you have read on this topic:\n* Have you ever seen any documentaries (e,g., on the Discovery channel) on architecture and home design?\n* If you answered \"yes\" above, please approximate the number of documentaries you have seen on this topic:\n* Have you ever heard any radio shows or podcasts (e.g., NPR) on architecture and home design?\n* If you answered \"yes\" above, please approximate the number of radio shows or podcasts you have heard on this topic:\n* Have you ever taken a class that discussed product design and usability?\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - High School\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - College\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - Graduate School\n* Have you ever read any books on product design and usability?\n* If you answered \"yes\" above, please approximate the number of books you have read on this topic:\n* Have you ever read any magazine, newspaper, or academic articles on product design and usabilityt?\n* If you answered \"yes\" above, please approximate the number of articles you have read on this topic:\n* Have you ever read any articles while browsing the internet  (e.g., on Reddit, Pinterest, Buzzfeed) on product design and usability?\n* If you answered \"yes\" above, please approximate the number of articles on the internet you have read on this topic:\n* Have you ever seen any documentaries (e,g., on the Discovery channel) on product design and usability?\n* If you answered \"yes\" above, please approximate the number of documentaries you have seen on this topic:\n* Have you ever heard any radio shows or podcasts (e.g., on Reddit, Pinterest, Buzzfeed) on product design and usability?\n* If you answered \"yes\" above, please approximate the number of radio shows or podcasts you have heard on this topic:\n* Have you ever taken a class that discussed language and linguistics?\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - High School\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - College\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - Graduate School\n* Have you ever read any books on language and linguistics?\n* If you answered \"yes\" above, please approximate the number of books you have read on this topic:\n* Have you ever read any magazine, newspaper, or academic articles on language and linguistics?\n* If you answered \"yes\" above, please approximate the number of articles you have read on this topic:\n* Have you ever read any articles while browsing the internet  (e.g., on Reddit, Pinterest, Buzzfeed) on language and linguistics?\n* If you answered \"yes\" above, please approximate the number of articles on the internet you have read on this topic:\n* Have you ever seen any documentaries (e,g., on the Discovery channel) on language and linguistics?\n* If you answered \"yes\" above, please approximate the number of documentaries you have seen on this topic:\n* Have you ever heard any radio shows or podcasts (e.g., NPR) on language and linguistics?\n* If you answered \"yes\" above, please approximate the number of radio shows or podcasts you have heard on this topic:\n* Have you ever taken a class that discussed telecommunication and network systems?\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - High School\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - College\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - Graduate School\n* Have you ever read any books on telecommunication and network systems?\n* If you answered \"yes\" above, please approximate the number of books you have read on this topic:\n* Have you ever read any magazine, newspaper, or academic articles on telecommunication and network systems?\n* If you answered \"yes\" above, please approximate the number of articles you have read on this topic:\n* Have you ever read any articles while browsing the internet  (e.g., on Reddit, Pinterest, Buzzfeed) on telecommunication and network systems?\n* If you answered \"yes\" above, please approximate the number of articles on the internet you have read on this topic:\n* Have you ever seen any documentaries (e,g., on the Discovery channel) on telecommunication and network systems?\n* If you answered \"yes\" above, please approximate the number of documentaries you have seen on this topic:\n* Have you ever heard any radio shows or podcasts (e.g., NPR) on telecommunication and network systems?\n* If you answered \"yes\" above, please approximate the number of radio shows or podcasts you have heard on this topic:\n* Have you ever taken a class that discussed anthropology and funeral rites?\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - High School\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - College\n* If you answered \"yes\" above, please approximate the number of classes of this sort you took in: - Graduate School\n* Have you ever read any books on anthropology and funeral rites?\n* If you answered \"yes\" above, please approximate the number of books you have read on this topic:\n* Have you ever read any magazine, newspaper, or academic articles on anthropology and funeral rites?\n* If you answered \"yes\" above, please approximate the number of articles you have read on this topic:\n* Have you ever read any articles while browsing the internet  (e.g., on Reddit, Pinterest, Buzzfeed) on anthropology and funeral rites?\n* If you answered \"yes\" above, please approximate the number of articles on the internet you have read on this topic:\n* Have you ever seen any documentaries (e,g., on the Discovery channel) on anthropology and funeral rites?\n* If you answered \"yes\" above, please approximate the number of documentaries you have seen on this topic:\n* Have you ever heard any radio shows or podcasts (e.g., NPR) on anthropology and funeral rites?\n* If you answered \"yes\" above, please approximate the number of radio shows or podcasts you have heard on this topic:\n* I enjoy having a clear and structured mode of life.\n* I like to have a place for everything and everything in its place.\n* I find that establishing a consistent routine enables me to enjoy life more\n* I find that a well ordered life with regular hours suits my temperament.\n* My personal space is usually messy and disorganized.\n* I believe that orderliness and organization are among the most important characteristics of a good student.\n* I think that having clear rules and order at work is essential for success.\n* I think that I would learn best in a class that lacks clearly stated objectives and requirements.\n* I dislike the routine aspects of my work (studies).\n* I hate to change my plans at the last minute.\n* I dislike it when a person's statement could mean many different things.\n* I feel uncomfortable when someone's meaning or intention is unclear to me.\n* I feel uncomfortable when I don't understand the reason why an event occurred in my life.\n* It's annoying to listen to someone who cannot seem to make up his or her mind.\n* When I am confused about an important issue, I feel very upset.\n* I like to know what people are thinking all the time.\n* In most social conflicts, I can easily see which side is right and which is wrong.\n* I'd rather know bad news than stay in a state of uncertainty.\n* I don't like situations that are uncertain.\n* When thinking about a problem, I consider as many different opinions on the issue as possible.\n* When considering most conflict situations, I can usually see how both sides could be right.\n* I always see many possible solutions to problems I face.\n* I do not usually consult many different opinions before forming own view.\n* Even after I've made up my mind about something, I am always eager to consider a different opinion.\n* I prefer interacting with people whose opinions are very different from my own.\n* I dislike questions which could be answered in many different ways.\n* I feel irritated when one person disagrees with what everyone else in a group believes.\n* I like to have friends who are unpredictable.\n* When dining out, I like to go to places where I have been before so that I know what to expect.\n* I don't like to go into a situation without knowing what I can expect from it\n* I think it is fun to change my plans at the last moment.\n* I enjoy the uncertainty of going into a new situation without knowing what might happen.\n* I don't like to be with people who are capable of unexpected actions.\n* I prefer to socialize with familiar friends because I know what to expect from them.\n* I dislike unpredictable situations.\n* When I go shopping, I have difficulty deciding exactly what it is that I want.\n* When faced with a problem I usually see the one best solution very quickly.\n* I tend to put off making important decisions until the last possible moment.\n* I usually make important decisions quickly and confidently.\n* I would describe myself as indecisive.\n* I tend to struggle with most decisions.\n* When trying to solve a problem I often see so many possible options that it's confusing.\n* Are you male or female?\n* Q60 - What is your date of birth? (mm/dd/yyyy)\n* What is the highest level of education you have completed?\n* Q64 - What is your yearly household income?\n* Q64 - What is your religious affiliation?\n* Q64 - What is your racial or ethnic identity?\n* Q64 - What is your age in years?\n* How would you describe your political attitudes? Please select one of the points on the scale below.\n* Please rate your overall ability in the English language:\n* 1. Did you find any aspect of the procedure odd or confusing?\n* 2. What did you think we were studying?\n* 3. Do you think that there may have been more to this study than meets the eye? If so, what do you think this might have been?\n* 4. Do you have any additional thoughts or comments about the study?\n* Thank you for completing this survey! We just have one last question for you. You will not be penalized for your answer to this question. Since you completed the whole survey, you will receive payment no matter what answer you give here.\n\t \n\n\tIt's very important to the quality and scientific aims of our study that participants pay attention (i.e., read the survey carefully, consider the response options, and avoid distractions).\n\n\t\n\n\tWere you paying attention while completing this survey?\n```\n:::\n:::\n\n\n### Exclusion Criterias\n\nData preparation for further analyses\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Attention check and deletion of cases that didn't attend or finish the study ----\nexpertise6_new<-expertise6_clean%>%\n  filter(attention==1&finished==1)\n\n##Exclude the participants that joined outside of US ----\n\n#view(expertise6_new)\nexpertise6_new<-expertise6_new%>%\n  filter(ip!=\"66.42.251.231\")\n\nexpertise6_new <- expertise6_new %>%\n  filter(!(ip %in% c(\"74.219.142.226\", \"24.12.92.17\", \"184.88.52.194\", \n                     \"97.103.220.145\", \"76.250.238.38\")))\n\n##selecting the columns that we want to keep ----\nexpertise6_new<-expertise6_new%>%\n  select(-finished,-birthdate,-proceure_confu,-whatwestudied,-moretothisstudy,-additional_thoughts,-attention, -surveyorder)\n\n#adds a column to the dataframe, with the name \"id\"\nexpertise6_new<-cbind(ID = 1:nrow(expertise6_new), expertise6_new)\n\n# Numeric variables ----\n# Change the data type of the variables to numeric \nexpertise6_new <- expertise6_new %>%\n  mutate_at(vars(stih_food, r_ih_food, stih_sports, r_ih_sports, stih_school, r_ih_school, stih_architect, r_ih_architect, \n                 stih_product, r_ih_product, stih_langu, r_ih_langu, stih_network, r_ih_network, stih_anthro, r_ih_anthro), as.numeric)\n```\n:::\n\n\n## Correlation for ih scores\n\nCheck the correlations between inherence (the variables starting with st) and reverse inherence (the variables starting with r) scores to check whether it's appropriate for averaging\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Correlations between ih scores ----\n\n# Create a list of variable names\nvariables <- c(\"stih_food\", \"r_ih_food\", \"stih_sports\", \"r_ih_sports\", \"stih_school\", \"r_ih_school\", \"stih_architect\", \"r_ih_architect\", \"stih_product\", \"r_ih_product\", \"stih_langu\", \"r_ih_langu\", \"stih_network\", \"r_ih_network\", \"stih_anthro\", \"r_ih_anthro\")\n\n# Initialize an empty data frame to store the correlation coefficients\ncorrelations <- data.frame(variable1 = character(), variable2 = character(), correlation = numeric(), p.value = numeric(), conf.int = character())\n\n# Iterate over the pairs of variables\nfor (i in seq(1, length(variables), 2)) {\n  j <- i + 1\n  \n  # Calculate the Pearson correlation coefficient and test the statistical significance\n  correlation_test <- cor.test(expertise6_new[, variables[i]], expertise6_new[, variables[j]], method = \"pearson\")\n  \n  # Add the correlation coefficient, p-value, and confidence interval to the data frame\n  correlations <- rbind(correlations, data.frame(variable1 = variables[i], variable2 = variables[j], correlation = correlation_test$estimate, p.value = correlation_test$p.value, conf.int = paste(correlation_test$conf.int[1], correlation_test$conf.int[2], sep = \" - \")))}\n\n## View the correlation coefficients and statistical measures ----\ncorrelations\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          variable1      variable2 correlation      p.value\ncor       stih_food      r_ih_food  -0.4463858 8.615861e-11\ncor1    stih_sports    r_ih_sports  -0.5753914 2.560996e-18\ncor2    stih_school    r_ih_school  -0.7267533 8.094747e-33\ncor3 stih_architect r_ih_architect  -0.6411731 1.275191e-23\ncor4   stih_product   r_ih_product  -0.6038601 1.834983e-20\ncor5     stih_langu     r_ih_langu  -0.5635496 1.740797e-17\ncor6   stih_network   r_ih_network  -0.5898848 2.202312e-19\ncor7    stih_anthro    r_ih_anthro  -0.5442449 3.376865e-16\n                                    conf.int\ncor  -0.553035668300546 - -0.325343101426943\ncor1 -0.662979980870432 - -0.472263234194049\ncor2  -0.787333698376605 - -0.65227324221234\ncor3 -0.717624383417027 - -0.549452592494561\ncor4 -0.686743816747194 - -0.505475832894281\ncor5 -0.653042937790108 - -0.458533845459347\ncor6  -0.675100225685499 - -0.48913522712539\ncor7 -0.636777009250974 - -0.436258848944745\n```\n:::\n:::\n\n\n## IH scores calculation\n\nIt seems that each pairs have negative significant correlation, so we can take the average scores to calculate inherence scores\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Average of ih scores  ----\n#It seems that each pairs have negative significant correlation, so we can take the average scores to measure inherence scores\nexpertise6_new <- expertise6_new %>%\n  mutate(ih_food = (stih_food + (10 - r_ih_food))/2,\n         ih_sports = (stih_sports + (10 - r_ih_sports)) / 2,\n         ih_school = (stih_school + (10 - r_ih_school)) / 2,\n         ih_architect = (stih_architect + (10 - r_ih_architect)) / 2,\n         ih_product = (stih_product + (10 - r_ih_product)) / 2,\n         ih_langu = (stih_langu + (10 - r_ih_langu)) / 2,\n         ih_network = (stih_network + (10-r_ih_network)) / 2,\n         ih_anthro = (stih_anthro + (10-r_ih_anthro)) / 2 )\n```\n:::\n\n\n## Need for Cognition scores calculation\n\nCalculate \"Need for cognition\" scale scores\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Need for cognition scale scores  ----\n\n# change the data type of the variables to numeric\nexpertise6_new <- expertise6_new %>%\n  mutate_at(vars( 'needforclo1','needforclo2','needforclo3','needforclo4','needforclo5',\n                        'needforclo6','needforclo7','needforclo8','needforclo9','needforclo10',\n                        'needforclo11','needforclo12','needforclo13','needforclo14',\n                        'needforclo15','needforclo16','needforclo17','needforclo18',\n                        'needforclo19','needforclo20','needforclo21','needforclo22',\n                        'needforclo23','needforclo24','needforclo25','needforclo26',\n                        'needforclo27','needforclo28','needforclo29','needforclo30',\n                        'needforclo31','needforclo32','needforclo33','needforclo34',\n                        'needforclo35','needforclo36','needforclo37','needforclo38',\n                        'needforclo39','needforclo40','needforclo41','needforclo42'), as.numeric)\n\n## Calculate needforclo scores  ----\n#add a new variable called needforclo, which is the sum of all the need for cognition items, the items are weighted according to the scoring key\nexpertise6_new <- expertise6_new %>%\n  group_by(ID)%>%\n  mutate(needforclo=(needforclo1+needforclo2+needforclo3+needforclo4+\n                       (10-needforclo5)+needforclo6+needforclo7+(10-needforclo8)+\n                       (10-needforclo9)+needforclo10+ needforclo11+needforclo12+\n                       needforclo13+needforclo14+needforclo15+needforclo16+\n                       needforclo17+needforclo18+needforclo19+(10-needforclo20)+\n                       (10-needforclo21)+(10-needforclo22)+needforclo23+(10-needforclo24)+\n                       (10-needforclo25)+needforclo26+needforclo27+(10-needforclo28)+\n                       needforclo29+needforclo30+(10-needforclo31)+(10-needforclo32)+\n                       needforclo33+needforclo34+needforclo35+(10-needforclo36)+\n                       needforclo37+(10-needforclo38)+needforclo39+(10-needforclo40)+\n                       (10-needforclo41)+needforclo42)/42)\n```\n:::\n\n\n## Data preparation\n\nPrepare the expertise scores and other scores ready for analyses\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Replace expertise variables' NA values in the expertise columns with 0  ----\nvariables <- c('know_food','know_sports','know_school','know_architect',\n                        'know_product','know_langu','know_network','know_anthro',\n                        'high_food', 'colle_food', 'grad_food',\n                        'numbook_food','numarticle_food',\n                        'numinarti_food', 'numdocu_food',\n                        'numradio_food',\n                        'high_sports', 'colle_sports', 'grad_sports',\n                        'numbook_sports','numarticle_sports',\n                        'numinarti_sports', 'numdocu_sports',\n                        'numradio_sports',\n                        'high_school', 'colle_school', 'grad_school',\n                        'numbook_school','numarticle_school',\n                        'numinarti_school','numdocu_school',\n                        'numradio_school',\n                        'high_architect', 'colle_architect','grad_architect',\n                        'numbook_architect',\n                        'numarticle_architect',\n                        'numinarti_architect', \n                        'numdocu_architect',\n                        'numradio_architect',\n                        'high_product', 'colle_product', 'grad_product',\n                        'numbook_product',\n                        'numarticle_product',\n                       'numinarti_product','numdocu_product',\n                        'numradio_product',\n                        'high_langu', 'colle_langu', 'grad_langu',\n                        'numbook_langu','numarticle_langu',\n                        'numinarti_langu', 'numdocu_langu',\n                        'numradio_langu',\n                        'high_network', 'colle_network', 'grad_network',\n                        'numbook_network','numarticle_network',\n                        'numinarti_network', 'numdocu_network',\n                        'numradio_network',\n                        'high_anthro', 'colle_anthro','grad_anthro',\n                        'numbook_anthro','numarticle_anthro',\n                        'numinarti_anthro', 'numdocu_anthro',\n                        'numradio_anthro')\n\nexpertise6_new <- expertise6_new %>%\n  mutate_at(vars('know_food','know_sports','know_school','know_architect',\n                        'know_product','know_langu','know_network','know_anthro',\n                        'high_food', 'colle_food', 'grad_food',\n                        'numbook_food','numarticle_food',\n                        'numinarti_food', 'numdocu_food',\n                        'numradio_food',\n                        'high_sports', 'colle_sports', 'grad_sports',\n                        'numbook_sports','numarticle_sports',\n                        'numinarti_sports', 'numdocu_sports',\n                        'numradio_sports',\n                        'high_school', 'colle_school', 'grad_school',\n                        'numbook_school','numarticle_school',\n                        'numinarti_school','numdocu_school',\n                        'numradio_school',\n                        'high_architect', 'colle_architect','grad_architect',\n                        'numbook_architect',\n                        'numarticle_architect',\n                        'numinarti_architect', \n                        'numdocu_architect',\n                        'numradio_architect',\n                        'high_product', 'colle_product', 'grad_product',\n                        'numbook_product',\n                        'numarticle_product',\n                       'numinarti_product','numdocu_product',\n                        'numradio_product',\n                        'high_langu', 'colle_langu', 'grad_langu',\n                        'numbook_langu','numarticle_langu',\n                        'numinarti_langu', 'numdocu_langu',\n                        'numradio_langu',\n                        'high_network', 'colle_network', 'grad_network',\n                        'numbook_network','numarticle_network',\n                        'numinarti_network', 'numdocu_network',\n                        'numradio_network',\n                        'high_anthro', 'colle_anthro','grad_anthro',\n                        'numbook_anthro','numarticle_anthro',\n                        'numinarti_anthro', 'numdocu_anthro',\n                        'numradio_anthro'), as.numeric)\n\n\n#expertise6_new[variables] <- lapply(expertise6_new[variables], \n                                    #function(x) ifelse(is.na(x), 0, ifelse(x=='no',0, x)))\n\nexpertise6_new[variables] <- lapply(expertise6_new[variables], function(x) replace(x, is.na(x) | !is.numeric(x) , 0))\n```\n:::\n\n\n## Detect the ID of missing values of expertise scores\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmissing_ids <- unlist(mapply(function(x) expertise6_new$ID[which(is.na(expertise6_new[x]))], c('know_food','know_sports','know_school','know_architect',\n                        'know_product','know_langu','know_network','know_anthro',\n                        'high_food', 'colle_food', 'grad_food',\n                        'numbook_food','numarticle_food',\n                        'numinarti_food', 'numdocu_food',\n                        'numradio_food',\n                        'high_sports', 'colle_sports', 'grad_sports',\n                        'numbook_sports','numarticle_sports',\n                        'numinarti_sports', 'numdocu_sports',\n                        'numradio_sports',\n                        'high_school', 'colle_school', 'grad_school',\n                        'numbook_school','numarticle_school',\n                        'numinarti_school','numdocu_school',\n                        'numradio_school',\n                        'high_architect', 'colle_architect','grad_architect',\n                        'numbook_architect',\n                        'numarticle_architect',\n                        'numinarti_architect', \n                        'numdocu_architect',\n                        'numradio_architect',\n                        'high_product', 'colle_product', 'grad_product',\n                        'numbook_product',\n                        'numarticle_product',\n                       'numinarti_product','numdocu_product',\n                        'numradio_product',\n                        'high_langu', 'colle_langu', 'grad_langu',\n                        'numbook_langu','numarticle_langu',\n                        'numinarti_langu', 'numdocu_langu',\n                        'numradio_langu',\n                        'high_network', 'colle_network', 'grad_network',\n                        'numbook_network','numarticle_network',\n                        'numinarti_network', 'numdocu_network',\n                        'numradio_network',\n                        'high_anthro', 'colle_anthro','grad_anthro',\n                        'numbook_anthro','numarticle_anthro',\n                        'numinarti_anthro', 'numdocu_anthro',\n                        'numradio_anthro')))\n\ncat(ifelse(length(missing_ids[missing_ids > 0]) > 0, paste(\"The following IDs are missing:\",missing_ids[missing_ids != 0]), \"There is no missing IDs.\"),\"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThere is no missing IDs. \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Expertise Ready Df ----\n\n#','high_food','colle_food','grad_food','numbook_food','numarticle_food','numinarti_food','numdocu_food','numradio_food','high_sports','colle_sports','grad_sports','numbook_sports','numarticle_sports','numinarti_sports','numdocu_sports','numradio_sports','high_school','colle_school','grad_school','numbook_school','numarticle_school','numinarti_school','numdocu_school','numradio_school','high_architect','colle_architect','grad_architect','numbook_architect','numarticle_architect','numinarti_architect','numdocu_architect','numradio_architect','high_product','colle_product','grad_product','numbook_product','numarticle_product','numinarti_product','numdocu_product','numradio_product','high_langu','colle_langu','grad_langu','numbook_langu','numarticle_langu','numinarti_langu','numdocu_langu','numradio_langu','high_network','colle_network','grad_network','numbook_network','numarticle_network','numinarti_network','numdocu_network','numradio_network','high_anthro','colle_anthro','grad_anthro','numbook_anthro','numarticle_anthro','numinarti_anthro','numdocu_anthro','numradio_anthro\n\n# change the data type of the variables to numeric\nexpertise6_new<-expertise6_new%>%mutate_at(vars('know_food','know_sports','know_school','know_architect','know_product','know_langu','know_network','know_anthro'),as.numeric)\n```\n:::\n\n\n## Long format\n\nLong format of expertise dataset for factor analysis\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexpertise6_factor <- melt(expertise6_new, id.vars = c(\"ID\",'duration',\"sex\" ,\"education\" ,\"income\",\"religion\",'identity','age','political_atti','english_level','needforclo'), \n                        measure.vars = c(\"ih_food\", \"ih_sports\",\"ih_school\",\"ih_architect\",\n                                         \"ih_product\",\"ih_langu\",\"ih_network\",\"ih_anthro\",\n                       'know_food','know_sports','know_school','know_architect',\n                        'know_product','know_langu','know_network','know_anthro',\n                        'high_food', 'colle_food', 'grad_food',\n                        'numbook_food','numarticle_food',\n                        'numinarti_food', 'numdocu_food',\n                        'numradio_food',\n                        'high_sports', 'colle_sports', 'grad_sports',\n                        'numbook_sports','numarticle_sports',\n                        'numinarti_sports', 'numdocu_sports',\n                        'numradio_sports',\n                        'high_school', 'colle_school', 'grad_school',\n                        'numbook_school','numarticle_school',\n                        'numinarti_school','numdocu_school',\n                        'numradio_school',\n                        'high_architect', 'colle_architect','grad_architect',\n                        'numbook_architect',\n                        'numarticle_architect',\n                        'numinarti_architect', \n                        'numdocu_architect',\n                        'numradio_architect',\n                        'high_product', 'colle_product', 'grad_product',\n                        'numbook_product',\n                        'numarticle_product',\n                       'numinarti_product','numdocu_product',\n                        'numradio_product',\n                        'high_langu', 'colle_langu', 'grad_langu',\n                        'numbook_langu','numarticle_langu',\n                        'numinarti_langu', 'numdocu_langu',\n                        'numradio_langu',\n                        'high_network', 'colle_network', 'grad_network',\n                        'numbook_network','numarticle_network',\n                        'numinarti_network', 'numdocu_network',\n                        'numradio_network',\n                        'high_anthro', 'colle_anthro','grad_anthro',\n                        'numbook_anthro','numarticle_anthro',\n                        'numinarti_anthro', 'numdocu_anthro',\n                        'numradio_anthro'),\n                        sep = \"_\", variable.name = \"Category\", value.name = \"Score\")\n\n# Split the Category column into two columns based on the underscore separator\nexpertise6_factor <- expertise6_factor %>% separate(Category, into = c(\"Category\", \"Score_Type\"), sep = \"_\")\n\n## spread the data from long to wide format  ----\nexpertise6_fact <- expertise6_factor %>% spread(Category, Score)\n\n# change the score type to a factor\nexpertise6_fact$Score_Type<-as.factor(expertise6_fact$Score_Type)\n\n# convert the column ih to numeric\nexpertise6_fact$ih<-as.numeric(expertise6_fact$ih)\n```\n:::\n\n\n###Create another data frame to winsorize expertise variables before factor analysis\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexpertise6_wins_fact<-expertise6_fact\n```\n:::\n\n\n## Winsorize the variables at 1%\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# winsorize the variables (at 1%)\nexpertise6_fact1 <- expertise6_fact%>%\n  mutate(numarticle=Winsorize(numarticle, probs = c(0,0.99)), \n         numbook=Winsorize(numbook, probs = c(0,0.99)),\n         high=Winsorize(high, probs = c(0,0.99)),\n         colle=Winsorize(colle, probs = c(0,0.99)),\n         grad=Winsorize(grad,na.rm=TRUE, probs = c(0,0.99)),\n         numdocu=Winsorize(numdocu,na.rm=TRUE, probs = c(0,0.99)),\n         numinarti=Winsorize(numinarti,na.rm=TRUE, probs = c(0,0.99)),\n         numradio=Winsorize(numradio,na.rm=TRUE, probs = c(0,0.99)))\n```\n:::\n\n\n## Factor analysis\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Factor analysis for expertise variables with raw scores ----\n\n# Import packages \n\nlibrary(psych) #PCA/EFA analysis\nlibrary(REdaS) #Produces KMO and Bartletts test\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: grid\n```\n:::\n\n```{.r .cell-code}\nlibrary(GPArotation)\n\n\n# Create a new dataframe that include only related variables\nfactor_exp<-expertise6_fact1%>%\n  select(colle, grad, high, numarticle, numbook, numdocu, numinarti, numradio)\n\n# Check missing values\napply(is.na(factor_exp), 2, sum)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     colle       grad       high numarticle    numbook    numdocu  numinarti \n         0          0          0          0          0          0          0 \n  numradio \n         0 \n```\n:::\n:::\n\n\nFactor analysis for expertise variables with raw scores\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Since grad classes for TV category is missing (nobody takes any class in the sample), listwise deletion is applied here.\nbart_spher(factor_exp, use = \"complete.obs\") ###### produces Bartletts test of spherecity \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\tBartlett's Test of Sphericity\n\nCall: bart_spher(x = factor_exp, use = \"complete.obs\")\n\n     X2 = 3334.573\n     df = 28\np-value < 2.22e-16\n```\n:::\n\n```{.r .cell-code}\nKMO(factor_exp)       ###### Kaiser-Meyer-Olkin measure, which is above .5.\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nKaiser-Meyer-Olkin factor adequacy\nCall: KMO(r = factor_exp)\nOverall MSA =  0.79\nMSA for each item = \n     colle       grad       high numarticle    numbook    numdocu  numinarti \n      0.77       0.83       0.70       0.79       0.81       0.83       0.77 \n  numradio \n      0.84 \n```\n:::\n\n```{.r .cell-code}\n#Check eigenvalues\n\nfa.parallel(factor_exp)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nParallel analysis suggests that the number of factors =  3  and the number of components =  2 \n```\n:::\n:::\n\n\n### Factor analysis\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# So we can reduce it to 2 factors\nfa(factor_exp, nfactors = 2, rotate =  \"oblimin\" )  \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFactor Analysis using method =  minres\nCall: fa(r = factor_exp, nfactors = 2, rotate = \"oblimin\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n             MR1   MR2   h2   u2 com\ncolle      -0.04  0.83 0.66 0.34 1.0\ngrad        0.09  0.35 0.16 0.84 1.1\nhigh       -0.03  0.40 0.15 0.85 1.0\nnumarticle  0.77  0.06 0.63 0.37 1.0\nnumbook     0.44  0.47 0.62 0.38 2.0\nnumdocu     0.58 -0.06 0.30 0.70 1.0\nnuminarti   0.89 -0.04 0.76 0.24 1.0\nnumradio    0.30  0.04 0.10 0.90 1.0\n\n                       MR1  MR2\nSS loadings           2.10 1.30\nProportion Var        0.26 0.16\nCumulative Var        0.26 0.42\nProportion Explained  0.62 0.38\nCumulative Proportion 0.62 1.00\n\n With factor correlations of \n     MR1  MR2\nMR1 1.00 0.49\nMR2 0.49 1.00\n\nMean item complexity =  1.2\nTest of the hypothesis that 2 factors are sufficient.\n\nThe degrees of freedom for the null model are  28  and the objective function was  2.18 with Chi Square of  3334.57\nThe degrees of freedom for the model are 13  and the objective function was  0.08 \n\nThe root mean square of the residuals (RMSR) is  0.03 \nThe df corrected root mean square of the residuals is  0.05 \n\nThe harmonic number of observations is  1536 with the empirical chi square  90.74  with prob <  1e-13 \nThe total number of observations was  1536  with Likelihood Chi Square =  123.66  with prob <  3.8e-20 \n\nTucker Lewis Index of factoring reliability =  0.928\nRMSEA index =  0.074  and the 90 % confidence intervals are  0.063 0.087\nBIC =  28.27\nFit based upon off diagonal values = 0.99\nMeasures of factor score adequacy             \n                                                   MR1  MR2\nCorrelation of (regression) scores with factors   0.93 0.88\nMultiple R square of scores with factors          0.87 0.77\nMinimum correlation of possible factor scores     0.73 0.54\n```\n:::\n\n```{.r .cell-code}\n# Figure for the analysis\n\nM1<-fa(factor_exp, nfactors = 2, rotate =  \"oblimin\" ) ##save the analysis as the object m1\nfa.diagram(M1,main=\"Expertise Variables\")  \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\nSo here we have two factors and we can investigate them as classes and media-literature part, let's extract values for now, but creating these categories with averaging related variables may be a better way for the sake of conceptual understanding.\n\n### Extracting factor values\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfactor_exp_score <- factanal(factor_exp, factors=2, scores=\"regression\", rotation = \"oblimin\", na.rm=TRUE)\n\nhead(factor_exp_score$scores)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         Factor1    Factor2\n[1,]  0.02119905 -0.4233947\n[2,]  0.22119338 -0.4679913\n[3,] -0.14582577 -0.3159511\n[4,]  0.17926929 -0.5193750\n[5,] -0.26945985 -0.2581370\n[6,]  0.82731651 -0.4607261\n```\n:::\n\n```{.r .cell-code}\nfactor_exp_comb <- bind_cols(factor_exp, data.frame(factor_exp_score$scores))\n\nfactor_exp_comb$class<-factor_exp_comb$Factor1\n\nfactor_exp_comb$media_grad<-factor_exp_comb$Factor2\n```\n:::\n\n\n### Histogram and descriptives for factor scores\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndescriptives(dat=factor_exp_comb, vars(Factor1, Factor2),\n             sd=T)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n DESCRIPTIVES\n\n Descriptives                                            \n ─────────────────────────────────────────────────────── \n                         Factor1          Factor2        \n ─────────────────────────────────────────────────────── \n   N                              1536            1536   \n   Missing                           0               0   \n   Mean                  -5.305905e-16    8.812395e-16   \n   Median                   -0.2694598      -0.2581370   \n   Standard deviation        0.9977506       0.9220806   \n   Minimum                   -2.856278       -3.778751   \n   Maximum                    7.978103        6.476149   \n ─────────────────────────────────────────────────────── \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(factor_exp_comb$Factor1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(factor_exp_comb$Factor2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\n## Winsorize the variables at 1%\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# winsorize the variables (at 1%)\nexpertise6_ready <- expertise6_fact%>%\n  mutate(numarticle_wins_1=Winsorize(numarticle, probs = c(0,0.99)), \n         numbook_wins_1=Winsorize(numbook, probs = c(0,0.99)),\n         high_wins_1=Winsorize(high, probs = c(0,0.99)),\n         colle_wins_1=Winsorize(colle, probs = c(0,0.99)),\n         grad_wins_1=Winsorize(grad,na.rm=TRUE, probs = c(0,0.99)),\n         docu_wins_1=Winsorize(numdocu,na.rm=TRUE, probs = c(0,0.99)),\n         inarti_wins_1=Winsorize(numinarti,na.rm=TRUE, probs = c(0,0.99)),\n         radio_wins_1=Winsorize(numradio,na.rm=TRUE, probs = c(0,0.99)))\n\n#check descriptives\ndescriptives(dat=expertise6_ready, vars(docu_wins_1,inarti_wins_1, radio_wins_1 ), median=F, n=F, missing=T, sd=T, skew =T)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n DESCRIPTIVES\n\n Descriptives                                                            \n ─────────────────────────────────────────────────────────────────────── \n                          docu_wins_1    inarti_wins_1    radio_wins_1   \n ─────────────────────────────────────────────────────────────────────── \n   Missing                          0                0               0   \n   Mean                      2.194010         9.023438       0.7988281   \n   Standard deviation        5.699259         26.33146        2.767993   \n   Minimum                   0.000000         0.000000        0.000000   \n   Maximum                   38.25000         191.2500        20.00000   \n   Skewness                  4.286225         4.959908        5.015617   \n   Std. error skewness     0.06243908       0.06243908      0.06243908   \n ─────────────────────────────────────────────────────────────────────── \n```\n:::\n\n```{.r .cell-code}\ndescriptives(dat=expertise6_ready, vars(high_wins_1, colle_wins_1), median=F, n=F, missing=T, sd=T)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n DESCRIPTIVES\n\n Descriptives                                          \n ───────────────────────────────────────────────────── \n                         high_wins_1    colle_wins_1   \n ───────────────────────────────────────────────────── \n   Missing                         0               0   \n   Mean                    0.3147135       0.5794271   \n   Standard deviation      0.8859480        1.522536   \n   Minimum                  0.000000        0.000000   \n   Maximum                  4.650000        10.00000   \n ───────────────────────────────────────────────────── \n```\n:::\n\n```{.r .cell-code}\ndescriptives(dat=expertise6_ready, vars(numarticle_wins_1, numbook_wins_1, grad_wins_1), median=F, n=F, missing=T, sd=T)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n DESCRIPTIVES\n\n Descriptives                                                                 \n ──────────────────────────────────────────────────────────────────────────── \n                         numarticle_wins_1    numbook_wins_1    grad_wins_1   \n ──────────────────────────────────────────────────────────────────────────── \n   Missing                               0                 0              0   \n   Mean                           5.916667          1.592448     0.07291667   \n   Standard deviation             16.05380          4.371575      0.4732818   \n   Minimum                        0.000000          0.000000       0.000000   \n   Maximum                        100.0000          30.00000       4.000000   \n ──────────────────────────────────────────────────────────────────────────── \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Standardize the Selected variables  ----\nvars_to_standardize <- c(\"know\", \"docu_wins_1\",\"inarti_wins_1\", \"radio_wins_1\",\"high_wins_1\", \"colle_wins_1\",\n                         \"numarticle_wins_1\", \"numbook_wins_1\", \"grad_wins_1\")\n\n\n\nexpertise6_ready<-expertise6_ready %>% \n  group_by(Score_Type) %>% \n  mutate_at(vars(vars_to_standardize), scale)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsum(is.na(expertise6_ready$grad_wins_1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 384\n```\n:::\n:::\n\n\n## Take the means of winsorized variables to create our new categories: media and class\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexpertise6_ready <- expertise6_ready %>% replace_na(list(colle_wins_1 = NA, high_wins_1 = NA, grad_wins_1 = NA, docu_wins_1= NA,inarti_wins_1 = NA, radio_wins_1 = NA, numarticle_wins_1 = NA, numbook_wins_1 = NA))\n\n\nexpertise6_ready <- expertise6_ready %>% mutate(class_wins1 = mapply(function(x, y, z) {\n  ifelse(is.na(x), (y + z)/2, \n         ifelse(is.na(y), (x + z)/2, \n                ifelse(is.na(z), (x + y)/2, (x + y + z)/3)))}, \n  grad_wins_1, colle_wins_1, high_wins_1))\n\nexpertise6_ready <- expertise6_ready %>% mutate(media_wins1 = mapply(function(x, y, z, t, r) {\n  ifelse(is.na(x), (y + z + t + r)/4, \n         ifelse(is.na(y), (x + z + t + r)/4,\n                ifelse(is.na(z), (x + y + t + r)/4, \n                       ifelse(is.na(t),(x + y + z + r)/4, \n                              ifelse(is.na(r),(x + y + z + t)/4,\n                                     (x + y + z + t + r)/5)))))}, \n  docu_wins_1,inarti_wins_1, radio_wins_1, numbook_wins_1, numarticle_wins_1))\n```\n:::\n\n\n## Center the variables\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexpertise6_ready$know_cent <- scale(expertise6_ready$know, center = TRUE, scale = FALSE)\n\nexpertise6_ready$needforclo_cent <- scale(expertise6_ready$needforclo, center = TRUE, scale = FALSE)\n\nexpertise6_ready$class_wins1_cent <- scale(expertise6_ready$class_wins1, center = TRUE, scale = FALSE)\n\nexpertise6_ready$media_wins1_cent <- scale(expertise6_ready$media_wins1, center = TRUE, scale = FALSE)\n```\n:::\n\n\nDescriptives\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndescribe(expertise6_ready)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                  vars    n  mean    sd median trimmed   mad   min     max\nID                   1 1536 96.50 55.44  96.50   96.50 71.16  1.00  192.00\nduration*            2 1536 92.35 52.86  92.50   92.45 68.94  1.00  182.00\nsex*                 3 1536  1.64  0.48   2.00    1.68  0.00  1.00    2.00\neducation*           4 1536  3.62  0.94   4.00    3.60  1.48  1.00    6.00\nincome*              5 1512 49.86 27.76  51.00   49.99 34.10  1.00  101.00\nreligion*            6 1536 22.26 12.86  17.00   21.98 16.31  1.00   51.00\nidentity*            7 1528 21.27 10.66  29.00   22.29  1.48  1.00   36.00\nage*                 8 1528 19.20 12.11  17.00   18.35 13.34  1.00   46.00\npolitical_atti*      9 1536  4.21  2.16   4.00    4.12  2.97  1.00    9.00\nenglish_level*      10 1536  1.01  0.10   1.00    1.00  0.00  1.00    2.00\nneedforclo          11 1528  5.63  0.69   5.69    5.63  0.71  3.79    7.36\nScore_Type*         12 1536  4.50  2.29   4.50    4.50  2.97  1.00    8.00\ncolle               13 1536  2.21 52.06   0.00    0.21  0.00  0.00 2000.00\ngrad                14 1536  0.94 26.07   0.00    0.00  0.00  0.00 1000.00\nhigh                15 1536  0.66 10.56   0.00    0.06  0.00  0.00  400.00\nih                  16 1536  5.11  2.14   5.00    5.16  2.97  1.00    9.00\nknow                17 1536  0.00  1.00  -0.17   -0.06  1.10 -2.20    3.05\nnumarticle          18 1536  7.38 36.32   0.00    2.09  0.00  0.00 1000.00\nnumbook             19 1536  2.07 12.09   0.00    0.55  0.00  0.00  400.00\nnumdocu             20 1536  3.70 36.82   0.00    0.79  0.00  0.00 1000.00\nnuminarti           21 1536 13.56 82.66   0.00    2.79  0.00  0.00 2000.00\nnumradio            22 1536  1.21  7.13   0.00    0.10  0.00  0.00  100.00\nnumarticle_wins_1   23 1536  0.00  1.00  -0.31   -0.23  0.13 -0.49    9.84\nnumbook_wins_1      24 1536  0.00  1.00  -0.29   -0.22  0.19 -0.55   11.10\nhigh_wins_1         25 1536  0.00  1.00  -0.27   -0.24  0.08 -0.82    7.96\ncolle_wins_1        26 1536  0.00  1.00  -0.26   -0.22  0.19 -0.66    9.84\ngrad_wins_1         27 1152  0.00  1.00  -0.12   -0.15  0.07 -0.31   13.78\ndocu_wins_1         28 1536  0.00  1.00  -0.34   -0.23  0.17 -0.59   10.47\ninarti_wins_1       29 1536  0.00  1.00  -0.29   -0.22  0.13 -0.45   10.74\nradio_wins_1        30 1536  0.00  1.00  -0.25   -0.23  0.06 -0.43   10.69\nclass_wins1         31 1536  0.00  0.72  -0.25   -0.16  0.08 -0.58    7.38\nmedia_wins1         32 1536  0.00  0.73  -0.26   -0.17  0.16 -0.48    6.57\nknow_cent           33 1536  0.00  1.00  -0.17   -0.06  1.10 -2.20    3.05\nneedforclo_cent     34 1528  0.00  0.69   0.06    0.00  0.71 -1.84    1.73\nclass_wins1_cent    35 1536  0.00  0.72  -0.25   -0.16  0.08 -0.58    7.38\nmedia_wins1_cent    36 1536  0.00  0.73  -0.26   -0.17  0.16 -0.48    6.57\n                    range  skew kurtosis   se\nID                 191.00  0.00    -1.20 1.41\nduration*          181.00 -0.01    -1.23 1.35\nsex*                 1.00 -0.59    -1.66 0.01\neducation*           5.00  0.33     0.17 0.02\nincome*            100.00 -0.01    -1.06 0.71\nreligion*           50.00  0.19    -1.09 0.33\nidentity*           35.00 -0.59    -1.40 0.27\nage*                45.00  0.51    -0.83 0.31\npolitical_atti*      8.00  0.34    -0.76 0.06\nenglish_level*       1.00  9.63    90.89 0.00\nneedforclo           3.57 -0.05    -0.24 0.02\nScore_Type*          7.00  0.00    -1.24 0.06\ncolle             2000.00 37.08  1410.78 1.33\ngrad              1000.00 36.95  1404.30 0.67\nhigh               400.00 35.76  1333.98 0.27\nih                   8.00 -0.12    -0.85 0.05\nknow                 5.25  0.46    -0.47 0.03\nnumarticle        1000.00 17.54   410.05 0.93\nnumbook            400.00 24.68   771.98 0.31\nnumdocu           1000.00 25.86   695.08 0.94\nnuminarti         2000.00 15.21   284.41 2.11\nnumradio           100.00 11.51   148.11 0.18\nnumarticle_wins_1   10.34  4.72    26.00 0.03\nnumbook_wins_1      11.66  4.80    28.73 0.03\nhigh_wins_1          8.78  3.87    18.44 0.03\ncolle_wins_1        10.50  4.42    26.23 0.03\ngrad_wins_1         14.09  7.79    71.79 0.03\ndocu_wins_1         11.07  4.15    22.30 0.03\ninarti_wins_1       11.18  5.44    35.99 0.03\nradio_wins_1        11.13  5.36    35.25 0.03\nclass_wins1          7.95  4.16    24.16 0.02\nmedia_wins1          7.06  4.05    20.95 0.02\nknow_cent            5.25  0.46    -0.47 0.03\nneedforclo_cent      3.57 -0.05    -0.24 0.02\nclass_wins1_cent     7.95  4.16    24.16 0.02\nmedia_wins1_cent     7.06  4.05    20.95 0.02\n```\n:::\n:::\n\n\n\n##Shapiro-Wilk values\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndescriptives(expertise6_ready, vars = vars(class_wins1, media_wins1, know, ih),n=FALSE, missing= FALSE, median=FALSE, sw = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n DESCRIPTIVES\n\n Descriptives                                                                         \n ──────────────────────────────────────────────────────────────────────────────────── \n                         class_wins1     media_wins1     know            ih           \n ──────────────────────────────────────────────────────────────────────────────────── \n   Mean                  2.746646e-18    2.432227e-17    5.261995e-17      5.112305   \n   Standard deviation       0.7210980       0.7266705       0.9977173      2.142995   \n   Minimum                 -0.5754818      -0.4829025       -2.197199      1.000000   \n   Maximum                   7.378558        6.574853        3.050287      9.000000   \n   Shapiro-Wilk W           0.5389428       0.5381616       0.9721739     0.9707327   \n   Shapiro-Wilk p          < .0000001      < .0000001      < .0000001    < .0000001   \n ──────────────────────────────────────────────────────────────────────────────────── \n```\n:::\n:::\n\n\n\n## HLM Models\n\n### Model objective expertise with grad courses, books and magazines\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Model objective expertise ----\n\nModel_obj<-lmer(ih ~class_wins1_cent*needforclo_cent+class_wins1_cent+needforclo_cent+\n                (1|Score_Type)+(1|ID), data=expertise6_ready)\nsummary(Model_obj)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: ih ~ class_wins1_cent * needforclo_cent + class_wins1_cent +  \n    needforclo_cent + (1 | Score_Type) + (1 | ID)\n   Data: expertise6_ready\n\nREML criterion at convergence: 6320.8\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.93455 -0.65040  0.02653  0.66826  2.56442 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.7734   0.8794  \n Score_Type (Intercept) 0.7745   0.8801  \n Residual               3.1236   1.7674  \nNumber of obs: 1528, groups:  ID, 191; Score_Type, 8\n\nFixed effects:\n                                   Estimate Std. Error         df t value\n(Intercept)                         5.10303    0.32079    7.58400  15.907\nclass_wins1_cent                   -0.07206    0.06840 1503.58449  -1.053\nneedforclo_cent                     0.18572    0.11261  189.81958   1.649\nclass_wins1_cent:needforclo_cent    0.02288    0.08782 1510.75809   0.261\n                                 Pr(>|t|)    \n(Intercept)                      4.24e-07 ***\nclass_wins1_cent                    0.292    \nneedforclo_cent                     0.101    \nclass_wins1_cent:needforclo_cent    0.794    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cls_1_ ndfrc_\nclss_wns1_c  0.000              \nnedfrcl_cnt  0.000  0.013       \nclss_wn1_:_  0.004  0.093 -0.050\n```\n:::\n:::\n\n\n### Model objective expertise with high school and college courses\n\n\n::: {.cell}\n\n```{.r .cell-code}\nModel_obj2<-lmer(ih ~media_wins1_cent * needforclo_cent +media_wins1_cent + needforclo_cent+(1|Score_Type)+(1|ID), data=expertise6_ready)\nsummary(Model_obj2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: ih ~ media_wins1_cent * needforclo_cent + media_wins1_cent +  \n    needforclo_cent + (1 | Score_Type) + (1 | ID)\n   Data: expertise6_ready\n\nREML criterion at convergence: 6312\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.95382 -0.66151  0.02337  0.66064  2.55803 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.7553   0.8691  \n Score_Type (Intercept) 0.7773   0.8817  \n Residual               3.1108   1.7638  \nNumber of obs: 1528, groups:  ID, 191; Score_Type, 8\n\nFixed effects:\n                                   Estimate Std. Error         df t value\n(Intercept)                         5.10190    0.32118    7.56820  15.885\nmedia_wins1_cent                   -0.22040    0.07087 1498.82315  -3.110\nneedforclo_cent                     0.18599    0.11163  188.65641   1.666\nmedia_wins1_cent:needforclo_cent   -0.06861    0.08735 1466.83328  -0.785\n                                 Pr(>|t|)    \n(Intercept)                      4.38e-07 ***\nmedia_wins1_cent                  0.00191 ** \nneedforclo_cent                   0.09735 .  \nmedia_wins1_cent:needforclo_cent  0.43236    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) md_w1_ ndfrc_\nmd_wns1_cnt  0.000              \nnedfrcl_cnt  0.000  0.017       \nmd_wns1_c:_  0.004  0.087 -0.044\n```\n:::\n:::\n\n\n### Model of perceived expertise and need for cognition\n\n\n::: {.cell}\n\n```{.r .cell-code}\nModel_perc<-lmer(ih ~know_cent*needforclo_cent+ know_cent + needforclo_cent +\n                (1|Score_Type)+(1|ID), data=expertise6_ready)\nsummary(Model_perc)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: ih ~ know_cent * needforclo_cent + know_cent + needforclo_cent +  \n    (1 | Score_Type) + (1 | ID)\n   Data: expertise6_ready\n\nREML criterion at convergence: 6321.4\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.92364 -0.65771  0.01587  0.66300  2.56876 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.7989   0.8938  \n Score_Type (Intercept) 0.7760   0.8809  \n Residual               3.1140   1.7646  \nNumber of obs: 1528, groups:  ID, 191; Score_Type, 8\n\nFixed effects:\n                            Estimate Std. Error         df t value Pr(>|t|)    \n(Intercept)                  5.10150    0.32130    7.60464  15.878 4.19e-07 ***\nknow_cent                   -0.06434    0.05369 1451.39265  -1.198    0.231    \nneedforclo_cent              0.18403    0.11385  185.25861   1.616    0.108    \nknow_cent:needforclo_cent   -0.02139    0.07491 1397.20099  -0.286    0.775    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) knw_cn ndfrc_\nknow_cent    0.001              \nnedfrcl_cnt  0.000  0.046       \nknw_cnt:nd_  0.012  0.063 -0.040\n```\n:::\n:::\n\n\n### Model of objective expertise, perceived expertise and need for cognition without interaction\n\n\n::: {.cell}\n\n```{.r .cell-code}\nModel.1.1<-lmer(ih ~class_wins1_cent + media_wins1_cent + know_cent +needforclo_cent +(1|Score_Type)+(1|ID),   \n              data=expertise6_ready)\nsummary(Model.1.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: \nih ~ class_wins1_cent + media_wins1_cent + know_cent + needforclo_cent +  \n    (1 | Score_Type) + (1 | ID)\n   Data: expertise6_ready\n\nREML criterion at convergence: 6316.6\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.94278 -0.66321  0.01385  0.66298  2.54206 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.7506   0.8664  \n Score_Type (Intercept) 0.7758   0.8808  \n Residual               3.1160   1.7652  \nNumber of obs: 1528, groups:  ID, 191; Score_Type, 8\n\nFixed effects:\n                   Estimate Std. Error         df t value Pr(>|t|)    \n(Intercept)       5.103e+00  3.209e-01  7.566e+00  15.905 4.36e-07 ***\nclass_wins1_cent  2.461e-02  7.825e-02  1.493e+03   0.314  0.75321    \nmedia_wins1_cent -2.259e-01  8.177e-02  1.508e+03  -2.762  0.00581 ** \nknow_cent        -2.644e-03  5.943e-02  1.411e+03  -0.044  0.96453    \nneedforclo_cent   1.823e-01  1.114e-01  1.840e+02   1.635  0.10367    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cls_1_ md_w1_ knw_cn\nclss_wns1_c  0.000                     \nmd_wns1_cnt  0.000 -0.357              \nknow_cent    0.001 -0.239 -0.259       \nnedfrcl_cnt  0.000 -0.001  0.003  0.044\n```\n:::\n:::\n\n\n### Model of objective expertise, perceived expertise and need for cognition with interaction\n\n\n::: {.cell}\n\n```{.r .cell-code}\nModel.2.1<-lmer(ih ~class_wins1_cent + media_wins1_cent  + know_cent+ needforclo_cent +\n                  class_wins1_cent*needforclo_cent + media_wins1_cent*needforclo_cent +\n                  know_cent*needforclo_cent+(1|Score_Type)+(1|ID),data=expertise6_ready)\nsummary(Model.2.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: \nih ~ class_wins1_cent + media_wins1_cent + know_cent + needforclo_cent +  \n    class_wins1_cent * needforclo_cent + media_wins1_cent * needforclo_cent +  \n    know_cent * needforclo_cent + (1 | Score_Type) + (1 | ID)\n   Data: expertise6_ready\n\nREML criterion at convergence: 6324.1\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.94235 -0.65807  0.01588  0.66131  2.55946 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.7574   0.8703  \n Score_Type (Intercept) 0.7741   0.8798  \n Residual               3.1172   1.7656  \nNumber of obs: 1528, groups:  ID, 191; Score_Type, 8\n\nFixed effects:\n                                   Estimate Std. Error         df t value\n(Intercept)                       5.102e+00  3.206e-01  7.573e+00  15.914\nclass_wins1_cent                  2.937e-02  7.858e-02  1.490e+03   0.374\nmedia_wins1_cent                 -2.337e-01  8.223e-02  1.499e+03  -2.842\nknow_cent                        -1.025e-03  5.960e-02  1.407e+03  -0.017\nneedforclo_cent                   1.836e-01  1.120e-01  1.834e+02   1.640\nclass_wins1_cent:needforclo_cent  9.434e-02  1.101e-01  1.439e+03   0.857\nmedia_wins1_cent:needforclo_cent -1.188e-01  1.108e-01  1.512e+03  -1.073\nknow_cent:needforclo_cent        -8.282e-03  8.415e-02  1.402e+03  -0.098\n                                 Pr(>|t|)    \n(Intercept)                      4.29e-07 ***\nclass_wins1_cent                  0.70864    \nmedia_wins1_cent                  0.00455 ** \nknow_cent                         0.98628    \nneedforclo_cent                   0.10278    \nclass_wins1_cent:needforclo_cent  0.39167    \nmedia_wins1_cent:needforclo_cent  0.28364    \nknow_cent:needforclo_cent         0.92162    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cls_1_ md_w1_ knw_cn ndfrc_ c_1_:_ m_1_:_\nclss_wns1_c  0.001                                          \nmd_wns1_cnt -0.001 -0.359                                   \nknow_cent    0.001 -0.234 -0.263                            \nnedfrcl_cnt  0.000 -0.005  0.002  0.043                     \nclss_wn1_:_ -0.001  0.075 -0.043  0.023 -0.024              \nmd_wns1_c:_  0.000 -0.044  0.099 -0.044 -0.014 -0.495       \nknw_cnt:nd_  0.011  0.023 -0.038  0.051 -0.018 -0.213 -0.249\n```\n:::\n:::\n\n\n\n#Plot of the model 1.1\n\n::: {.cell}\n\n```{.r .cell-code}\nModel.1.1 <- lmer(ih ~class_wins1_cent + media_wins1_cent + know_cent +needforclo_cent +(1|Score_Type)+(1|ID),   \n              data=expertise6_ready)\nsummary(Model.1.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: \nih ~ class_wins1_cent + media_wins1_cent + know_cent + needforclo_cent +  \n    (1 | Score_Type) + (1 | ID)\n   Data: expertise6_ready\n\nREML criterion at convergence: 6316.6\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.94278 -0.66321  0.01385  0.66298  2.54206 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.7506   0.8664  \n Score_Type (Intercept) 0.7758   0.8808  \n Residual               3.1160   1.7652  \nNumber of obs: 1528, groups:  ID, 191; Score_Type, 8\n\nFixed effects:\n                   Estimate Std. Error         df t value Pr(>|t|)    \n(Intercept)       5.103e+00  3.209e-01  7.566e+00  15.905 4.36e-07 ***\nclass_wins1_cent  2.461e-02  7.825e-02  1.493e+03   0.314  0.75321    \nmedia_wins1_cent -2.259e-01  8.177e-02  1.508e+03  -2.762  0.00581 ** \nknow_cent        -2.644e-03  5.943e-02  1.411e+03  -0.044  0.96453    \nneedforclo_cent   1.823e-01  1.114e-01  1.840e+02   1.635  0.10367    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cls_1_ md_w1_ knw_cn\nclss_wns1_c  0.000                     \nmd_wns1_cnt  0.000 -0.357              \nknow_cent    0.001 -0.239 -0.259       \nnedfrcl_cnt  0.000 -0.001  0.003  0.044\n```\n:::\n\n```{.r .cell-code}\n# save fitted data\nef.1.media <- effect(term = \"media_wins1_cent\",\n                                   mod = Model.1.1)\nef.1.media.data <- as.data.frame(ef.1.media) #convert the effects list to a data frame\nef.1.media.data #print effects data frame\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  media_wins1_cent      fit        se    lower    upper\n1             -0.5 5.215930 0.3234555 4.581465 5.850395\n2              1.0 4.877152 0.3310815 4.227728 5.526575\n3              3.0 4.425447 0.4038177 3.633349 5.217545\n4              5.0 3.973743 0.5196216 2.954493 4.992992\n5              7.0 3.522038 0.6560706 2.235141 4.808936\n```\n:::\n\n```{.r .cell-code}\nexpert_media_graph <- expertise6_ready %>%\n  group_by(ID) %>%\n  summarise(ih = mean(ih),\n            media_wins1_cent = media_wins1)\nexpert_media_graph <- as.data.frame(expert_media_graph)\n\nexpert_media_graph$media_wins1_cent<-expert_media_graph$media_wins1_cent-min(expert_media_graph$media_wins1_cent)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfig.1.expert6_w1 <- ggplot(data = ef.1.media.data, aes(x = media_wins1_cent,\n                                                  y = fit)) +\n  geom_point(data=expert_media_graph, aes(x=media_wins1_cent,y=ih),\n             show.legend = FALSE, pch=21, color=\"blueviolet\", alpha=.25, size=3) +\n  geom_line(aes(x=media_wins1_cent), color = \"darkred\", size = 1.2)  +\n  geom_ribbon(aes(ymin = fit-se, ymax = fit+se), alpha = 0.50, fill = \"gray70\") +\n  labs(title = \"Expertise and Inherence Bias\", x = \"Expertise (books,articles)\",\n       y = \"Inherence bias\") +\n  scale_x_continuous(limits = c(0, 8), breaks = seq(0, 8, 2), expand = c(0, 0)) +\n  scale_y_continuous(limits = c(0, 10), breaks = seq(0, 10, 2), expand = c(0, 0)) +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n        axis.title = element_text(size = 14, face = \"bold\"),\n        axis.text = element_text(size = 12),\n        legend.title = element_blank(),\n        panel.grid = element_blank(),\n        panel.border = element_blank(),\n        plot.background=element_rect(fill = \"gray96\"),\n        panel.background = element_rect(fill = \"gray96\"))\nfig.1.expert6_w1\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-35-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggsave(paste0(\"figure_expertise6+media_w1\",\n              format(Sys.time(), \"%Y-%m-%d\")\n              ,\".eps\"), width = 20, height = 20, units = \"cm\") ###Save the Figure 3C\n```\n:::\n\n\n\n## Winsorize the variables at 2.5%\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# winsorize the variables (at 2.5%)\nexpertise6_readyw25 <- expertise6_fact%>%\n  mutate(numarticle_wins_1=Winsorize(numarticle, probs = c(0,0.975)), \n         numbook_wins_1=Winsorize(numbook, probs = c(0,0.975)),\n         high_wins_1=Winsorize(high, probs = c(0,0.975)),\n         colle_wins_1=Winsorize(colle, probs = c(0,0.975)),\n         grad_wins_1=Winsorize(grad,na.rm=TRUE, probs = c(0,0.975)),\n         docu_wins_1=Winsorize(numdocu,na.rm=TRUE, probs = c(0,0.975)),\n         inarti_wins_1=Winsorize(numinarti,na.rm=TRUE, probs = c(0,0.975)),\n         radio_wins_1=Winsorize(numradio,na.rm=TRUE, probs = c(0,0.975)))\n\n#check descriptives\ndescriptives(dat=expertise6_readyw25, vars(docu_wins_1,inarti_wins_1, radio_wins_1 ), median=F, n=F, missing=T, sd=T, skew =T)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n DESCRIPTIVES\n\n Descriptives                                                            \n ─────────────────────────────────────────────────────────────────────── \n                          docu_wins_1    inarti_wins_1    radio_wins_1   \n ─────────────────────────────────────────────────────────────────────── \n   Missing                          0                0               0   \n   Mean                      1.908203         7.897135       0.6673177   \n   Standard deviation        4.235883         19.75722        1.995598   \n   Minimum                   0.000000         0.000000        0.000000   \n   Maximum                   20.00000         100.0000        10.00000   \n   Skewness                  3.115780         3.750920        3.538529   \n   Std. error skewness     0.06243908       0.06243908      0.06243908   \n ─────────────────────────────────────────────────────────────────────── \n```\n:::\n\n```{.r .cell-code}\ndescriptives(dat=expertise6_readyw25, vars(high_wins_1, colle_wins_1), median=F, n=F, missing=T, sd=T)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n DESCRIPTIVES\n\n Descriptives                                          \n ───────────────────────────────────────────────────── \n                         high_wins_1    colle_wins_1   \n ───────────────────────────────────────────────────── \n   Missing                         0               0   \n   Mean                    0.3079427       0.5032552   \n   Standard deviation      0.8547027        1.128830   \n   Minimum                  0.000000        0.000000   \n   Maximum                  4.000000        5.000000   \n ───────────────────────────────────────────────────── \n```\n:::\n\n```{.r .cell-code}\ndescriptives(dat=expertise6_readyw25, vars(numarticle_wins_1, numbook_wins_1, grad_wins_1), median=F, n=F, missing=T, sd=T)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n DESCRIPTIVES\n\n Descriptives                                                                 \n ──────────────────────────────────────────────────────────────────────────── \n                         numarticle_wins_1    numbook_wins_1    grad_wins_1   \n ──────────────────────────────────────────────────────────────────────────── \n   Missing                               0                 0              0   \n   Mean                           4.841797          1.345052     0.02929688   \n   Standard deviation             10.50945          3.036276      0.1686923   \n   Minimum                        0.000000          0.000000       0.000000   \n   Maximum                        50.00000          15.00000       1.000000   \n ──────────────────────────────────────────────────────────────────────────── \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Standardize the Selected variables  ----\nvars_to_standardize <- c(\"know\", \"docu_wins_1\",\"inarti_wins_1\", \"radio_wins_1\",\"high_wins_1\", \"colle_wins_1\",\n                         \"numarticle_wins_1\", \"numbook_wins_1\", \"grad_wins_1\")\n\n\n\nexpertise6_readyw25<-expertise6_readyw25 %>% \n  group_by(Score_Type) %>% \n  mutate_at(vars(vars_to_standardize), scale)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsum(is.na(expertise6_readyw25$grad_wins_1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 384\n```\n:::\n:::\n\n\n## Take the means of winsorized variables to create our new categories: media and class\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexpertise6_readyw25 <- expertise6_readyw25 %>% replace_na(list(colle_wins_1 = NA, high_wins_1 = NA, grad_wins_1 = NA, docu_wins_1= NA,inarti_wins_1 = NA, radio_wins_1 = NA, numarticle_wins_1 = NA, numbook_wins_1 = NA))\n\n\nexpertise6_readyw25 <- expertise6_readyw25 %>% mutate(class_wins1 = mapply(function(x, y, z) {\n  ifelse(is.na(x), (y + z)/2, \n         ifelse(is.na(y), (x + z)/2, \n                ifelse(is.na(z), (x + y)/2, (x + y + z)/3)))}, \n  grad_wins_1, colle_wins_1, high_wins_1))\n\nexpertise6_readyw25 <- expertise6_readyw25 %>% mutate(media_wins1 = mapply(function(x, y, z, t, r) {\n  ifelse(is.na(x), (y + z + t + r)/4, \n         ifelse(is.na(y), (x + z + t + r)/4,\n                ifelse(is.na(z), (x + y + t + r)/4, \n                       ifelse(is.na(t),(x + y + z + r)/4, \n                              ifelse(is.na(r),(x + y + z + t)/4,\n                                     (x + y + z + t + r)/5)))))}, \n  docu_wins_1,inarti_wins_1, radio_wins_1, numbook_wins_1, numarticle_wins_1))\n```\n:::\n\n\n## Center the variables\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexpertise6_readyw25$know_cent <- scale(expertise6_readyw25$know, center = TRUE, scale = FALSE)\n\nexpertise6_readyw25$needforclo_cent <- scale(expertise6_readyw25$needforclo, center = TRUE, scale = FALSE)\n\nexpertise6_readyw25$class_wins1_cent <- scale(expertise6_readyw25$class_wins1, center = TRUE, scale = FALSE)\n\nexpertise6_readyw25$media_wins1_cent <- scale(expertise6_readyw25$media_wins1, center = TRUE, scale = FALSE)\n```\n:::\n\n\n\n##Shapiro-Wilk values\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndescriptives(expertise6_readyw25, vars = vars(class_wins1, media_wins1, know, ih),n=FALSE, missing= FALSE, median=FALSE, sw = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n DESCRIPTIVES\n\n Descriptives                                                                           \n ────────────────────────────────────────────────────────────────────────────────────── \n                         class_wins1      media_wins1      know            ih           \n ────────────────────────────────────────────────────────────────────────────────────── \n   Mean                  -1.966020e-17    -3.545341e-17    5.261995e-17      5.112305   \n   Standard deviation        0.7133831        0.7273781       0.9977173      2.142995   \n   Minimum                  -0.6311378       -0.5679514       -2.197199      1.000000   \n   Maximum                    5.997473         5.102448        3.050287      9.000000   \n   Shapiro-Wilk W            0.5950088        0.6332209       0.9721739     0.9707327   \n   Shapiro-Wilk p           < .0000001       < .0000001      < .0000001    < .0000001   \n ────────────────────────────────────────────────────────────────────────────────────── \n```\n:::\n:::\n\n\n\n## HLM Models\n\n### Model of objective expertise, perceived expertise and need for cognition without interaction\n\n\n::: {.cell}\n\n```{.r .cell-code}\nModel.1.1<-lmer(ih ~class_wins1_cent + media_wins1_cent + know_cent +needforclo_cent +(1|Score_Type)+(1|ID),   \n              data=expertise6_readyw25)\nsummary(Model.1.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: \nih ~ class_wins1_cent + media_wins1_cent + know_cent + needforclo_cent +  \n    (1 | Score_Type) + (1 | ID)\n   Data: expertise6_readyw25\n\nREML criterion at convergence: 6314\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.95570 -0.66262  0.01905  0.66183  2.53467 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.7398   0.8601  \n Score_Type (Intercept) 0.7757   0.8808  \n Residual               3.1141   1.7647  \nNumber of obs: 1528, groups:  ID, 191; Score_Type, 8\n\nFixed effects:\n                   Estimate Std. Error         df t value Pr(>|t|)    \n(Intercept)         5.10310    0.32075    7.55730  15.910 4.39e-07 ***\nclass_wins1_cent    0.02293    0.07879 1496.51118   0.291  0.77104    \nmedia_wins1_cent   -0.26334    0.08251 1499.89932  -3.192  0.00144 ** \nknow_cent           0.01554    0.06023 1401.19389   0.258  0.79639    \nneedforclo_cent     0.18163    0.11090  183.85178   1.638  0.10319    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cls_1_ md_w1_ knw_cn\nclss_wns1_c  0.000                     \nmd_wns1_cnt -0.001 -0.335              \nknow_cent    0.001 -0.235 -0.300       \nnedfrcl_cnt  0.000  0.001  0.004  0.042\n```\n:::\n:::\n\n\n#Plot of the model 1.1\n\n::: {.cell}\n\n```{.r .cell-code}\nModel.1.1 <- lmer(ih ~class_wins1_cent + media_wins1_cent + know_cent +needforclo_cent +(1|Score_Type)+(1|ID),   \n              data=expertise6_readyw25)\nsummary(Model.1.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: \nih ~ class_wins1_cent + media_wins1_cent + know_cent + needforclo_cent +  \n    (1 | Score_Type) + (1 | ID)\n   Data: expertise6_readyw25\n\nREML criterion at convergence: 6314\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.95570 -0.66262  0.01905  0.66183  2.53467 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.7398   0.8601  \n Score_Type (Intercept) 0.7757   0.8808  \n Residual               3.1141   1.7647  \nNumber of obs: 1528, groups:  ID, 191; Score_Type, 8\n\nFixed effects:\n                   Estimate Std. Error         df t value Pr(>|t|)    \n(Intercept)         5.10310    0.32075    7.55730  15.910 4.39e-07 ***\nclass_wins1_cent    0.02293    0.07879 1496.51118   0.291  0.77104    \nmedia_wins1_cent   -0.26334    0.08251 1499.89932  -3.192  0.00144 ** \nknow_cent           0.01554    0.06023 1401.19389   0.258  0.79639    \nneedforclo_cent     0.18163    0.11090  183.85178   1.638  0.10319    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cls_1_ md_w1_ knw_cn\nclss_wns1_c  0.000                     \nmd_wns1_cnt -0.001 -0.335              \nknow_cent    0.001 -0.235 -0.300       \nnedfrcl_cnt  0.000  0.001  0.004  0.042\n```\n:::\n\n```{.r .cell-code}\n# save fitted data\nef.1.media <- effect(term = \"media_wins1_cent\",\n                                   mod = Model.1.1)\nef.1.media.data <- as.data.frame(ef.1.media) #convert the effects list to a data frame\nef.1.media.data #print effects data frame\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  media_wins1_cent      fit        se    lower    upper\n1             -0.6 5.261047 0.3245589 4.624418 5.897677\n2              0.8 4.892372 0.3274497 4.250072 5.534673\n3              2.0 4.576366 0.3606657 3.868912 5.283820\n4              4.0 4.049687 0.4601571 3.147079 4.952296\n5              5.0 3.786348 0.5224948 2.761463 4.811234\n```\n:::\n\n```{.r .cell-code}\nexpert_media_graph <- expertise6_readyw25 %>%\n  group_by(ID) %>%\n  summarise(ih = mean(ih),\n            media_wins1_cent = media_wins1)\nexpert_media_graph <- as.data.frame(expert_media_graph)\n\nexpert_media_graph$media_wins1_cent<-expert_media_graph$media_wins1_cent-min(expert_media_graph$media_wins1_cent)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfig.1.expert6_w1 <- ggplot(data = ef.1.media.data, aes(x = media_wins1_cent,\n                                                  y = fit)) +\n  geom_point(data=expert_media_graph, aes(x=media_wins1_cent,y=ih),\n             show.legend = FALSE, pch=21, color=\"blueviolet\", alpha=.25, size=3) +\n  geom_line(aes(x=media_wins1_cent), color = \"darkred\", size = 1.2)  +\n  geom_ribbon(aes(ymin = fit-se, ymax = fit+se), alpha = 0.50, fill = \"gray70\") +\n  labs(title = \"Expertise and Inherence Bias\", x = \"Expertise (books,articles)\",\n       y = \"Inherence bias\") +\n  scale_x_continuous(limits = c(0, 6), breaks = seq(0, 6, 2), expand = c(0, 0)) +\n  scale_y_continuous(limits = c(0, 8), breaks = seq(0, 8, 2), expand = c(0, 0)) +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n        axis.title = element_text(size = 14, face = \"bold\"),\n        axis.text = element_text(size = 12),\n        legend.title = element_blank(),\n        panel.grid = element_blank(),\n        panel.border = element_blank(),\n        plot.background=element_rect(fill = \"gray96\"),\n        panel.background = element_rect(fill = \"gray96\"))\nfig.1.expert6_w1\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-44-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggsave(paste0(\"figure_expertise6+media_w25\",\n              format(Sys.time(), \"%Y-%m-%d\")\n              ,\".eps\"), width = 20, height = 20, units = \"cm\") ###Save the Figure\n```\n:::\n\n\n\n#Sqrt variables\n\n##Square root of the raw variables for our new expertise variables\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# take the square root of the variables\nexpertise6_sqrt <- expertise6_fact%>%\n  mutate(numarticle_sqrt=sqrt(numarticle),\n         numbook_sqrt=sqrt(numbook),\n         high_sqrt=sqrt(high),\n         colle_sqrt=sqrt(colle),\n         grad_sqrt=sqrt(grad),\n         docu_sqrt=sqrt(numdocu),\n         inarti_sqrt=sqrt(numinarti),\n         radio_sqrt=sqrt(numradio))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Standardize the Selected variables  ----\nvars_to_standardize <- c(\"know\", \"docu_sqrt\",\"inarti_sqrt\", \"radio_sqrt\",\"high_sqrt\", \"colle_sqrt\",\n                         \"numarticle_sqrt\", \"numbook_sqrt\", \"grad_sqrt\")\n\n\n\nexpertise6_sqrt<-expertise6_sqrt %>% \n  group_by(Score_Type) %>% \n  mutate_at(vars(vars_to_standardize), scale)\n```\n:::\n\n\n## Take the means of winsorized variables to create our new categories: media and class\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexpertise6_sqrt <- expertise6_sqrt %>% mutate(class_wins1 = mapply(function(x, y, z) {\n  ifelse(is.na(x), (y + z)/2, \n         ifelse(is.na(y), (x + z)/2, \n                ifelse(is.na(z), (x + y)/2, (x + y + z)/3)))}, \n  grad_sqrt, colle_sqrt, high_sqrt))\n\nexpertise6_sqrt <- expertise6_sqrt %>% mutate(media_wins1 = mapply(function(x, y, z, t, r) {\n  ifelse(is.na(x), (y + z + t + r)/4, \n         ifelse(is.na(y), (x + z + t + r)/4,\n                ifelse(is.na(z), (x + y + t + r)/4, \n                       ifelse(is.na(t),(x + y + z + r)/4, \n                              ifelse(is.na(r),(x + y + z + t)/4,\n                                     (x + y + z + t + r)/5)))))}, \n  docu_sqrt,inarti_sqrt, radio_sqrt, numbook_sqrt, numarticle_sqrt))\n```\n:::\n\n\n## Center the variables\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexpertise6_sqrt$know_cent <- scale(expertise6_sqrt$know, center = TRUE, scale = FALSE)\n\nexpertise6_sqrt$needforclo_cent <- scale(expertise6_sqrt$needforclo, center = TRUE, scale = FALSE)\n\nexpertise6_sqrt$class_wins1_cent <- scale(expertise6_sqrt$class_wins1, center = TRUE, scale = FALSE)\n\nexpertise6_sqrt$media_wins1_cent <- scale(expertise6_sqrt$media_wins1, center = TRUE, scale = FALSE)\n```\n:::\n\n\n\n## HLM Models\n\n### Model of objective expertise, perceived expertise and need for cognition without interaction\n\n\n::: {.cell}\n\n```{.r .cell-code}\nModel.1.1<-lmer(ih ~class_wins1_cent + media_wins1_cent + know_cent +needforclo_cent +(1|Score_Type)+(1|ID),   \n              data=expertise6_sqrt)\nsummary(Model.1.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: \nih ~ class_wins1_cent + media_wins1_cent + know_cent + needforclo_cent +  \n    (1 | Score_Type) + (1 | ID)\n   Data: expertise6_sqrt\n\nREML criterion at convergence: 6310.1\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.94094 -0.67254  0.00759  0.67196  2.53215 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.7285   0.8535  \n Score_Type (Intercept) 0.7760   0.8809  \n Residual               3.1093   1.7633  \nNumber of obs: 1528, groups:  ID, 191; Score_Type, 8\n\nFixed effects:\n                   Estimate Std. Error         df t value Pr(>|t|)    \n(Intercept)       5.103e+00  3.207e-01  7.548e+00  15.912 4.44e-07 ***\nclass_wins1_cent  1.329e-01  7.130e-02  1.496e+03   1.864 0.062523 .  \nmedia_wins1_cent -3.284e-01  8.534e-02  1.475e+03  -3.849 0.000124 ***\nknow_cent         8.013e-03  5.948e-02  1.392e+03   0.135 0.892865    \nneedforclo_cent   1.785e-01  1.104e-01  1.834e+02   1.617 0.107519    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cls_1_ md_w1_ knw_cn\nclss_wns1_c  0.000                     \nmd_wns1_cnt -0.001 -0.374              \nknow_cent    0.001 -0.157 -0.329       \nnedfrcl_cnt  0.000 -0.026  0.008  0.050\n```\n:::\n:::\n\n\n#Cube root variables\n\n##Cube root of the raw variables for our new expertise variables\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# cube root of the variables \nexpertise6_cube <- expertise6_fact%>%\n  mutate(numarticle_cube=(numarticle^(1/3)),\n         numbook_cube=(numbook^(1/3)),\n         high_cube=(high^(1/3)),\n         colle_cube=(colle^(1/3)),\n         grad_cube=(grad^(1/3)),\n         docu_cube=(numdocu^(1/3)),\n         inarti_cube=(numinarti^(1/3)),\n         radio_cube=(numradio^(1/3)))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Standardize the Selected variables  ----\nvars_to_standardize <- c(\"know\", \"docu_cube\",\"inarti_cube\", \"radio_cube\",\"high_cube\", \"colle_cube\",\n                         \"numarticle_cube\", \"numbook_cube\", \"grad_cube\")\n\n\n\nexpertise6_cube<-expertise6_cube %>% \n  group_by(Score_Type) %>% \n  mutate_at(vars(vars_to_standardize), scale)\n```\n:::\n\n\n## Take the means of winsorized variables to create our new categories: media and class\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexpertise6_cube <- expertise6_cube %>% mutate(class_wins1 = mapply(function(x, y, z) {\n  ifelse(is.na(x), (y + z)/2, \n         ifelse(is.na(y), (x + z)/2, \n                ifelse(is.na(z), (x + y)/2, (x + y + z)/3)))}, \n  grad_cube, colle_cube, high_cube))\n\nexpertise6_cube <- expertise6_cube %>% mutate(media_wins1 = mapply(function(x, y, z, t, r) {\n  ifelse(is.na(x), (y + z + t + r)/4, \n         ifelse(is.na(y), (x + z + t + r)/4,\n                ifelse(is.na(z), (x + y + t + r)/4, \n                       ifelse(is.na(t),(x + y + z + r)/4, \n                              ifelse(is.na(r),(x + y + z + t)/4,\n                                     (x + y + z + t + r)/5)))))}, \n  docu_cube,inarti_cube, radio_cube, numbook_cube, numarticle_cube))\n```\n:::\n\n\n\n##Histograms of square root variables\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(expertise6_sqrt$class_wins1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-53-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(expertise6_sqrt$media_wins1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-54-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(expertise6_sqrt$know)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-55-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(expertise6_sqrt$ih)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-56-1.png){width=672}\n:::\n:::\n\n\n##Shapiro-Wilk values\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndescriptives(expertise6_sqrt, vars = vars(class_wins1, media_wins1, know, ih), n=FALSE, missing= FALSE, median=FALSE, sw = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n DESCRIPTIVES\n\n Descriptives                                                                         \n ──────────────────────────────────────────────────────────────────────────────────── \n                         class_wins1     media_wins1     know            ih           \n ──────────────────────────────────────────────────────────────────────────────────── \n   Mean                  1.420305e-17    2.533419e-17    5.261995e-17      5.112305   \n   Standard deviation       0.7850757       0.7272822       0.9977173      2.142995   \n   Minimum                 -0.3681922      -0.6669302       -2.197199      1.000000   \n   Maximum                   12.85515        6.200443        3.050287      9.000000   \n   Shapiro-Wilk W           0.4392085       0.7072822       0.9721739     0.9707327   \n   Shapiro-Wilk p          < .0000001      < .0000001      < .0000001    < .0000001   \n ──────────────────────────────────────────────────────────────────────────────────── \n```\n:::\n:::\n\n\n\n## Center the variables\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexpertise6_cube$know_cent <- scale(expertise6_cube$know, center = TRUE, scale = FALSE)\n\nexpertise6_cube$needforclo_cent <- scale(expertise6_cube$needforclo, center = TRUE, scale = FALSE)\n\nexpertise6_cube$class_wins1_cent <- scale(expertise6_cube$class_wins1, center = TRUE, scale = FALSE)\n\nexpertise6_cube$media_wins1_cent <- scale(expertise6_cube$media_wins1, center = TRUE, scale = FALSE)\n```\n:::\n\n\n\n##Histograms of cube root variables\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(expertise6_cube$class_wins1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-59-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(expertise6_cube$media_wins1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-60-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(expertise6_cube$know)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-61-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(expertise6_cube$ih)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-62-1.png){width=672}\n:::\n:::\n\n\n##Shapiro-Wilk values\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndescriptives(expertise6_cube, vars = vars(class_wins1, media_wins1, know, ih),n=FALSE, missing= FALSE, median=FALSE, sw = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n DESCRIPTIVES\n\n Descriptives                                                                         \n ──────────────────────────────────────────────────────────────────────────────────── \n                         class_wins1     media_wins1     know            ih           \n ──────────────────────────────────────────────────────────────────────────────────── \n   Mean                  4.452457e-17    1.832663e-16    5.261995e-17      5.112305   \n   Standard deviation       0.7589839       0.7322954       0.9977173      2.142995   \n   Minimum                 -0.5470988      -0.8251761       -2.197199      1.000000   \n   Maximum                   10.38594        4.701118        3.050287      9.000000   \n   Shapiro-Wilk W           0.5526782       0.8353051       0.9721739     0.9707327   \n   Shapiro-Wilk p          < .0000001      < .0000001      < .0000001    < .0000001   \n ──────────────────────────────────────────────────────────────────────────────────── \n```\n:::\n:::\n\n\n\n## HLM Models\n\n### Model of objective expertise, perceived expertise and need for cognition without interaction\n\n\n::: {.cell}\n\n```{.r .cell-code}\nModel.1.1<-lmer(ih ~class_wins1_cent + media_wins1_cent + know_cent +needforclo_cent +(1|Score_Type)+(1|ID),   \n              data=expertise6_cube)\nsummary(Model.1.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: \nih ~ class_wins1_cent + media_wins1_cent + know_cent + needforclo_cent +  \n    (1 | Score_Type) + (1 | ID)\n   Data: expertise6_cube\n\nREML criterion at convergence: 6308.2\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.93913 -0.66733  0.00914  0.65985  2.52937 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ID         (Intercept) 0.7172   0.8469  \n Score_Type (Intercept) 0.7758   0.8808  \n Residual               3.1096   1.7634  \nNumber of obs: 1528, groups:  ID, 191; Score_Type, 8\n\nFixed effects:\n                   Estimate Std. Error         df t value Pr(>|t|)    \n(Intercept)         5.10324    0.32057    7.54005  15.919 4.48e-07 ***\nclass_wins1_cent    0.12867    0.07402 1503.91694   1.738   0.0824 .  \nmedia_wins1_cent   -0.35420    0.08629 1418.15202  -4.105 4.28e-05 ***\nknow_cent           0.02167    0.06034 1373.20152   0.359   0.7195    \nneedforclo_cent     0.17650    0.10979  182.95923   1.608   0.1096    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cls_1_ md_w1_ knw_cn\nclss_wns1_c  0.000                     \nmd_wns1_cnt  0.000 -0.358              \nknow_cent    0.001 -0.177 -0.346       \nnedfrcl_cnt  0.000 -0.022  0.014  0.046\n```\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}